\documentclass[a4paper,fleqn]{article}
% for the print version, 11pt must be added in the documentclass options;
% this must be deleted in the screen version.

%Required files: pdfscreen.sty, pdfscreen.cfg, ilcampo_bg.jpg, ilcampo.jpg
%\usepackage{times}
\usepackage{natbib}
\usepackage{rotating}
\usepackage{longtable, lscape}
\usepackage{threeparttablex}
\usepackage{amsmath}
 \usepackage[top=2.5cm, bottom=2.5cm, left=2cm , right=1.8cm]{geometry}
%\usepackage[bookmarksopen=false]{hyperref}
% in the newer version of pdfscreen, hyperref can be loaded first with its own parameters
\usepackage[pdftex,dvipsnames]{color}
%\usepackage[pdfstartview={XYZ null null 1}]{hyperref}  % this gives an option clash but it compiles anyway
\usepackage[pdfstartview={},pdftex,bookmarksopen,colorlinks]{hyperref}  % this gives an option clash but it compiles anyway
\usepackage[print,nopanel,sectionbreak,palegreen]{pdfscreen}


% choose between print and screen; panelright, paneltoc;
% To get the screen version right: delete the s_man*.toc file
% and compile 3 times.


\usepackage{pictexwd}
%\usepackage{supertabular}
%\usepackage{tabls}
\usepackage{enumitem}


\setlength{\bibsep}{0.01in}
\begin{screen}
 \margins{.65in}{.65in}{.65in}{.65in}
 \screensize{6.25in}{8in}
% \changeoverlay
 \overlay{ilcampo_bg.jpg}
 \emblema{ilcampo.jpg}
 \paneloverlay{but.pdf}
 \def\pfill{\vskip6pt}
 \definecolor{section}{rgb}{1,.4,0}
 \definecolor{section0}{named}{RawSienna}
\end{screen}

\begin{print}
\setlength{\oddsidemargin}{15mm}
\end{print}


%\usepackage[pdftex]{graphicx}
%\usepackage[pdftex,dvipsnames]{color}

%%\renewcommand\floatpagefraction{1}
%\renewcommand\textfraction{0}
%\def\pdfscreen{\texttt{\small\color{section1}pdfscreen}\xspace}

%\def\bibsection{\section{\refname}}
\renewcommand\bibsection{\section{\refname}}

\newcommand{\opmerking}[1]{\par \fbox{\Large #1} \par}
%\newcommand{\opmerking}[1]{}
\newcommand{\ch}{\mbox{$\chi^{2}$ }}
\newcommand{\boldpi}{\mbox{\boldmath$\pi$ }}
\renewcommand{\l}{\mbox{$\lambda$ }}
\newcommand{\informationy}{\mbox{${\cal E}$}}
\newcommand{\var}{\mbox{var}}
\newcommand{\cov}{\mbox{cov}}
\newcommand{\mathbold}[1]{\mbox{\boldmath $\bf#1$}}
\newcommand{\Reals}{\mbox{I} \! \mbox{R}}
\newcommand{\+}{\, + \,}
\renewcommand{\min}{\, - \,}
\newcommand{\half}{{\textstyle \frac{1}{2}}}
\newcommand{\neqsum}[3]
{\, \sum_{\stackrel{\scriptstyle #1 = 1}{\scriptstyle #2 \neq #3}}^n \,}
\newcommand{\vit}{\theenumi}

\newcommand{\firsttabitem}{\hspace{4mm} $\bullet$ \hspace{1mm}}
\newcommand{\tabitem}{\\ \\ \hspace{4mm} $\bullet$ \hspace{1mm}}

\newcommand{\E}{\mbox{$\cal E$}}
\renewcommand{\P}{\mbox{P}}
\newcommand{\se}{\mbox{s.e. }}

\newcommand{\sfn}[1]{\textsf{#1}}

\newcommand{\R}{{\sf R }}
\newcommand{\Rn}{{\sf R}}
\newcommand{\rs}{{\sf RSiena}}
\newcommand{\RS}{{\sf RSiena }}
\newcommand{\SI}{{\sf SIENA }}
\newcommand{\SN}{{\sf StOCNET }}
\newcommand{\si}{{\sf SIENA}}
\newcommand{\sn}{{\sf StOCNET}}

\newcommand{\mcc}[2]{\multicolumn{#1}{c}{#2}}
\newcommand{\mcp}[2]{\multicolumn{#1}{c|}{#2}}

\renewcommand{\th}[1]{$\theta_{#1}$}
\newcommand{\be}[1]{$\beta_{#1}$}
\newcommand{\ga}[1]{$\gamma_{#1}$}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
%\renewcommand{\bibitem}[1]{\bigskip \par \noindent \hspace{-4pt}}
\makeatletter
\newenvironment{indentation}[2]
{\par \setlength{\leftmargin}{#1}       \setlength{\rightmargin}{#2}
  \advance\linewidth -\leftmargin       \advance\linewidth -\rightmargin
  \advance\@totalleftmargin\leftmargin  \@setpar{{\@@par}}%
  \parshape 1 \@totalleftmargin         \linewidth \ignorespaces}{\par}
\makeatother
%\renewcommand{\bibitem}[1]{\par \noindent \hskip-\parindent}

\newcommand{\separationb}{\\[0.5ex]\hline\rule{0pt}{2ex}}


\newcounter{savenumi}

\newcounter{thisno}
\newcommand{\startno}{\setcounter{thisno}{0}}
\newcommand{\nextno}{\addtocounter{thisno}{1}\thethisno .\ }

\hyphenation{Snij-ders Duijn Huis-man Steg-lich Schwein-ber-ger}

\newcommand{\interruptenum}{
      \setcounter{savenumi}{\value{enumi}}
      \end{numlijst}
      \end{slid} \begin{slid}
      \begin{numlijst}
      \setcounter{lijstnum}{\value{savenumi}}}

%\renewcommand{\baselinestretch}{1.2}


\begin{print}
\setlength{\oddsidemargin}{0.6cm}
\setlength{\textwidth}{15cm}
\end{print}


\begin{screen}
\title{\color{section0}{\Huge Manual for \textsf{SIENA} version 4.0} }
\end{screen}
\begin{print}
\title{{\Huge Manual for \textsf{SIENA} version 4.0 \protect\newline \normalsize \emph{Provisional version} } }
\end{print}
\author{\color{section1}\Large Ruth M. Ripley\\[1ex]
        \color{section1}\Large Tom A.B.\ Snijders\\[4ex]
       {\color{section1}\large University of Oxford: Department of Statistics; Nuffield College}\\[1ex]
    }
%\date{}

\definecolor{lc}{cmyk}{0,0.5,0,0.5}

\begin{document}
\begin{print}
\addtocontents{toc}{\small}
\end{print}

\maketitle

\begin{screen}
\vfill
\end{screen}
\begin{print}

%\setlength{\unitlength}{1mm}
%\begin{picture}(100,100)
%\put(0,0){\includegraphics*[scale=4]{ilcampo.jpg}}
%\end{picture}
\vfill
\begin{center}
\includegraphics*[scale=3]{ilcampo.jpg}
\end{center}
\vfill
\end{print}

\begin{abstract}
\noindent \SI (for {\sf Simulation Investigation for Empirical
Network Analysis}) is a computer program that carries out the
statistical estimation of models for the evolution of social
networks according to the dynamic actor-oriented model of \citet{Snijders01,
Snijders05} and \citet{SnijdersEA07}.
This is the manual for \SI version 4, which is a contributed package to
the statistical system \Rn.
The manual is based on the earlier manual for \SI version 3,
and also contains contributions written for that manual by
Mark Huisman, Michael Schweinberger, and Christian Steglich.
\end{abstract}


%\addtocontents{toc}{\setlength{\parsep}{1pt plus1pt minus1pt}}

%\addtocontents{toc}{\setlength{\itemsep}{1pt plus1pt minus1pt}}

\begin{print}
\vfill
\newpage
\tableofcontents
\newpage
\end{print}

\begin{screen}
\vfill
\sloppy
\end{screen}


\begin{print}
\makeatletter
\def\@linkcolor{lc}
\makeatother
\end{print}

\section{General information}


\si
\begin{print}
\footnote{This program was first presented at the
International Conference for Computer Simulation and the Social
Sciences, Cortona (Italy), September 1997, which originally was
scheduled to be held in Siena. See \citet{SnijdersDuijn97}.}
\end{print}
\begin{screen}
\footnote{This program was first presented
at the International Conference for Computer Simulation and the
Social Sciences, Cortona (Italy), September 1997, which originally
was scheduled to be held in Siena. See \citet{SnijdersDuijn97} .
The background picture in this manual is the Palazzo Pubblico with
the Torre del Mangia in Siena.}
\end{screen}
$\!\!\!$, shorthand for {\sf Simulation Investigation for Empirical
Network Analysis}, is a computer program that carries out the
statistical estimation of models for repeated measures of social
networks according to the dynamic actor-oriented model of \citet{SnijdersDuijn97}, \citet{Snijders01}, and
\citet*{SnijdersEA07}; also see
\citet*{SteglichEA10}.
A tutorial for these models is in \citet*{SnijdersEA10b}.
Some examples are
presented, e.g., in \citet*{vanBunt99, vanBuntEA99} and \citet*{vanDuijnEA03};
and \citet*{SteglichEA06}.

A website for \SI is maintained at \url{http://www.stats.ox.ac.uk/~snijders/siena/}~.
At this website
(`publications' tab) you shall also find references to introductions in various other languages.


This is a provisional manual for \SI version 4.0,
which is also called \rs.
This is a contributed package for the \R statistical system
which can be downloaded from\\
\url{http://cran.r-project.org}. For the operation of \Rn,
the reader is referred
to the corresponding manual. If desired, \SI can be operated \emph{apparently}
independently of \Rn, as is explained in Section~\ref{Gui}.

Sometimes latest versions are available at
\url{http://r-forge.r-project.org/R/?group_id=461}
before being incorporated into the R package that can be downloaded from CRAN.

\RS was programmed by
Ruth Ripley and Krists Boitmanis, in collaboration with Tom Snijders.

This manual is updated rather frequently, and it may be worthwhile
to check now and then for updates.
It is possible that the current version still bears some traces
from the conversion of \SI version 3 to 4, and has (mistakenly)
some remarks that apply to version 3 and not to 4.

\iffalse
The manual focuses on the use of \SI for analysing the dynamics
of directed networks. The case of non-directed networks is very similar,
and at various points this case is described more in particular.
\bigskip

\framebox[0.9\textwidth]{\begin{minipage}[t]{0.84\textwidth}
\vspace*{0.5ex}
For getting started, there are various options:
\begin{enumerate}
\item One excellent option is to read the User's Manual
from start to finish (leaving aside the Programmer's Manual).
\item A second option is to read the Minimal Introduction contained
     in Section \ref{S_minsi1}, together with the
     table of contents to have an idea of what can be looked up later.
\item Another option is first to read the Minimal Introduction and further
to focus on Sections
\ref{S_modspec} for the model specification,
\ref{S_Est} to get a basic insight in what happens in the parameter estimation,
\ref{S_output} to understand the output file (which is meant to be
as self-explanatory as possible),
and \ref{S_getting} for the basis of getting started.
\end{enumerate}
\vspace*{0.5ex}
\end{minipage}}
\bigskip
\fi

%\newpage

We are grateful to NIH (National Institutes of Health)
for their funding of programming \rs.
This is done
as part of the project \emph{Adolescent Peer Social Network Dynamics
and Problem Behavior}, funded by NIH (Grant Number 1R01HD052887-01A2),
Principal Investigator John M. Light (Oregon Research Institute).

For earlier work on \si, we are grateful to NWO (Netherlands Organisation for
Scientific Research) for their support to the integrated research program
\emph{The dynamics of networks and behavior} (project number 401-01-550),
the project \emph{Statistical methods for the joint development of
individual behavior and peer networks} (project number 575-28-012),
the project \emph{An open software system for the statistical
analysis of social networks} (project number 405-20-20),
and to the foundation ProGAMMA,
which all contributed to the work on \si.

\newpage
\begin{print}
%\addtocontents{toc}{\vspace{-2ex}}  % hack to avoid a 3-page table of contents
\part{Minimal Intro}
\end{print}
\begin{screen}
{\color{section0}\LARGE\bf\noindent
Part I\\[1.5ex] Minimal Introduction to \SI  \\[1.8ex]}
\end{screen}
The following is a minimal cookbook-style
introduction for getting started with \SI using
the graphical user interface (\emph{gui}) {siena.exe}. Later sections explain
other ways to run \si. If you are looking for help with a specific problem, read
the section \ref{sec:problems}.

\section{Getting started with \SI}
\label{S_minsi1}

\subsection{Installation and running the graphical user interface under Windows}
\label{Gui}
\begin{enumerate}
\item % Install the \RS version of \Rn.
  Install \R (version 2.9.0 or later). Note that if this leads to any
  problems or questions, \R has an extensive list of `frequently asked
  questions' which may contain adequate help for you.\\
  Start \Rn, click on \sfn{Packages} and
  then on \sfn{Install packages(s)...}. You will be prompted to select a mirror
  for download. Then select the packages \sfn{xtable},
  \sfn{network}, \sfn{rlecuyer}, \sfn{snow},
  and \rs. (There may be later zipped version of \RS available on our web
  site: to install this, use \sfn{Install package(s) from local zip files}, and
  select \sfn{RSiena.zip} (with the appropriate version number in the file
  name).\\
  If you are using Windows Vista and get an error of denied permission
  when trying to install the packages,
  you may get around this by right-clicking the \R icon and selecting
  `Run as administrator'.
\item If you want to get the latest beta version of \rs, before installing the
  packages, select \sfn{Packages/Select repositories...} and select
  \sfn{R-forge}. Then install the packages in the normal way. \\
  Note: On Windows if
  you select \sfn{R-forge}, by default, \sfn{CRAN} will be removed. On Linux or
  Mac, by default, both will be selected. Ensure that \sfn{CRAN} is deselected.
\item Install the program \sfn{siena.exe} by, within \Rn, loading the package
  RSiena using the \sfn{Packages/Load package...} menu. Then, still within \Rn,
  type \sfn{installGui()}. This will launch the installer which will create
  shortcuts and Start menu entries for \sfn{siena.exe}. You can then close \Rn.
\item On Linux or Mac, it may be necessary to use \\
 \sfn{install.packages("RSiena", repos="http://www.stats.ox.ac.uk/pub/RWin")}\\
or, for to get the version from \sfn{R-forge}, \\
 \sfn{install.packages("RSiena", repos="http://R-Forge.R-project.org")}


\item Run \sfn{siena.exe} from the menu or by (double-)clicking a shortcut on
  the taskbar (or desktop).\\
  If this does not work for some reason, then see item number \ref{it:prob}
  below or consult Section~\ref{S_guiinR}.
\item In Windows, by right-clicking the shortcut and clicking `Properties'
      you can change the current working directory, given in the
      `Start in' field. Data files will be searched in first instance
      in this directory.
\item You should see a screen like that shown in \hyperlink{siena1}{Figure
    \ref{fig:siena1}}
  \begin{figure}[ht]
    \begin{center}
      \includegraphics[width=\textwidth]{siena1.png}
\hypertarget{siena1}{}
    \end{center}
\caption{Siena Data Entry Screen}
\label{fig:siena1}
  \end{figure}
\item \label{it:prob} If you do not see this screen, navigate in MyComputer to your \Rn{}
  distribution (probably somewhere like \textsf{c:/Program Files/R/R-2.9.0}),
  then move to the \textsf{bin} folder and double click on \textsf{RSetReg.exe}.
\item Then try running siena again.
\item If the initial screen appears correctly, then check your working directory
  or folder. This is
  the directory that is opened immediately when clicking the \textsf{Add} button.
  Various problems can be avoided by making sure that the working directory
  is the directory that also contains the data files and the saved session file
  (see below)!\\
  You need to have permission to write files in the working directory,
  and the data files you want to use need to be in the same directory. To
  do this:
\begin{enumerate}
\item Right click on the shortcut, and select Properties. (if somehow you
don't have permission to do this, try copying the shortcut and pasting
to create another with fewer restrictions.)  In the \textsf{Start in:} field
type the name of the directory in which you wish to work, i.e., a
directory in which you can both read and write files. Then click OK.

\item To run the examples, put the session file and the two data files in
the chosen directory before starting siena.

\item To use your own data, put that data in the chosen directory before
starting siena.
\end{enumerate}
\end{enumerate}

\subsection{Using the graphical user interface from Mac or Linux}
\begin{enumerate}
\item Install \R (version 2.9.0 or greater) as appropriate for your computer.
\item Within \Rn, type\\
  \sfn{install.packages("RSiena")}\\
To use the latest beta version, use\\
 \sfn{install.packages("RSiena", repos="http://R-Forge.R-project.org")}

%  \sfn{install.packages("RSiena", repos="http://www.stats.ox.ac.uk/pub/RWin")}

%\item It is possible that Mac users on `Tiger' will need\\
%  \sfn{install.packages("RSiena", repos="http://www.stats.ox.ac.uk/pub/RWin",
%    type="source")}

\item Navigate to the directory RSiena package, (which you can find from within
  R by running \sfn{system.file(package="RSiena")}) and find a file called
  \sfn{sienascript}.  Run this to produce the Siena GUI screen.(You will
  probably have to change the permissions first (e.g.\ \\ \textsf{chmod u+x
    sienascript})).
\item If you want to use the GUI, you need tcl/tk installed. This is an
  (optional) part of the R installation on Mac. On Linux, you may need to
  install Tcl/tk and the extra Tcl/tk package \sfn{tktable}. On
  Ubuntu Linux, the following commands will do what is
  necessary:\protect\footnote{Thanks to Michael Schweinberger and Krists
    Boitmanis for supplying these.}
\begin{verbatim}
sudo apt-get install tk8.5
sudo apt-get install libtktable2.9
\end{verbatim}
\end{enumerate}

\subsection{Running  the graphical user interface from within \R}
\label{S_guiinR}

The GUI interface can be just as easily be executed from within \Rn,
which may be helpful if for some reason \textsf{siena.exe}
does not operate as desired.\protect\footnote{We are
grateful to Paul Johnson for supplying
these ideas.}
This is done by starting up \R and working with the following commands.
Note that \R is case-sensitive, so you must use upper and lower
case letters as indicated.

First, set the `working directory' of the \R session
to the same directory that holds the data files;
for example,\\
\sfn{setwd('C:/SienaTest')}

\noindent (Note the forward slash
 \protect\footnote{You can use backward ones but they
 must be doubled: \sfn{setwd('C:\textbackslash\textbackslash SienaTest')}.},
 and the quotes are necessary
 \protect{\footnote{Single or double, as long as they match.}.)
Windows users can use the \sfn{Change dir...} option on the \sfn{File} menu.

You can use the following commands to make sure the working directory is
what you intend and see which files are included in it:\\
\sfn{ getwd()}\\
\sfn{ list.files()}

Assuming you see the data files, then you can proceed to load the
\RS package, with the library function:\\
\sfn{ library(RSiena)}\\
The other packages will be loaded as required, but if you wish to examine them
or use other facilities from them you can load them using:\\
\sfn{ library(snow)}\\
\sfn{ library(network)}\\
\sfn{ library(rlecuyer)}\\
The following command
will give a review of
the functions that \RS offers:\\
\sfn{ library(help=RSiena)}\\
After that, you can use the \RS GUI. It will `launch' out
of the \R session.\\
\sfn{ siena01Gui()}\\
You can monitor the \R window for error messages -- sometimes they are
informative.

When you are done, quit \R in the polite way:\\
\sfn{q()} \\(Windows users may quit from the \sfn{File} menu or by closing the
window.)

\subsection{Entering Data.}
\label{thegui}
There are two ways to enter the data.
\begin{enumerate}
\item Enter each of your data files using \sfn{Add}.\\
      Fill in the various columns as described in Section~\ref{S_de_screen}.
\item If you have earlier saved the specification
      of data files, e.g., using \sfn{Save to file}, then you can
      use \sfn{Load new session from File}.\\
      This requires a file in the format described
      at the end of  Section~\ref{S_de_screen};
      such a file can be created and read in an editor or spreadsheet program,
      and it is created in .csv (comma separated) format
      by the \sfn{ siena01Gui()} when you request
      \sfn{Save to file}.
\item If you wish to remove files, use the \sfn{Remove} option rather than
  blanking out the entries.
\end{enumerate}
Once you have done this, check that the \sfn{Format},
\sfn{Period}, \sfn{Type}, etc., are correct, and enter any
values which indicate missingness in the \sfn{Missing Values} column.
A (minimal) complete screen is shown in \hyperlink{siena2}
{Figure~\ref{fig:siena2}}.
The details of this screen are explained in Section~\ref{S_de_screen}.
  \begin{figure}[ht]
\hypertarget{siena2}{}
    \begin{center}
      \includegraphics[width=\textwidth]{siena2.png}
    \end{center}
 \caption{Example of a Completed Data Entry Screen}
 \label{fig:siena2}
\end{figure}


\subsection{Running the Estimation Program}
\label{estgui}
\begin{enumerate}
\item Click \sfn{Apply}: you will be prompted to save your work. Then you should
  see the \sfn{Model Options} screen shown in \hyperlink{options}{Figure
    \ref{fig:options}}.
  \begin{figure}[ht]
      \hypertarget{options}{}
    \begin{center}
      \includegraphics[width=\textwidth]{siena3.png}
    \end{center}
\caption{Model options screen}
\label{fig:options}
  \end{figure}
    If this does not happen, then one possible source of error is that the
    program cannot find your files; e.g.,
    the files are not in the working directory (see above) but in a different
    directory.\\
    If errors occur at this moment and the options screen does not appear,
    then you can obtain diagnostic error messages
    working not through the \sfn{siena01Gui}, but directly  within \R
    as described in Section~\ref{S_slightlyR}.
    This will hopefully help you solving this problem; later on
    you can then work through the \sfn{siena01Gui} again.
\item Select the options you require.
\item Use \sfn{Edit Effects} to choose the effects you wish to include. Note you
  can edit the effects for just one dependent variable at a time if you wish
  by selecting one dependent variable in `Effects dependent variable'.
\item Click \sfn{Estimate}.
\item You should see the \SI screen of the estimation program.
\item When the program has finished, you should see the results. If not, click
  \sfn{Display Results} to see the results.  The output file which you will see
  is stored, with extension \texttt{.out} in the directory in which you start
  \sfn{siena.exe}.
\item You may restart your estimation session at a later date using the
  \sfn{Continue session from file} on the \sfn{Data Entry Screen}.\\
  The restart needs a saved version of the data, effects and model as R
  objects. This will be created automatically when you first enter the
  \sfn{Model Options Screen}, using the default effects and model. You may save
  the current version at any time using the \sfn{Save to file} button, and will
  be prompted to do so when you leave this screen.
\end{enumerate}

\subsection{Details of The Data Entry Screen}
\label{S_de_screen}

\begin{description}
\item[\sfn{Group}] May be left blank unless you wish to use the
  \sfn{multi-group} option described in Section~\ref{S_multigroup}. Should not
  contain embedded blanks.
\item[\sfn{Name}] Network files or dyadic covariates should use the same name
  for each file of the set. Other files should have unique names, a list of
  space separated ones for constant covariates.
\item[\sfn{File Name}] Usually entered by using a file selection box, after
  clicking \sfn{Add}.
\item[\sfn{Format}] Only relevant for networks or dyadic covariates. Can be
  a matrix; a single
  Pajek network (\sfn{.net}) (not for two-mode networks);
  or a \sfn{Siena network file} (an edgelist,
  containing three or four columns: (from, to, value, wave (optional)), not yet
  tested for dyadic covariates!).
%(\sfn{.paj} file support will be added
  %later, with a specific button to load a complete project.)
\item[\sfn{Period(s)}] Only relevant for networks and dyadic covariates. All
  other files cover all the relevant periods. Indicates the order of the network
  and dyadic covariate files. Should range from 1 to \sfn{M} within each
  \sfn{group}, where \sfn{M} is the number of time points (waves).
  Use multiple numbers separated by spaces for multi-wave Siena
  network files.
\item[\sfn{ActorSet}] If you have more than one set of nodes, use this column to
  indicate which is relevant to each file. Should not contain embedded blanks.
\item[\sfn{Type}] Indicate here what type of data the file contains. Options
  are:
\begin{description}
\item[\sfn{network}] (i.e., a one-mode network)
\item[\sfn{bipartite}] (i.e., a two-mode network)
\item[\sfn{behavior}]
\item[\sfn{constant covariate}]
\item[\sfn{changing covariate}]
\item[\sfn{constant dyadic covariate}]
\item[\sfn{changing dyadic covariate}]
\item[\sfn{exogenous event}] (for changing composition of the actor set)
\end{description}
\item[\sfn{Selected}] Yes or No. Files with \sfn{Yes} \emph{or blank} will be
included in the model. Use this field to remove any networks or behavior
variables that are not required in the model.
\item[\sfn{Missing Values}] Enter any values which indicate missingness, with
  spaces between different entries.
\item[\sfn{Nonzero Codes}] Enter any values which indicate ties, with spaces
  between different entries.
\item[\sfn{NbrOfActors}] For \sfn{Siena network files}, enter the number of
  actors. For \sfn{Siena net bipartite files}, enter the two dimensions
  (number of rows, number of columns) of the network, separated by a blank space.
 \end{description}

 The details of the screen can be saved to a \emph{session} file, from which
 they can be reloaded. But you can create a session file directly: it should
 have columns with exactly the same names and in exactly the same order as those
 of the \sfn{Data Entry} screen, and be of any of the following types:
\begin{center}
\begin{tabular}{lll}\\
Extension&Type\\
\texttt{.csv}&Comma separated\\
\texttt{.dat} or \texttt{.prn}&Space delimited\\
\texttt{.txt}&Tab delimited\\
\end{tabular}
\end{center}

\noindent
The root name of this input file will also be the root name of the output file.

\bigskip

\subsection{Data formats}
\label{S_datform}

\begin{enumerate}
\item
Network and covariate files should be text files with a row for each node. The
numbers should be separated by spaces or tabs.
\item
An exogenous events file can be given, indicating change of composition of the
network in the sense that some actors are not part of the network during
all the observations.
This will trigger treatment of such change of composition
according to \citet{HuismanSnijders03}.
This file must have one row for each node.
Each row should be
consist of a set of pairs of numbers which indicate the periods
during which the corresponding actor
was present. For example,
\begin{verbatim}
1 3
1.5 3
1 1.4 2.3 3
2.4 3
\end{verbatim}
would describe a network with 4 nodes, and 3 observations. Actor 1 is present
all the time, actor 2 joins at time 1.5, actor 3 leaves and time 1.4 then
rejoins at time 2.3, actor 4 joins at time 2.4. All intervals are treated as
closed.
\end{enumerate}
\subsection{Continuing the estimation}
\begin{enumerate}
\item Below you will see some points about how to evaluate the reliability of
  the results.  If the convergence of the algorithm is not quite satisfactory
  but not extremely poor, then you can continue just by \textsf{Apply}ing the
  estimation algorithm again.
\item If the parameter estimates obtained are very poor (not in a reasonable
  range), then it usually is best to start again, with a simpler model, and from
  a standardized starting value.  The latter option must be selected in the
  \textsf{Model Options} screen.
\end{enumerate}
\bigskip

\subsection{Using \SI within \Rn}

There are two alternatives, depending on your familiarity with \Rn.

Section \ref{S_Rscript} presents an example of an \R script
for getting started with \rs.

\smallskip

\subsubsection{For those who are slightly familiar with \Rn}
\label{S_slightlyR}

\begin{enumerate}
\item Install \Rn.
\item Install (within \Rn) the package \rs, and
  possibly \sfn{network} (required to read Pajek files), \sfn{snow} and
  \sfn{rlecuyer} (required to use multiple processors).
\item Set the working directory of \R appropriately (\sfn{setwd()} within \Rn
 or via a desktop shortcut).
\item You can get help by the command
\begin{verbatim}
help(RSiena)
\end{verbatim}
      In \R version 2.10 this will open a browser window with help information;
      by clicking on the `Index' link in the bottom line of this window,
      you get a window with all  \RS commands.\\
      The command
\begin{verbatim}
RShowDoc("s_man400", package="RSiena")
\end{verbatim}
      opens the official \RS manual.
\item Create a session file using \sfn{siena01Gui()} within \Rn, or using an
  external program.
\item Then, within \Rn,
\begin{enumerate}
\item Use \sfn{sienaDataCreateFromSession()} to create your data objects.
\item Use \sfn{getEffects()} to create an effects object.
\item Use \sfn{fix()} to edit the effects object and select the required
  effects, by altering the \sfn{Include} column to \sfn{TRUE}.
\item Use \sfn{sienaModelCreate()} to create a model object.
\item Use \sfn{siena07()} to run the estimation procedure.
\end{enumerate}
Basic output will be written to a file. Further output can be obtained by using
the\\ \sfn{verbose=TRUE} option of \sfn{siena07}.
\end{enumerate}
\subsubsection{For those fully conversant with \Rn}

\begin{enumerate}
\item Add the package \RS
\item Get your network data (including dyadic covariates)
   into matrices, or sparse matrices of type
  \sfn{dgTMatrix}. \sfn{spMatrix()} (in package \sfn{Matrix}) is useful to
  create the latter.
\item Covariate data should be in vectors or matrices.
\item All missing data should be set to NA.
\item Create \SI objects for each network, behavior variable and covariate,
  using the functions \sfn{sienaNet()} (for both networks and behavior
  variables), \sfn{coCovar()} etc.
\item Create a \SI data object using \sfn{SienaDataCreate()}.
\item Use \sfn{getEffects()} to create an effects object.
\item Use \sfn{fix()} to edit the effects object and select the required
  effects. Alternatively use normal \R commands to change the effects object: it
  is just a data frame.
\item Use \sfn{sienaModelCreate()} to create a model object.
\item Use \sfn{siena07()} to run the estimation procedure.
\item Note that it is possible to use multiple processes in \sfn{siena07}. For
  details see section~\ref{S_multipleProcesses}.
\item Also note the availability of the parameter \sfn{prevAns} to reuse
  estimates and derivatives from a previous run with the same effects.
\end{enumerate}
Basic output will be written to a file. Further output can be obtained by using
the \sfn{verbose=TRUE} option of \sfn{siena07}.

\subsubsection{An example \R script for getting started}
\label{S_Rscript}

The following is an example \R script,
which one may use to get started with \rs.

\begin{verbatim}
#####################################GENERAL###################################

# This is an R script for getting started with RSiena, written by
# Robin Gauthier, Tom Snijders, and Ruth Ripley.
# Lines starting with # are not processed by R but treated as comments.

# R is case sensitive.

# Help within R can be called by typing a question mark and the name of the
# function you need help with. For example ?library loading will bring up a
# file titled "loading and listing of packages".
# Comments are made at the end of commands, or in lines staring with # telling
# R to ignore everything beyond it.
# This session will be using s50 data which are supposed to be
# present in the working directory.
# Note that any command in R is called a function;
# in general the command syntax for calling R's functions is function(x) where
# function is a saved function and x the name of the object to be operated on.

####################CALLING THE DATA AND PRELIMINARY MANIPULATIONS#############

# The library command loads the packages needed during the session.

        library(RSiena)
        library(snow) # (these three additional libraries will be loaded
        library(network)# automatically if required)
        library(rlecuyer)
        library(xtable)

# Where are you?

         getwd()

# By something like setwd('C:/SienaTest') you can set the directory
# but note the quotes and forward slash. Also possible to set the directory
# using the menus if you have them.

# What is there?

         list.files()

# What is available in RSiena?

         ?RSiena

# Where is the manual?

         RShowDoc("s_man400", package="RSiena")

# (Note, however, that it is possible that the Siena website
# at http://www.stats.ox.ac.uk/~snijders/siena/ contains a more recent version.)

# The data is named (for example I name it friend.data.w1) so that we can call
# it as an object within R.
# If you read an object straight into R, it will treat it as a
# dataset, which is not what we want because it will generally be harder to work
# with than a matrix (unless you want it to be a dataset (i.e. non-network data).
# R will read in many data formats, these are saved as .dat files, the command
# to read them is read.table if we wished to read a .csv file we would have
# used the read.csv command.
# The pathnames have forward slashes, or double backslashes
# if single backslashes are used, one of the error messages will be:
#   1: '\R' is an unrecognized escape in a character string


        friend.data.w1 <- as.matrix(read.table("s50-network1.dat"))
        friend.data.w2 <- as.matrix(read.table("s50-network2.dat"))
        drink <- as.matrix(read.table("s50-alcohol.dat"))

# Before we work with the data, we want to be sure it is correct. A simple way
# to check that our data is a matrix is the command class()

        class(friend.data.w1)

# To check that all the data has been read in, we can use the dim() command. The
# matrix should have the same dimensions as the original data (here, 50 by 50).

        dim(friend.data.w1)

# To check the values are correct, including missing values, we can use

        table(friend.data.w1, useNA='always')

# We do the same for the changing covariate that I have labelled "drink". Unlike
# the two matrices it should be 50 by 3 because there are three time points in
# the data, although we will only work with two (we are only working with
# two adjacency matrices.

        dim(drink)

# to create NA's, use eg:

        friend.data.w1[friend.data.w1 %in% c(6,9)] <- NA

# to select a subset of the data based on an actor variable, say,
# (the possibilities are endless, but hopefully this will serve as a pattern)
#
        use <- drink[, 1] %in% c(2, 3)
        friend1.data.w1 <- friend.data.w1[use, use]
        friend1.data.w2 <- friend.data.w2[use, use]
        drink1 <- drink[use, ]

####################FROM VECTORS AND MATRICES TO SIENA OBJECTS##################

# A number of objects need to be created in R, as preparations to letting Siena07
# execute the estimation.
# sienaModelCreate creates a control object which can be used as an argument for Siena07
# You can look in the RSiena help files, requested by typing ?RSiena,
# to find out about options that you may use here;
# for beginning users, only the two options mentioned below are relevant.
#
# Output will be written to a file with name projname.out, where projname is
# whatever name is given; the default (used if no name is given) is Siena.
# This file will be written to your current directory.
# New estimation runs will append to it.
# A new call to print01Report will overwrite it!

        mymodel <- sienaModelCreate(useStdInits = TRUE, projname = 's50_2')

# sienaNet creates a Siena network object from a matrix or array or list of sparse
# matrix of triples.
# The name of this network object (here: friendship) will be used
# in the output file.
        friendship <- sienaNet(array(c(friend.data.w1, friend.data.w2), dim=c(50, 50, 2)))
# the integers in the dim() here refer to the number of nodes (senders,
# receivers) and the number of waves.
#
# sienaNet is also used to create a behavior variable object
# with the extra argument type="behavior"
# e.g  using the drinking behavior matrix
        alcohol <- sienaNet(drink, type="behavior")
# (but only use the variable once: behavior variable or changing covariate!)

# To create bipartite network objects you need two nodesets and must create
# the node sets too eg:

        friendship <- sienaNet(array(c(friend.data.w1, friend.data.w2),
                               dim=c(50, 50, 2)), nodeSet=c("senders", "receivers"))

        senders <- sienaNodeSet(50, nodeSetName="senders")
        receivers <- sienaNodeSet(50, nodeSetName="receivers")
        mydata <- sienaDataCreate(friendship, alcohol,
        nodeSets=list(senders, receivers))

# varCovar creates a changing covariate object from a matrix;
# the name comes from 'varying covariate'. We are only using
# two waves of data, so we only want drinking behavior at time 1 and 2, the
# first two columns of the data. The brackets slice the data into the first two
# columns while the comma indicates that R should read all of the rows.
# Omitting the [,1:2] will lead to the same result, as
# RSiena drops an unnecessary final column automatically.
# The name (alcohol) again will be used in the output file.


        alcohol <- varCovar(val = drink[,1:2])

# sienaDataCreate creates a Siena data object from input networks,
# covariates and composition change objects.

        mydata <- sienaDataCreate(friendship,alcohol)

# If you would like to use different names, you could request this as follows:
        mydata <- sienaDataCreate(nominations = friendship, drinking = alcohol)

# This finishes the data specification. Now we have to specify the model.

# getEffects creates a dataframe of effects

        myeff <- getEffects(mydata)

# A basic report of data input, which serves as a check and also contains
# a number of descriptives, can be obtained as follows.
# It produces a file named 'modelname.out' in the current working directory.

        print01Report(mydata,myeff, modelname = 's50_2_init')

# fix calls a data editor, so we can manually edit the effects as in the Gui

        fix(myeff)

# fix() may not be usable if you do not have tcl/tk available

# Alternatively we can edit the dataframe directly by using R functions.
# Note that the columns of the dataframe of effects have names indicated
# in the top of the dataframe:
# name, effectName, type, include, fix, test, initialValue, parm,
# effectnumber, effect1, effect2, effect3.
# The commands below are used to set "include" to TRUE or FALSE,
# as an alternative to using the data editor.
# TRUE or FALSE will always be located at the 9th column,
# transitive triplets will not always be at the 11th row as this depends
# on the number of periods; further, the list of available effects
# may change in future versions.
# In general the advantage of this method is
# that we can save the last parameters and rerun the model
# later without opening the editor. (Saving can now be done in the GUI).
# Note: These row numbers may not be the current ones, as they depend on the
# list of effects implemented, which is changeable.

        #myeff[11,9] <- TRUE   #transitive triples
        #myeff[15,9] <- TRUE   #3 cycles
        #myeff[17,9] <- TRUE   #transitive ties
        #myeff[27,9] <- TRUE   #indegree popularity
        #myeff[31,9] <- TRUE   #outdegree popularity
        #myeff[34,9] <- TRUE   #indegree based activity
        #myeff[36,9] <- TRUE   #outdegree based activity
        #myeff[46,9] <- TRUE   #indegree-indegree assortativity
        #myeff[48,9] <- TRUE   #alcohol alter
        #myeff[50,9] <- TRUE   #alcohol alter (squared)
        #myeff[52,9] <- TRUE   #alcohol ego
        #myeff[54,9] <- TRUE   #alcohol similarity
        #myeff[62,9] <- TRUE   #alcohol ego x alcohol alter

# Alternatively, and more robustly against future changes in the structure,
# use the following for the last effect:

        #myeff[myeff$effectName=='alcohol ego x alcohol alter' &
        #myeff$type=='eval', 'include']=TRUE

# and similarly for the earlier ones.
# To specify an interaction between say alcohol and reciprocity,
# where 69 is the row number of an unspecified interaction effect,
# and 52 and 9 are now used as the numbers of the effects
# that get the roles of effect1 and effect2,
# meaning that they are to be interacted, the following can be used;
# the name will be created by siena07.

        #myeff[69, c('effect1', 'effect2')] <- c(52, 9)
        #myeff[69, 'include'] <- TRUE


# new, more robust, methods to amend an effects object:

        #myeff <- includeEffects(myeff, transTrip, cycle3, between)
        #myeff <- includeInteraction(myeff, egoX, recip,
        #   interaction1="alcohol")
        #myeff <- setEffect(myeff, outInv, 3) ##sets parameter

# There is a table of short names etc available as pdf via

        #RShowDoc("effects", package="RSiena")

# or as html by running the function

        #effectsDocumentation()

# siena07 actually fits the specified model to the data

        ans <- siena07(mymodel, data=mydata, effects=myeff, batch=FALSE, verbose=TRUE)

# By using various different effects objects, you can switch between specifications.
# The batch=FALSE parameters will give a graphical user interface being opened;
# verbose=TRUE leads to diagnostic information being sent to the console
# during the estimation, and results after the estimation
# (these results are also copied to the output file projname.out, mentioned above);
# while batch=TRUE gives only a limited amount of printout sent to the console
# during the estimation (which is seen when clicking in the console,
# or more immediately if the Buffered Output is deselected in the Misc menu)
# which helps monitor the progress of the estimation.

# To use multiple processors, in the simplest case where your computer has 2
# processors, use
         ans <- siena07(mymodel, data=mydata, effects=myeff, batch=FALSE,
                        verbose=TRUE, nbrNodes=2, useCluster=TRUE, initC=TRUE)

# Adjust the nbrNodes to the number available. If you wish to use other machines
# as well, see the more detailed instructions below. You will need to use the
# clusterString argument as well.
#
# If you wish the fitted object to include the simulated networks, use the
# parameter returnDeps=TRUE. The fitted object will then have a component named
# sims which will contain a list (each iteration) of lists (each data object)
# of lists (each dependent network or behavior variable) of edgelists for
# networks or vectors for behavior variables.
#
# This option would require rather a lot of communication between multiple
# processes so it might be better to avoid using the two options together.
#
#################################################################################

# Depending on the random seed, the results could be something like the following.

#Rates and standard errors

#1 rate basic rate parameter friendship      7.19745  (   1.46778 )
#2 eval outdegree (density)                 -1.64754  (   0.21366 )
#3 eval reciprocity                          2.09008  (   0.38726 )
#4 eval transitive triplets                  0.27810  (   0.16612 )
#5 eval 3-cycles                             0.50407  (   0.37948 )
#6 eval transitive ties                      0.63643  (   0.23843 )
#7 eval indegree - popularity                0.04709  (   0.02693 )
#8 eval outdegree - popularity              -0.26251  (   0.66212 )
#9 eval indegree - activity                 -0.17380  (   0.01324 )
#10 eval outdegree - activity               -0.06880  (   0.06258 )
#11 eval in-in degree^(1/2) assortativity     0.03142  (   0.90979 )
#12 eval alcohol alter                      -0.08973  (   0.13641 )
#13 eval alcohol ego                         0.03142  (   0.10044 )
#14 eval alcohol similarity                  1.10065  (   0.72948 )

# With function siena07 we made ans as the object containing
# all the results of the estimation. For example,

        ans$theta

# contains the vector of parameter estimates while

        ans$covtheta

# contains the covariance matrix of the estimates.
# There are several `methods' available for the object containing the results of
# the estimation.

        ans

# will produce a short table.

        summary(ans)

# will produce a longer report, and

        xtable(ans)

# will produce a table formatted for inclusion in a LaTeX document
# or formatted in html. Use e.g.

        xtable(ans, type='html')

# to get html, and e.g.

        xtable(ans, file='ff.tex')

# to write the results to a file.

# The option useStdInits = TRUE, used above in sienaModelCreate, will make
# each estimation run start with standard initial values.
# If you wish to start the next estimation with the results
# produced by the previous estimation, first change this option:

        mymodel$useStdInits <- FALSE

# and then initialise the next estimation by the current results.
# If you used unconditional estimation (as here was the default), then request:

        myeff$initialValue[myeff$include] <- ans$theta

# and if you used conditional estimation, conditional on the first network:
        myeff$initialValue[myeff$include] <- c(ans$rate, ans$theta)

# By using a different vector instead of ans$theta you can
# initialise differently.
# Note that this initial vector will be used until you change it again,
# e.g., to the results of a new run,
# or until you change the useStdInits option.

# You can use the prevAns= option in siena07 to supply the result of a
# previous run from which to extract the theta estimates and the
derivatives. Phase 1 will be omitted in this case.

#######################VIEWING THE NETWORK IN R#######################

# We can make connections with other R packages, e.g., Carter Butts's
# sna (Social Network Analysis) package.
# This package is documented in
# Carter T. Butts, Social Network Analysis with sna,
# Journal of Statistical Software Vol. 24, Issue 6, May 2008
# http://www.jstatsoft.org/v24/i06
# Also see,
# Carter T. Butts, network: A Package for Managing Relational Data in R
# Journal of Statistical Software Vol. 24, Issue 2, May 2008
# http://www.jstatsoft.org/v24/i02
# Here we demonstrate the use of sna for plotting.

        library(sna)

# First we must make the data available in a network format for plotting.
# The function as.network will convert a matrix to a network object.

# NB this command needs the network package loaded (library(network))
net1 <- as.network(friend.data.w1)

# The command plot will visualize the network for you according to the defaults

        plot(net1)

# The plot function is part of the network package, and you can find the
# documentation by requesting ?network and then looking for plot.network
# or ?plot.network

# Now the same for the second network to the network at the second time period:

        net2 <- as.network(friend.data.w2)
        plot(net2)

# You might try to add the parameter  interactive=TRUE
# which will allow to change vertex positions in the plot.

# We can also color nodes by attributes
# First we must add the node values to the network.
# The %v% operator, documented in the ?network help files, does this.

        net1 %v% "drink1" <- drink[,1]
        net2 %v% "drink2" <- drink[,2]

# Now we can color the node by alcohol attribute.
# In addition we make the arrowheads and nodes a bit larger.

        plot(net1, vertex.col="drink1", object.scale = 0.012, arrowhead.cex=1.1)
        plot(net2, vertex.col="drink2", object.scale = 0.012, arrowhead.cex=1.1)

# Each value of the discrete value of the covariate drink is given a different
# color and we can see if there are clear trends toward homophily in either
# time point.

# We can see that in time one there is one girl holding the groups together,
# and we may wish to know which respondent she is.
# This command simply pulls the id from the nodes in the network:

        plot(net1,label=network.vertex.names(net1), boxed.labels=FALSE)

# If you do not like the place where the labels are put, look in the help file
# at labels.pos and try label.pos = 1, 2, 3, 4, or 5.

# If we want to know how much she drinks, we'll put the commands together:

        plot(net1,vertex.col="drink1",label=network.vertex.names(net1),
             boxed.labels=FALSE, object.scale = 0.012)

# for the network at time two

        plot(net2,vertex.col="drink2",label=network.vertex.names(net2))

# Each time we make a plot the coordinates move - because always
# the starting values are random. We can also save coordinates
# and use them for later plotting:

        coordin1 <- plot(net1, vertex.col="drink1", object.scale = 0.012, arrowhead.cex=1.1)
        plot(net2, coord = coordin1, vertex.col="drink2", object.scale = 0.012, arrowhead.cex=1.1)

# The second plot is not so nice as the first - not surprisingly.
# Another option is to determine the coordinates from both networks together.
# See the "Value" entry in the help file of plot in package network.

        net12 <- net1 + net2
        coordin12 <- plot(net12)
        plot(net1, coord = coordin12, vertex.col="drink1", object.scale = 0.012, arrowhead.cex=1.1)
        plot(net2, coord = coordin12, vertex.col="drink2", object.scale = 0.012, arrowhead.cex=1.1)

# There are many other functions in sna that may be useful.
# The following is an example: see the documentation mentioned above for more.
# evcent is the Bonacich eigenvector centrality.

        triad.census(net1)
        betweenness(net1)
        evcent(net1)
\end{verbatim}

\subsection{Outline of estimation procedure}
\noindent
\SI estimates parameters by the following procedure:
\begin{enumerate}
\item  Certain statistics are chosen that should reflect the parameter values;\\
  the finally obtained parameters should be such that the \emph{expected
    values}
  of the statistics are equal to the \emph{observed values}.\\
  Expected values are approximated as the averages over a lot of simulated
  networks.\\
  Observed values are calculated from the data set. These are also called the
  \emph{target values}.
\item To find these parameter values, an \emph{iterative stochastic simulation
    algorithm}
  is applied.\\
  This works as follows:
\begin{enumerate}
\item In Phase 1, the sensitivity of the statistics to the parameters is roughly
  determined.
\item In Phase 2, provisional parameter values are updated:\\
  this is done by simulating a network according to the provisional parameter
  values, calculating the statistics and the deviations between these simulated
  statistics and the \emph{target values}, and making a little change (the
  `update') in the parameter values
  that hopefully goes into the right direction.\\
  (Only a `hopefully' good update is possible, because the simulated network is
  only a random draw from the distribution of networks, and not the expected
  value itself.)
\item In Phase 3, the final result of Phase 2 is used, and it is checked if the
  average statistics of many simulated networks are indeed close to the target
  values. This is reflected in the so-called \texttt{t statistics for deviations
    from targets}.
\end{enumerate}
\end{enumerate}

\subsection{Using multiple processes}
\label{S_multipleProcesses}
\begin{enumerate}
\item
If multiple processors are available, then using
multiple processes can speed up the estimation in \sfn{siena07}.

\item In Phases 1 and 3 the simulations are performed in parallel. In Phase 2,
  multiple simulations are done with the same parameters, and the resulting
  statistics are averaged. The gain parameter is increased and the minimum numb
  er of iterations in phase 2 reduced to take advantage of
  the increased accuracy.

\item The parameters required to run all processes on one computer are fairly
  simple: in your call to \sfn{siena07}, set \sfn{nbrNodes} to the number of
  processes and \sfn{useCluster} and \sfn{initC} to TRUE. The \sfn{Model
    Options} screen also allows you to specify the number of processors, and
  will automatically set the other required parameters for you.

\item To use more than one machine is more complicated, but it can be done by
  using, in addition, the \sfn{clusterString} parameter.  The computers need to
  be running incoming \sfn{ssh}.
\item For machines with exactly the same layout of \R
  directories on each, simply set \sfn{clusterString} to a character vector of
  the names of the machines.
\item For other cases, e.g.\ using Macs alongside Linux,
  see the documentation for the package \sfn{snow}.

\item Currently \RS uses sockets for inter-process communication.
\item Each process needs a copy of the data in memory. If there is insuffient
  memory available there will be no speed gain as too much time will be spent
  paging.
\item In each iteration the main process waits until all the other processes
  have finished. The overall speed is therefore that of the slowest process, and
  there should be enough processors to allow them all to run at speed.
\end{enumerate}
\subsection{Steps for looking at results: Executing \si .}
\label{S_exec}

\begin{enumerate}
\item Look at the start of the output file for general data
      description (degrees, etc.), to check your data input.
    \item When parameters have been estimated, first look at the \texttt{t
        ratios for deviations from targets}.  These are good if they are all
      smaller than 0.1 in absolute
      value, and reasonably good if they are all smaller than 0.2.\\
      We say that the algorithm has converged if they are all smaller than 0.1
      in absolute value, and that it has nearly converged if they are all
      smaller than 0.2.\\
      These bounds are indications only, and may be taken with a grain of
      salt.\\

      \smallskip

\item The \textsf{Initial value of gain parameter} determines the
      step sizes in the parameter updates in the iterative
      algorithm.
      A too low value implies that it takes very long to attain a
      reasonable parameter estimate when starting from an initial
      parameter value that is far from the `true' parameter estimate.
      A too high value implies that the algorithm will be unstable,
      and may be thrown off course into a region of unreasonable
      (e.g., hopelessly large) parameter values.\\
      It usually is unnecessary to change
      this.
    \item If all this is of no avail, then the conclusion may be that the model
      specification is incorrect for the given data set.
    \item Further help in interpreting output is in Section~\ref{S_output} of
      this manual.
\end{enumerate}

\newpage
\subsection{Giving references}

When using \si, it is appreciated that you refer to this manual and to one or
more relevant references of the methods implemented in the program.  The
reference to this manual is the following.  \smallskip

\noindent
Ripley, Ruth M., and Snijders, Tom A.B.
2010.
Manual for SIENA version 4.0 (provisional version, \today).
Oxford: University of Oxford, Department of Statistics; Nuffield College.
\textsf{http://www.stats.ox.ac.uk/siena/}

\smallskip

A basic reference for the network dynamics model is \citet{Snijders01}
or \citet{Snijders05}.
Basic references for the model of network-behavior co-evolution
are \citet*{SnijdersEA07} and \citet*{SteglichEA10}.

More specific references are \citet{Schweinberger10} for the score-type goodness
of fit tests and \citet{SchweinbergerSnijders07a} for the calculation of
standard errors of the Method of Moments estimators .

A tutorial is \citet*{SnijdersEA10b}.


\subsection{Getting help with problems}
\label{sec:problems}
If you have a problem running \rs, please read through the following hints to
see if any of them help. If not, please send an email to
rsiena-help@lists.r-forge.r-project.org, or post in the help forum for \RS in
R-forge. You need to be a registered member of R-forge (and possibly of \rs)
to post to a forum, but anyone can send emails (at present!). In your message,
please tell us which operating system , which version of \Rn, and which version
of \RS you are using.

For queries about the modelling aspects of \si, or interpretation, please
continue to use the \SN/ \RS mailing list.


\begin{description}
\item[Check your version of \RS] Details of the latest version available can
  be found at \url{http://r-forge.r-project.org/R/?group_id=461}. The version is
  identified by a version number (e.g.\ 1.0.9) and an R-forge revision
  number. You can find both the numbers of your current installed version by
  opening \R, and typing \verb|packageDescription("RSiena")|. The version is
  near the top, the revision number near the end. Both are also displayed at the
  start of \SI output files (use \sfn{print01Report} to get the relevant output
  file if you are not using the gui.)
\item[Check your version of \Rn] When there is a new version or revision of \RS
  it will only be available to you automatically if you are running the most
  recent major version of \Rn, currently 2.10. (You can force an installation if
  necessary by downloading the tarball or binary and installing from that, but
  it is better to update your \Rn.)
\item [Check both repositories] We have two repositories in use for \rs: CRAN
  and R-forge. The latest version will always be available from
  R-forge. (Frequent updates are discouraged on CRAN, so bug-fixes are likely to
  appear first on R-forge.)
\item[Installation] When using the repository at R-forge, \emph{install} the
  package rather than updating it. Then check the version and revision numbers.
\end{description}


\newpage
\begin{print}
%\addtocontents{toc}{\vspace{-2ex}}  % hack to avoid a 3-page table of contents
\part{User's manual}
\end{print}
\begin{screen}
{\color{section0}\LARGE\bf\noindent
Part II\\[1.5ex] User's manual\\[1.8ex]}
\end{screen}


\begin{print}
%\newpage
\end{print}

\section[Program parts]{Parts of the program}
\label{S_parts}

The operation of the \SI program is comprised of four main parts:
\begin{enumerate}
 \item input of basic data description,
 \item model specification,
 \item estimation of parameter values using stochastic simulation,
 \item simulation of the model with given and fixed parameter values.
\end{enumerate}

The normal operation is to start with data input, then specify a
model and estimate its parameters, and then continue with new
model specifications followed by estimation or simulation. For the
comparison of (nested) models, statistical tests can be carried out.


The main output is written to a text file named \textsf{{\em
pname}.out}, where \textsf{{\em pname}} is the root name of the
file specifying the data files (if any).

\begin{print}
\newpage
\end{print}

\section{Input data}
\label{S_InputData}

The main statistical method implemented in \SI  is for the analysis
of repeated measures of social networks, and requires network data
collected at two or more time points. It is possible to include
changing actor variables (representing behavior, attitudes,
outcomes, etc.) which also develop in a dynamic process, together
with the social networks.
As repeated measures data on social networks, at the very least, {\em two or more data
files with digraphs} are required: the observed networks, one for
each time point. The number of time points is denoted $M$.


In addition, various kinds of variables are allowed:

\begin{enumerate}
\item {\em actor-bound} or {\em individual variables},
      also called {\em actor attributes},
      which can be symbolized as $v_i$ for each actor $i$;
      these can be constant over time or changing; \\
      the changing individual variables can be dependent variables
      (changing dynamically in mutual dependence with the changing network)
      or independent variables (exogenously changing variables;
      then they are also called individual covariates).
\item {\em dyadic covariates}, which can be symbolized as $w_{ij}$
      for each ordered pair of actors $(i,j)$;
%they are allowed only
%      to have integer values ranging from 0 to 255.
%      If one has real-valued dyadic covariates, then one option
%      is to multiply them e.g. by 10 or 100 so that they still have a range
%      between 0 and 255, and used the rounded values.
     these likewise can be constant over time or changing.
\end{enumerate}


All variables must be available in ASCII (`raw text') data files, described in
detail below. It is best to use the `classical' type of filenames, without embedded blanks
and not containing special characters.
These files, the names of the corresponding
variables, and the coding of missing data, must be made available
to \si.

Names of variables must be composed of at most 12 characters. This
is because they are used as parts of the names of effects which
can be included in the model, and the effect names should not be
too long.

\begin{screen}
% \newpage
\end{screen}
\subsection{Digraph data files}

Each digraph must be contained in a separate input file.  Two data formats are
allowed currently.  For large number of nodes (say, larger than 100), the Pajek
format is preferable to the adjacency matrix format.  For more than a few
hundred nodes,
\begin{enumerate}
\item \emph{Adjacency matrices}.\\
      The first is an adjacency matrix, i.e., $n$ lines each with $n$ integer
      numbers, separated by blanks or tabs, each line ended by a hard return.
      The diagonal values are meaningless but must be present.

      Although this section talks only about digraphs (directed graphs), it is
      also possible that all observed adjacency matrices are symmetric.
      This will be automatically detected by \si, and
      the program will then utilize methods for non-directed networks.

      The data matrices for the digraphs
       must be coded in the sense
      that their values are converted by the program to the 0 and 1
      entries in the adjacency matrix. A set of code numbers is required
      for each digraph data matrix; these codes are regarded as the
      numbers representing a present arc in the digraph, i.e., a 1 entry
      in the adjacency matrix; all other numbers will be regarded as 0
      entries in the adjacency matrix. Of course, there must be at least
      one such code number. All code numbers must be in the range from 0
      to 9, except for structurally determined values (see below).

      This implies that if the data are already in 0-1 format, the
      single code number 1 must be given. As another example, if the
      data matrix contains values 1 to 5 and only the values 4 and 5 are
      to be interpreted as present arcs, then the code numbers 4 and 5
      must be given.
    \item \emph{Pajek format}.\\
      If the digraph data file has extension name \texttt{.net}, then the
      program assumes that the data file has Pajek format.  The format required
      differs from that in the previous versions of \SI.  The file should relate
      to one observation only, and should contain a list of vertices (using the
      keyword \texttt{*Vertices}, together with (currently) a list of arcs,
      using the keyword \texttt{*Arcs}
      followed by data lines according to the Pajek rules.
      These keywords must be in lines that contain no further characters.
      An example of such input files is given in the s50 data set
      that is distributed in the \texttt{examples} directory.
    \item \emph{Siena format}.\\
      An edge list containing three or four columns:
      from, to, value, wave (optional).\\
      Like the Pajek format, this has the advantage that absent ties
      (tie variables with the value 0) do not need to be mentioned
      in the data file.
\end{enumerate}

Code numbers for missing numbers also must be indicated -- in the case of either
input data format. These
codes must, of course, be different from the code numbers
representing present arcs.

Although this section talks only about digraphs (directed graphs), it is
also possible that all observed ties (for all time points) are mutual.
This will be automatically detected by \si, and
the program will then utilize methods for non-directed networks.

If the data set is such that it is never observed that ties are terminated,
then the network dynamics is automatically specified internally in such a way
that termination of ties is impossible.
(In other words, in the simulations of the actor-based model
the actors have only the option to create new ties or to retain
the status quo, not to delete existing ties.)


\subsubsection{Structurally determined values}
\label{S_struct}

It is allowed that some of the values in the digraph are
structurally determined, i.e., deterministic rather than random.
This is analogous to the phenomenon of `structural zeros' in
contingency tables, but in \SI not only structural zeros but also
structural ones are allowed. A structural zero means that it is
certain that there is no tie from actor $i$ to actor $j$; a
structural one means that it is certain that there is a tie. This
can be, e.g., because the tie is impossible or formally imposed,
respectively.

Structural zeros provide an easy way to deal with actors leaving
or joining the network between the start and the end
of the observations. Another way
(more complicated but it gives possibilities to represent actors
entering or leaving at specified moments between observations)
is described in Section~\ref{S_comp}.

Structurally determined values are defined by reserved codes in
the input data: the value 10 indicates a structural zero, the
value 11 indicates a structural one. Structurally determined
values can be different for the different time points. (The
diagonal of the data matrix always is composed of structural
zeros, but this does not have to be indicated in the data matrix
by special codes.) The correct definition of the structurally
determined values can be checked from the brief report of this in
the output file.
%, and by looking at the file \textsf{\em pname}.s01
%(for the first time point), \textsf{\em pname}.s02 (second time
%point), etc. In these files, the structurally determined positions
%(structural zeros as well as structural ones) are indicated by the
%value 1, all others (i.e., the positions where ties are random) by
%the value 0.

Structural zeros offer the possibility of analyzing several
networks simultaneously under the assumption that the parameters
are identical.
Another option to do this is given in Section~\ref{S_mulev}.
E.g., if there are three networks with 12, 20 and
15 actors, respectively, then these can be integrated into one
network of 12 + 20 + 15 = 47 actors, by specifying that ties
between actors in different networks are structurally impossible.
This means that the three adjacency matrices are combined in one
$47 \times 47$ data file, with values 10 for all entries that
refer to the tie from an actor in one network to an actor in a
different network. In other words, the adjacency matrices will be
composed of three diagonal blocks, and the off-diagonal blocks
will have all entries equal to 10. In this example, the number of
actors per network (12 to 20) is rather small to obtain good
parameter estimates, but if the additional assumption of identical
parameter values for the three networks is reasonable, then the
combined analysis may give good estimates.

In such a case where $K$ networks (in the preceding paragraph, the
example had $K = 3$) are combined artificially into one bigger
network, it will often be helpful to define $K-1$ dummy variables
at the actor level to distinguish between the $K$ components.
These dummy variables can be given effects in the rate function
and in the evaluation function (for ``ego"), which then will
represent that the rate of change and the out-degree effect are
different between the components, while all other parameters are
the same.

It will be automatically discovered by \SI when functions
depend only on these components defined by structural zeros,
between which tie values are not allowed.
For such variables, only the ego effects are defined
and not the other effects defined for the regular
actor covariates and described in Section ~\ref{S_eff_cov}.
This is because the other effects then are meaningless.
If at least one case is missing (i.e., has the missing value data code
for this covariate),
then the other covariate effects are made available.

When \SI simulates networks including some structurally determined values,
if these values are constant across all observations then
the simulated tie values are likewise constant.
If the structural fixation varies over time, the situation
is more complicated.
Consider the case of two consecutive observations
$m$ and $m+1$,
and let $X^{\text{sim}}_{ij}$ be the simulated value
at the end of the period from $t_m$ to $t_{m+1}$.
If the tie variable $X_{ij}$ is structurally fixed at time $t_m$
at a value $x_{ij}(t_m)$,
then $X^{\text{sim}}_{ij}$ also is equal to $x_{ij}(t_m)$,
independently of whether this tie variable is structurally fixed
at time $t_{m+1}$ at the same or a different value or not at all.
This is the direct consequence of the structural fixation.
On the other hand, the following rule is also used.
If $X_{ij}$ is \emph{not} structurally fixed at time $t_m$
but it is structurally fixed at time $t_{m+1}$ at some value $x_{ij}(t_{m+1})$,
then in the course of the simulation process from  $t_m$ to $t_{m+1}$
this tie variable can be changed as part of the process in the usual way,
but after the simulation is over and before the statistics are calculated it will be fixed
to the value $x_{ij}(t_{m+1})$.

The target values for the algorithm of the Method of Moments estimation
procedure are calculated for all observed digraphs $x(t_{m+1})$.
However, for tie variables $X_{ij}$ that are
structurally fixed at time $t_m$, the observed value  $x_{ij}(t_{m+1})$
is replaced by the structurally fixed value  $x_{ij}(t_{m})$.
This gives the best possible correspondence between target values
and simulated values in the case of changing structural fixation.

\subsection{Dyadic covariates}

As the digraph data, also each measurement of a dyadic covariate
must be contained in a separate input file with a square data
matrix, i.e., $n$ lines each with $n$ integer numbers, separated by
blanks or tabs, each line ended by a hard return. The diagonal values are
meaningless but must be present.
Pajek input format is currently not possible for dyadic covariates.

A distinction is made between constant and changing dyadic
covariates, where change refers to changes over time. Each constant
covariate has one value for each pair of actors, which is valid for
all observation moments, and has the role of an independent
variable. Changing covariates, on the other hand, have one such
value for each period between measurement points. If there are $M$
waves of network data, this covers $M-1$ periods, and accordingly,
for specifying a single changing dyadic covariate, $M-1$ data files
with covariate matrices are needed.

% The reasons for restricting dyadic covariates to integer values from
% 0 to 255 are historical and have to do with how the constant dyadic
% covariate data are stored internally. If the user wishes to use a
% dyadic covariate with a different range, this variable first must be
% transformed to integer values from 0 to 255. E.g., for a continuous
% variable ranging from 0 to 1, the most convenient way probably is to
% multiply by 100 (so the range becomes 0--100) and round to integer
% values.

The mean is always subtracted from the covariates.
See the section on \hyperlink{T_S_centering}{\emph{Centering}}.

\subsection{Individual covariates}

Individual (i.e., actor-bound) variables can be combined in one or
more files. If there are $k$ variables in one file, then this data
file must contain $n$ lines, with on each line $k$ numbers which all
are read as real numbers (i.e., a decimal point is allowed). The
numbers in the file must be separated by blanks and each line must
be ended by a hard return. There must not be blank lines after the
last data line.

Also here, a distinction is made between constant and changing actor
variables. Each constant actor covariate has one value per actor
valid for all observation moments, and has the role of an
independent variable.

Changing variables can change between observation moments. They
can have the role of dependent variables (changing dynamically in
mutual dependence with the changing network) or of independent
variables; in the latter case, they are also called `changing
individual covariates'. Dependent variables are treated in the
section below, this section is about individual variables
in the role of independent variables -- then they are also
called individual covariates.

When changing individual variables have the role of
independent variables, they are assumed to have constant values from one
observation moment to the next. If observation moments for the
network are $t_1, t_2, ..., t_M$, then the changing covariates
should refer to the $M-1$ moments $t_1$ through $t_{M-1}\,$, and
the $m$-th value of the changing covariates is assumed to be valid
for the period from moment $t_m$ to moment $t_{m+1}\,$.
The value at $t_M$, the last moment, does not play a role.
Changing covariates, as independent variables, are meaningful
only if there are 3 or more observation moments,
because for 2 observation moments the distinction between
constant and changing covariates is not meaningful.

Each changing individual covariate must be given in one file,
containing $k = M-1$ columns that correspond to the $M-1$ periods
between observations.
It is not a problem if there is an $M$'th column in the file,
but it will not be read.

The mean is always subtracted from the covariates.
See the section on \hyperlink{T_S_centering}{\emph{Centering}}.

When an actor covariate is constant within waves, or constant
within components separated by structural zeros (which means that
ties between such components are not allowed), then only the ego effect
of the actor covariate is made available.
This is because the other effects then are meaningless.
This may cause problems for combining several data sets
in a meta-analysis (see Section ~\ref{S_mulev}).
If at least one case is missing (i.e., has the missing value data code),
then the other covariate effects are made available.
When analysing multiple data sets in parallel,
for which the same set of effects is desired to be included % in
%the .MO file,
it is therefore advisable to give data sets in which
a given covariate has the same value for all actors
one missing value in this covariate;
purely to make
the total list of effects % in the MO file
independent of the observed data.


\subsection{Interactions and dyadic transformations of covariates}

For actor covariates, two kinds of transformations to dyadic covariates
are made internally in \si. Denote the actor covariate by $v_i$,
and the two actors in the dyad by $i$ and $j$.
Suppose that the range of $v_i$ (i.e., the difference between the
highest and the lowest values) is given by $r_V$.
The two transformations are the following:
\begin{enumerate}
\item \emph{dyadic similarity}, defined by $ 1 - \big( \vert v_i - v_j \vert / r_V \big) $,
      and centered so the the mean of this similarity variable becomes 0;\\
      note that before centering, the similarity variable is 1 if
      the two actors have the same value, and 0 if one has the highest and the
      other the lowest possible value;
\item \emph{same $V$}, defined by 1 if $v_i = v_j$,
      and 0 otherwise (not centered) ($V$ is the name of the variable).
      This can also be referred to as \emph{dyadic identity} with respect to $V$.
\end{enumerate}
Dyadic similarity is relevant for variables that can be treated as interval-level
variables; dyadic identity is relevant for categorical variables.


In addition, \SI offers the possibility of user-defined two- and three-variable
interactions between covariates; see Section~\ref{S_int_eff}.


\subsection{Dependent action variables}
\label{S_depaction}

\SI also allows dependent action variables,
also called dependent behavior variables. This can be used in studies
of the co-evolution of networks and behavior, as described
in \citet*{SnijdersEA07} and \citet*{SteglichEA10}.
These action variables represent the actors' behavior, attitudes, beliefs, etc.
The difference between dependent action variables and changing actor
covariates is that the latter change exogenously, i.e., according
to mechanisms not included in the model, while the dependent
action variables change endogenously, i.e.,
depending on their own values and on the changing network.
In the current implementation only one dependent network variable is
allowed, but the number of dependent action variable can be larger than one.
Unlike the changing individual covariates,
the values of dependent action variables are not assumed to be
constant between observations.

Dependent action variables must have nonnegative integer values;
e.g., 0 and 1, or a range of integers like 0,1,2 or 1,2,3,4,5.
Each dependent action variable must be given in one
file, containing $k = M$ columns, corresponding to the $M$
observation moments.

If any values are not integers, a warning will be printed on the initial report
and the values will be truncated towards zero.


\begin{screen}
\newpage
\end{screen}
\subsection{Missing data}

\SI allows that there are some missing data on network variables,
on covariates, and on dependent action
variables. Missing data in changing dyadic covariates are not yet
implemented. Missing data must be indicated by missing data codes,
{\em not} by blanks in the data set.

Missingness of data is treated as non-informative.
One should be aware that having many missing data can seriously
impair the analyses: technically, because estimation will be
less stable; substantively, because the assumption of
non-informative missingness often is not quite justified.
Up to $10\%$ missing data will usually not give many difficulties
or distortions, provided missingness is indeed non-informative.
When one has more than $20\%$ missing data on any variable, however,
one may expect problems in getting good estimates.

In the current implementation of \si, missing data are treated in
a simple way, trying to minimize their influence on the estimation
results.
This method is further explained in \citet{HuismanSteglich08},
where comparisons are also made with other ways of dealings with the missing
information.

The basic idea is the following.
A brief sketch of the procedure is that
missing values are imputed to allow meaningful simulations;
for the calculation of the target statistics in the Method of Moments,
tie variables and actor variables with missings are not
used.
More in detail, the procedure is as follows.

The simulations are carried out over all variables,
as if they were complete.
To enable this, missing data are imputed.
In the initial observation, missing entries in the adjacency
matrix are set to 0,
i.e., it is assumed that there is \emph{no} tie;
this is done because normally data are sparse, so `no tie'
is the modal value of the tie variable.
In the further observations, for any variable,
if there is an earlier observed value of this variable then
the last observed value is used to impute the current
value (the `last observation carry forward' option,
cf.\ \citet{Lepkowski89}; if there is no earlier observed
value, the value 0 is imputed.
For the dependent behavior variables the same principle
is used: if there is a previous observation of the same variable
then this value is imputed, if there is none then the
observationwise mode of the variable is imputed.
Missing covariate data are replaced by the
variable's average score at this observation moment. In the course
of the simulations, however, the adjusted values of the dependent
action variables and of the network variables are allowed to
change.

In order to ensure a minimal impact of missing data treatment on
the results of parameter estimation (method of moments estimation)
and/or simulation runs, the calculation of the target statistics
used for these procedures uses only non-missing data. When
for an actor in a given period, any variable is missing that is
required for calculating a contribution to such a statistic, this
actor in this period does not contribute to the statistic in
question. For network and dependent action variables, the tie variable
or the actor variable, respectively,
must provide valid data both at the beginning and at the end of a
period for being counted in the respective target statistics.

\begin{print}
%\newpage
\end{print}
\subsection{Composition change}
\label{S_comp}

\SI can also be used to analyze networks of which the composition
changes over time, because actors join or leave the network
between the observations.
This can be done in two ways: using the method of \citet{HuismanSnijders03},
or using structural zeros.
(For the maximum likelihood estimation option, the Huisman-Snijders method
is not implemented, and only the structural zeros method can be used.)
Structural zeros can specified for all elements of the tie variables
toward and from actors who are absent at a given observation moment.
How to do this is described in subsection~\ref{S_struct}.
This is straightforward and not further explained here.
This subsection explains the method of Huisman and Snijders
(2003), which uses the information about composition change
in a sightly more efficient way.

For this case, a data file is needed in which the
\emph{times of composition change} are given. For networks with
constant composition (no entering or leaving actors), this file is
omitted and the current subsection can be disregarded.

Network composition change, due to actors joining or leaving the
network, is handled separately from the treatment of missing data.
The digraph data files must contain all actors who are part of the
network at any observation time (denoted by $n$) and each actor
must be given a separate (and fixed) line in these files, even for
observation times where the actor is not a part of the network
(e.g., when the actor did not yet join or the actor already left
the network). In other words, the adjacency matrix for each
observation time has dimensions $n \times n$.

\begin{screen}
\newpage
\end{screen}
At these times, where the actor is not in the network, the entries
of the adjacency matrix can be specified in two ways. First as
missing values using missing value code(s). In the estimation
procedure, these missing values of the joiners before they joined
the network are regarded as 0 entries, and the missing entries of
the leavers after they left the network are fixed at the last
observed values. This is different from the regular missing data
treatment. Note that in the initial data description the missing
values of the joiners and leavers are treated as regular missing
observations. This will increase the fractions of missing data and
influence the initial values of the density parameter.

A second way is by giving the entries a regular observed code,
representing the absence or presence of an arc in the digraph (as
if the actor was a part of the network). In this case, additional
information on relations between joiners and other actors in the
network before joining, or leavers and other actors after leaving
can be used if available. Note that this second option of
specifying entries always supersedes the first specification: if a
valid code number is specified this will always be used.

For joiners and leavers, crucial information is contained in the
times they join or leave the network (i.e., the times of
composition change), which must be presented in a separate input
file, the \emph{exogenous events file}
described in Section~\ref{S_datform}.


\subsection{Centering}

Individual as well as dyadic covariates are
\hypertarget{T_S_centering}{centered}
by the program in the following way.

For individual covariates, the mean value is subtracted
immediately after reading the variables. For the changing
covariates, this is the global mean (averaged over all periods).
The values of these subtracted means are reported in the output.

For the dyadic covariates and the similarity variables derived
from the individual covariates, the grand mean is calculated,
stored, and subtracted during the program calculations. (Thus,
dyadic covariates are treated by the program differently than
individual covariates in the sense that the mean is subtracted at
a different moment, but the effect is exactly the same.)

The formula for balance is a kind of dissimilarity between rows of
the adjacency matrix. The mean dissimilarity is subtracted in this
formula and also reported in the output. This mean dissimilarity
is calculated by a
\hyperlink{T_meanbal}{formula given in Section}~\ref{S_math}.

%The dependent network variable is not centered.

\begin{print}
\newpage
\end{print}
\section{Model specification}
\label{S_modspec}

After defining the data, the next step is to specify a model.\medskip

The model specification consists of a selection of `effects' for
the evolution of each dependent variable (network or behavior).
%  A
% list of all available effects for a given \SI project is given in
% the secondary output file \textsf{{\em pname}.log}.
% A list of all effects in the objective function is given in
% the file \textsf{{\em pname}.eff}.

For the longitudinal case, three types of
effects are distinguished \citep*[see][]{Snijders01, SnijdersEA10b}:

\begin{itemize}
\item {\em rate function effects}\\
The rate function models the speed by which the dependent variable
changes; more precisely: the speed by which each network actor
gets an opportunity for changing her score on the dependent
variable.\\
Advice: in most cases, start modeling with a constant rate function without
additional rate function effects. Constant rate functions are
selected by exclusively checking the `basic rate parameter' (for
network evolution) and the main rate effects (for behavioral
evolution) on the {\sf model specification} screen.
(When there are important size or activity differences between
actors, it is possible that different advice must be given,
and it may be necessary to let the rate function
depend on the individual covariate that indicates this size;
or on the out-degree.)
%See XXXXXXX.
\item {\em evaluation function effects}\\
The evaluation function\footnote{The evaluation function was called
\emph{objective function} in \citet{Snijders01}} models the network actors' satisfaction with their local
network neighborhood configuration. It is assumed that actors
change their scores on the dependent variable such that they
improve their total satisfaction -- with a random element
to represent the limited predictability of behavior.
In contrast to the endowment
function (described below), the evaluation function evaluates only
the local network neighborhood configuration that results from the
change under consideration.
In most applications, the evaluation function will
be the main focus of model selection.\\
The network evaluation function normally should always contain the
`density', or `out-degree' effect, to account for the observed
density. For directed networks,
it mostly is also advisable to include the reciprocity
effect, this being one of the most fundamental network effects.
Likewise, behavior evaluation functions should normally always
contain the shape parameter, to account for the observed
prevalence of the behavior, and
(unless the behavior is dichotomous) the quadratic shape effect,
to account more precisely for the distribution of the behavior.
\item {\em endowment function effects}\\
The endowment function\footnote{The endowment function is similar to the {\it gratification
function} in \citet{Snijders01}} is an extension of the evaluation
function that allows to distinguish between new and old network
ties (when evaluating possible network changes) and between
increasing or decreasing behavioral scores (when evaluating
possible behavioral changes). The function models the loss of
satisfaction incurred when existing network ties are dissolved or
when behavioral scores
are decreased to a lower value (hence the label `endowment').\\
For a number of effects, the endowment function is implemented
not for the Method of Moments estimation method,
but only for Maximum Likelihood and Bayesian estimation.
This is indicated in Section~\ref{S_math}.\\
Advice: start modeling without any endowment effects,
and add them at a later stage.
Do not use endowment effects for behavior unless
the behavior variable is dichotomous.
\end{itemize}

The estimation and simulation procedures of \SI operate on the basis
of the model specification which comprises the set of
effects included in the model as described above,
together with the current
parameter values.
% and the Model Type
%(see Section~\ref{S_modeltype}).
After data input, the constant rate
parameters and the density effect in the network evaluation function
have default initial values, depending on the data. All other
parameter values initially are 0. The estimation process changes
the current value of the parameters to the estimated values.
Values of effects not included in the model are not changed by the
estimation process. It is possible for the user to change
parameter values and to request that some of the parameters are
fixed in the estimation process at their current value.

\subsection{Important structural effects for network dynamics:
           \protect\newline one-mode networks}
\label{S_imp_str1}

For the structural part of the model for network dynamics,
for one-mode (or unipartite) networks,
the most important effects are as follows.
The mathematical formulae for these and other effects are given
in Section~\ref{S_math}. Here we give a more qualitative description.

A default model choice could consist of (1) the out-degree and reciprocity
effects; (2) one network closure effect,
e.g.\ transitive triplets or transitive ties; the 3-cycles effect;
(3) the in-degree popularity effect (raw or square root version);
the out-degree activity effect (raw or square root version);
and either the in-degree activity effect or the out-degree popularity effect
(raw or square root function).
The two effects (1) are so basic they cannot be left out.
The two effects selected under (2) represent the dynamics in local (triadic) structure;
and the three effects selected under (3) represent the dynamics
in in- and out-degrees (the first for the dispersion of in-degrees,
the second for the dispersion of out-degrees, and the third for the
covariance between in- and out-degrees) and also should offer
some protection, albeit imperfect, for potential ego- and alter-effects
of omitted actor-level variables.

The basic list of these and other effects is as follows.

\begin{enumerate}
\item The \emph{out-degree effect} which always must be included.
\item The \emph{reciprocity effect} which practically always must be included.
\item There is a choice of four network closure effects.
      Usually it will be sufficient to express the tendency to network
      closure by including one or two of these. They can be selected
      by theoretical considerations and/or by their empirical
      statistical significance.
      Some researchers may find the last effect (distances two)
      less appealing because it expresses network closure
      inversely.
      \begin{enumerate}
      \item[a.]
      \begin{minipage}[t]{.6\textwidth}
      The \emph{transitive triplets effect}, which is
               the classical representation of network closure by the number of transitive
               triplets.
               For this effect the contribution
               of the tie $i \rightarrow j$ is proportional to the total number
               of transitive triplets that it forms -- which can be transitive triplets
               of the type
               $\{i \rightarrow j \rightarrow h ;\ i \rightarrow h \}$
               as well as $\{i \rightarrow h \rightarrow j ;\ i \rightarrow j \}$;
      \end{minipage}
\hfill
\begin{minipage}[t]{.2\textwidth}
\linethickness{0.3pt}
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 2 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\arrow <2mm> [.2,.6]  from 2.2 1 to 3.8 1
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559
\arrow <2mm> [.2,.6]  from 3.1 2.559 to 3.9 1.1732
\endpicture
\end{center}
\end{minipage}
      \item[b.] The \emph{balance effect}, which may also be called \emph{structural equivalence
                with respect to outgoing ties}.
                This expresses a preference of actors to have ties to those other actors
                who have a similar set of outgoing ties as themselves.
                Whereas the transitive triplets effect focuses on how many same choices
                are made by ego (the focal actor) and alter (the other actor)
                --- the number of $h$ for which
                $i \rightarrow h$ and $j \rightarrow h $, i.e., $x_{ih} = x_{jh} = 1$
                where $i$ is ego and $j$ is alter --- ,
                the balance effect considers in addition how many the same
                non-choices are made --- $x_{ih} = x_{jh} = 0$.
      \item[c.] The \emph{transitive ties effect} is similar to the transitive
                triplets effect, but instead of considering for each other actor $j$
                how many two-paths $i \rightarrow h \rightarrow j $ there are,
                it is only considered whether there is at least one such indirect connection.
                Thus, one indirect tie suffices for the network embeddedness.
      \item[d.] The \emph{number of actors at distance two effect} expresses network closure inversely:
                stronger network closure (when the total number of ties is fixed)
                will lead to fewer geodesic distances equal to 2.
                When this effect has a negative parameter, actors will have a preference
                for having few others at a geodesic distance of 2 (given their
                out-degree, which is the number of others at distance 1);
                this is one of the ways for expressing network closure.
      \end{enumerate}
\item
      \begin{minipage}[t]{.7\textwidth}
      The \emph{three-cycles effect}, which can be regarded as
      generalized reciprocity (in an exchange interpretation
      of the network) but also as the opposite of hierarchy
      (in a partial order interpretation of the network).
      A negative three-cycles effect, together with a positive
      transitive triplets or transitive ties effect, may be
      interpreted as a tendency toward local hierarchy.
      The three-cycles effect also contributes to
      network closure.\\
      In a non-directed network, the three-cycles effect is identical
      to the transitive triplets effect.
      \end{minipage}
\hfill
\begin{minipage}[t]{.2\textwidth}
\linethickness{0.3pt}
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 2 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\arrow <2mm> [.2,.6]  from 2.2 1 to 3.8 1
\arrow <2mm> [.2,.6]  from 2.9 2.559  to 2.1 1.1732
\arrow <2mm> [.2,.6]  from 3.9 1.1732 to 3.1 2.559
\endpicture
\end{center}
\end{minipage}
\item Another triadic effect is the \emph{betweenness effect},
      which represents brokerage: the tendency for actors
      to position themselves between not directly connected
      others, i.e., a preference of $i$ for ties
      $i \rightarrow j$ to those $j$
      for which there are many $h$ with
      $h \rightarrow i$ and $h \not\rightarrow j$.

\item[{\hspace*{-1ex}$\bigodot$}]
     The following eight degree-related effects may be important especially for networks
     where degrees are theoretically important and represent social status
     or other features important for network dynamics;
     and/or for networks with high dispersion in in- or out-degrees
     (which may be an empirical reflection of the theoretical importance
     of the degrees).
     Include them if there are theoretical reasons for doing so,
     but only in such cases.

\item The \emph{in-degree popularity effect} (again, with or without `sqrt',
     with the same considerations applying)
     reflects tendencies to dispersion in in-degrees of the actors;
     or, tendencies for actors with high in-degrees to attract extra incoming ties
     `because' of their high current in-degrees.
\item The \emph{out-degree popularity effect} (again, with or without `sqrt',
     with the same considerations applying)
     reflects tendencies for
     actors with high out-degrees to attract extra incoming ties
     `because' of their high current out-degrees.
     This leads to a higher correlation between in-degrees and out-degrees.
\item The \emph{in-degree activity effect} (with or without `sqrt')
     reflects tendencies for
     actors with high in-degrees to send out extra outgoing ties
     `because' of their high current in-degrees.
     This leads to a higher correlation between in-degrees and out-degrees.
     The in-degree popularity and out-degree activity effects are
     not distinguishable in Method of Moments estimation; then the choice between them
     must be made on theoretical grounds.
\item The \emph{out-degree activity effect} (with or without `sqrt')
     reflects tendencies for
     actors with high out-degrees to send out extra outgoing ties
     `because' of their high current out-degrees.
     This also leads to dispersion in out-degrees of the actors.
\item The \emph{in-in degree assortativity effect} (where parameter 2 is the same
    as the sqrt version, while parameter 1 is the non-sqrt version)
     reflects tendencies for actors with high in-degrees
     to preferably be tied to other actors with high in-degrees.
\item The \emph{in-out degree assortativity effect} (with parameters
     2 or 1 in similar roles)
     reflects tendencies for actors with high in-degrees
     to preferably be tied to other actors with high out-degrees.
\item The \emph{out-in degree assortativity effect} (with parameters
     2 or 1 in similar roles)
     reflects tendencies for actors with high out-degrees
     to preferably be tied to other actors with high in-degrees.
\item The \emph{out-out degree assortativity effect} (with parameters
     2 or 1 in similar roles)
     reflects tendencies for actors with high out-degrees
     to preferably be tied to other actors with high out-degrees.
\end{enumerate}

\subsection{Important structural effects for network dynamics: \protect\newline
            two-mode networks}
\label{S_imp_str2}

For the structural part of the model for network dynamics,
for two-mode (or bipartite) networks,
treated in \citet{KoskinenEdling2010},
the most important effects are as follows.
The mathematical formulae for these and other effects are given
in Section~\ref{S_math}. Here we give a more qualitative description.
\begin{enumerate}
\item The \emph{out-degree effect} which always must be included.

\item \begin{minipage}[t]{.6\textwidth}
      Transitivity in two-mode networks is expressed in the first
      place by the number of \emph{four-cycles} \citep{RobinsAlexander04}.
      This reflects the extent to which actors who make one choice in common
      also make other choices in common.
      \vfill
\end{minipage}
\hfill
\begin{minipage}[t]{.25\textwidth}
\linethickness{0.3pt}
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 0.5 to 4.5, y from 0 to 5
\put{\large$\bullet$} at  1 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  1 3
\put{\large$\bullet$} at  4 3
\put{$i_2$} at 0.5 1
\put{$i_1$} at 0.5 3
\put{$j_2$} at 4.5 1
\put{$j_1$} at 4.5 3
\arrow <2mm> [.2,.6]  from 1.2 3 to 3.8 3
\arrow <2mm> [.2,.6]  from 1.2 2.9 to 3.8 1.1
\arrow <2mm> [.2,.6]  from 1.2 1 to 3.8 1
\arrow <2mm> [.2,.6]  from 1.2 1.1 to 3.8 2.9
\endpicture
\end{center}
\end{minipage}

\item[{\hspace*{-1ex}$\bigodot$}]
     The following three degree-related effects may be important especially for networks
     where degrees are theoretically important and represent social status
     or other features important for network dynamics;
     and/or for networks with high dispersion in in- or out-degrees
     (which may be an empirical reflection of the theoretical importance
     of the degrees).
     Include them if there are theoretical reasons for doing so,
     but only in such cases.

\item The \emph{out-degree activity effect} (with or without `sqrt'; often the sqrt
     version, which transforms the degrees in the explanatory role by the square root, works better)
     reflects tendencies to dispersion in out-degrees of the actors.
\item The \emph{in-degree popularity effect} (again, with or without `sqrt',
     with the same considerations applying)
     reflects tendencies to dispersion in in-degrees of the column units.
\item The \emph{out-in degree assortativity effect} (where parameter 2 is the same
    as the sqrt version, while parameter 1 is the non-sqrt version)
     reflects tendencies for actors with high out-degrees
     to preferably be tied to column units with high in-degrees.
\end{enumerate}


\subsection{Effects for network dynamics associated with covariates}
\label{S_eff_cov}

For each individual covariate, there are several effects which
can be included in a model specification, both in the network
evolution part and in the behavioral evolution part (should there be
dependent behavior variables in the data).
Of course for two-mode networks, the covariates must be compatible
with the network with respect to number of units (rows/columns).
%The following list is very incomplete.
\begin{itemize}
\item {\em network rate function}
\begin{enumerate}
\item the covariate's effect on the rate of network change of the
actor;
\end{enumerate}
\item {\em network evaluation and endowment functions}
\begin{enumerate}
\item the covariate-similarity effect;
      a positive parameter implies that actors prefer
      ties to others with similar values on this variable --
      thus contributing to the
      network-autocorrelation of this variable not by changing
      the variable but by changing the network;
\item the effect on the actor's activity (covariate-ego);
      a positive parameter will imply the tendency that
      actors with higher values on this covariate
      increase their out-degrees more rapidly;
\item the effect on the actor's popularity to other actors (covariate-alter);
      a positive parameter will imply the tendency that
      the in-degrees of actors with higher values on this covariate
      increase more rapidly;
\item the effect of the squared variable
      on the actor's popularity to other actors (squared covariate-alter)
      (included only if the range of the variable is at least 2).
      This normally makes sense only if the covariate-alter effect
      itself also is included in the model.
      A negative parameter implies a unimodal preference
      function with respect to alters' values on this covariate;
\item the interaction between the value of the covariate
      of ego and of the other actor (covariate ego $\times$ covariate alter);
      a positive effect here means, just like a positive similarity effect,
      that actors with a higher value on the covariate
      will prefer ties to others who likewise have a relatively high
      value;
      when used together with the alter effect of the squared variable
      this effect is quite analogous to the similarity effect,
      and for dichotomous covariates, in models where the ego and
      alter effects are also included, it even is equivalent
      to the similarity effect (although expressed differently),
      and then the squared alter effect is superfluous;
\item the `same covariate', or covariate identity, effect, which expresses the tendency of the
      actors to be tied to others with exactly the same value on the covariate;
      whereas the preceding four effects are appropriate for interval scaled
      covariates (and mostly also for ordinal variables),
      the identity effect is suitable for categorical variables;
\item the interaction effect of covariate-similarity with reciprocity;
\item the effect of the covariate of those to whom the actor is
      indirectly connected, i.e., through one intermediary but not
      with a direct tie; this value-at-a-distance can represent
      effects of indirectly accessed social capital.
%\item
%several other interaction effects of the covariate or
%covariate-similarity with endogenous network effects;
\end{enumerate}
\end{itemize}
The usual order of importance of these covariate effects on
network evolution is: evaluation effects are most important, followed
by endowment and rate effects. Inside the group of evaluation
effects, it is the covariate-similarity effect that is most
important, followed by the effects of covariate-ego and
covariate-alter.

When the network dynamics is not smooth over the observation waves --- meaning that
the pattern of ties created and terminated, as reported in the initial part of the output
file under the heading \emph{Initial data description -- Change in networks --
Tie changes between subsequent observations},
is very irregular across the observation periods --- it can be important to include
effects of time variables on the network.
Time variables are changing actor covariates that depend only on the
observation number and not on the actors. E.g., they could be dummy variables, being 1
for one or some observations, and 0 for the other observations.

For actor covariates that are constant within observation waves,
or -- in the case that there are structurally determined values --
constant within connected components,
only the ego effects are defined, because only those
effects are meaningful.
This exclusion of the alter, similarity and other effects for
such actor variables applies only to variables without any missing values.

For each dyadic covariate, the following network evaluation effects
can be included in the model for network evolution:
\begin{itemize}
\item {\em network evaluation and endowment functions}
\begin{enumerate}
\item main effect of the dyadic covariate;
\item the interaction
effect of the dyadic covariate with reciprocity.
\end{enumerate}
\end{itemize}
The main evaluation effect is usually the most important. In the
current version of \si, there are no effects of dyadic covariates
on behavioral evolution.

\subsection{Cross-network effects for dynamics of multiple networks}

If the are multiple dependent network variables,
the following effects may be important.
This is explained here jointly for the case of one-mode and two-mode
networks. The \emph{number of columns} is defined as the number of actors
for one-mode networks, and as the number of units/nodes/...
in the second node set for two-mode networks.
For cross-network effects the network in the role of dependent variable
is denoted by $X$ and the network in the role of explanatory variable
by $W$; thus, effects go from $W$ to $X$.
All these effects are regarded as effects determining the dynamics of network $X$.

\begin{enumerate}
\item If both networks have the same number of columns,
      then the basic effect is of $W$ on  $X$,
      representing the extent to which the existence of a tie
      $i \stackrel{W}{\rightarrow} j$ promotes
      the creation or maintenance of a tie $i \stackrel{X}{\rightarrow} j$.
\item If both networks are one-mode,
      then a next effect is the reciprocity effect with $W$ on  $X$,
      representing the extent to which the existence of a tie
      $j \stackrel{W}{\rightarrow} i$ promotes
      the creation or maintenance of a tie,
      in the reverse direction, $i \stackrel{X}{\rightarrow} j$.
\item If both networks are one-mode,
      then a next effect is the mutuality effect with $W$ on  $X$,
      representing the extent to which the existence of a mutual tie
      $i \stackrel{W}{\leftrightarrow} j$ promotes
      the creation or maintenance of a tie $i \stackrel{X}{\rightarrow} j$.
\item The \emph{outdegree W activity effect} (where parameter 2 is
    the sqrt version, while parameter 1 is the non-sqrt version -- see above
    for explanations of this) reflects the extent to which actors
    with high outdegrees on $W$ will make more choices in the
    $X$ network.

\item[{\hspace*{-1ex}$\bigodot$}] Several mixed transitivity effects can be important.
\item
\begin{minipage}[t]{0.7\textwidth}
If $X$ is a one-mode network, the \emph{from W agreement} effect
      represents the extent to which agreement between $i$ and $j$
      with respect to outgoing
      $W$-ties promotes the creation or maintenance
      of a tie $i \stackrel{X}{\rightarrow} j$.
\end{minipage}
\hfill
\begin{minipage}[t]{.15\textwidth}
\linethickness{0.3pt}
\vfill
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 2 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\put{{\small $W$}} at 2.1 1.9
\put{{\small $W$}} at 3.9 1.9
\put{{\small $X$}} at 3.0 0.6
\arrow <2mm> [.2,.6]  from 2.2 1 to 3.8 1
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559
\arrow <2mm> [.2,.6]  from 3.9 1.1732 to 3.1 2.559
\endpicture
\end{center}
\vfill
\end{minipage}

\item
\begin{minipage}[t]{0.7\textwidth}
If $W$ is a one-mode network, the \emph{W to agreement} effect
      represents the extent to which a $W$ tie $i \stackrel{W}{\rightarrow} j$
      leads to agreement between $i$ and $j$
      with respect to outgoing $X$-ties to others, i.e.,
      $X$-ties to the same third actors $h$,
      $i \stackrel{X}{\rightarrow} h$ and $j \stackrel{X}{\rightarrow} h$.
\end{minipage}
\hfill
\begin{minipage}[t]{.15\textwidth}
\linethickness{0.3pt}
\vfill
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 2 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\put{{\small $X$}} at 2.1 1.9
\put{{\small $X$}} at 3.9 1.9
\put{{\small $W$}} at 3.0 0.6
\arrow <2mm> [.2,.6]  from 2.2 1 to 3.8 1
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559
\arrow <2mm> [.2,.6]  from 3.9 1.1732 to 3.1 2.559
\endpicture
\end{center}
\vfill
\end{minipage}

\item
\begin{minipage}[t]{0.7\textwidth}
If $X$ and $W$ both are one-mode networks, the \emph{closure of W} effect
 represents the tendency closure of $W-W$ two-paths
 $i \stackrel{W}{\rightarrow} h \stackrel{W}{\rightarrow} j$
 by an $X$ tie
  $i \stackrel{X}{\rightarrow} j$.
\end{minipage}
\hfill
\begin{minipage}[t]{.15\textwidth}
\linethickness{0.3pt}
\vfill
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 2 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\put{{\small $W$}} at 2.1 1.9
\put{{\small $W$}} at 3.9 1.9
\put{{\small $X$}} at 3.0 0.6
\arrow <2mm> [.2,.6]  from 2.2 1 to 3.8 1
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559
\arrow <2mm> [.2,.6]  from 3.1 2.559 to 3.9 1.1732
\endpicture
\end{center}
\vfill
\end{minipage}

\end{enumerate}


\subsection{Effects on behavior evolution}
\label{S_eff_beh}

For models with one or more dependent behavior variables, i.e.,
models for the co-evolution of networks and behavior,
the most important effects for the behavior dynamics are the following;
see \citet*{SteglichEA10}.
In these descriptions, with the `alters' of an actor
we refer to the other actors to whom
the focal actor has an outgoing tie.
The dependent behavior variable is referred to as $Z$.
\begin{enumerate}
\item The shape effect, expressing the basic drive toward high values on $Z$.
      A zero value for the shape will imply a drift toward the midpoint
      of the range of the behavior variable.
\item The effect of the behavior $Z$ on itself,
      or quadratic shape effect, which is relevant
      only if the number of behavioral categories is 3 or more.
      This can be interpreted as giving a quadratic preference function
      for the behavior.
      When the coefficient for the shape effect is $\beta^Z_1$ and for the
      effect of $Z$ on itself, or quadratic shape effect, is $\beta^Z_2$,
      then the contributions
      of these two effects are jointly $\beta^Z_1\, (z_i - \bar z) \,+\,
                   \beta^Z_2\, (z_i - \bar z)^2$.
      With a negative coefficient $\beta^Z_2$, this
      is a unimodal preference function, with the maximum attained
      for $z_i \,=\, \bar z - 2\,\beta^Z_1/\beta^Z_2$.
      (Of course additional effects will lead to a different picture;
      but as long as the additional effects are linear in $z_i$ -- which is not
      the case for similarity effects! --, this will change the location of the maximum
      but not the unimodal shape of the function.)
      This can also be regarded as negative feedback, or a self-correcting
      mechanism: when $z_i$ increases, the further push toward higher values
      of $z_i$ will become smaller and when $z_i$ decreases, the further push toward lower values
      of $z_i$ will become smaller. On the other hand, when the coefficient $\beta^Z_2$
      is positive, the feedback will be positive, so that changes in $z_i$
      are self-reinforcing. This can be an indication of addictive behavior.
\item The average similarity effect, expressing the preference of actors
      to being similar with respect to $Z$ to their alters,
      where the total influence of the alters is the same
      regardless of the number of alters.
\item The total similarity effect, expressing the preference of actors
      to being similar to their alters,
      where the total influence of the alters is proportional to
      the number of alters.
\item The average alter effect, expressing that actors
      whose alters have a higher average value of the behavior $Z$,
      also have themselves a stronger tendency toward high values on the behavior.
\item The indegree effect, expressing that actors with a higher indegree
      (more `popular' actors) have a stronger tendency toward high values on the behavior.
\item The outdegree effect, expressing that actors with a higher outdegree
      (more `active' actors) have a stronger tendency toward high values on the behavior.
\end{enumerate}
Effects 1 and 2 will practically always have to be included as control variables.
(For dependent behavior variables with 2 categories, this applies only to effect 1.)
When the behavior dynamics is not smooth over the observation waves --- meaning that
the pattern of steps up and down, as reported in the initial part of the output
file under the heading \emph{Initial data description -- Dependent actor variables -- Changes},
is very irregular across the observation periods --- it can be important to include
effects of time variables on the behavior.
Time variables are changing actor covariates that depend only on the
observation number and not on the actors. E.g., they could be dummy variables, being 1
for one or some observations, and 0 for the other observations.

The average similarity, total similarity, and average alter effects
are different specifications of social influence.
The choice between them will be made on theoretical grounds
and/or on the basis of statistical significance.
\medskip

For each actor-dependent covariate as well as for each of the other
dependent behavior variables,
the effects on $Z$ which can be included is the following.
\begin{enumerate}
\item The main effect: a positive value implies that actors with a
      higher value on the covariate will have a stronger tendency
      toward high $Z$ values.
\iffalse
\item An interaction effect, which is a choice among three, dependent on the
      \hyperlink{T_effpar}{internal parameter} for this effect:
      \smallskip \\
      value 1: interaction of actor variable with average similarity;\\
      value 2: interaction of actor variable with total similarity;\\
      value 3: interaction of actor variable with average alter.\\
      See Section \ref{S_f_b}.
\fi
\item Interactions between two or three actor variables, see
      Section~\ref{S_int_eff}.
\end{enumerate}


\iffalse

\subsection{Model Type}
\label{S_modeltype}

When the data is perfectly symmetric, this will be detected by \si.
Then the analysis options for nondirected networks will be followed.

%The Model Type is specified in the
%\hyperlink{T_S_options}{model options} as (part of) the
%\hyperlink{T_modelcode}{Model Code}.

\subsubsection{Model Type: directed networks}

These are currently not implemented in \SI 4.
\fi

\iffalse
For directed networks, the Model Type distinguishes between
the model of \citet{Snijders01} (Model Type 1),
that of \citet{Snijders03} (Model Type 2),
and the tie-based model described in \citet{Snijders06} (Model Type 3).
Model Type 1 is the default model and is
described in the basic publications on Stochastic Actor-Oriented
Models for network dynamics.

Model type 2 is at this moment not implemented in \SI version 3.
\medskip
\fi

\iffalse
In Model Type 2, the `decisions' by the actors
consist of two steps: first a step to increase or decrease their
out-degree; when this step has been taken, the selection of the
other actor towards whom a new tie is extended (if the out-degree
rises) or from a an existing tie is withdrawn (if the out-degree
drops).
The decision by an actor to increase or decrease the number of outgoing ties
is determined on the basis
of only the current degree; the probabilities of increasing or
decreasing the out-degree are expressed by the distributional
tendency function $\xi$ (indicated in the output as \emph{xi}) and
the volatility function $\nu$ (indicated as \emph{nu}). Which new
tie to create, or which existing tie to withdraw, depends in the
usual way on the evaluation and endowment functions. Thus, the
outdegree distribution is governed by parameters that are not
connected to the parameters for the structural dynamics. The use of
such an approach in statistical modeling minimizes the influence of
the observed degrees on the conclusions about the structural aspects
of the network dynamics. This is further explained in \citet{Snijders03}.

For Model Type 2, in the rate function, effects connected to these
functions $\xi$ and $\nu$ are included. On the other hand, effects
in the evaluation function that depend only on the out-degrees are
canceled from the model specification, because they are not
meaningful in Model Type 2. To evaluate whether Model Type 1 or
Model Type 2 gives a better fit to the observed degree distribution,
the output gives a comparison between the observed out-degrees and
the fitted distribution of the out-degrees (as exhibited by the
simulated out-degrees). For Model Type 2 this comparison is always
given. For Model Type 1, this comparison is given by adding 10 to the
Model Code in the advanced options. (For \LaTeX\ users: the log
file contains code that can be used to make a graph of the type
given in \citet{Snijders03}.

For using Model Type 2, it is advised to first estimate some model
according to Model Type 1 (this may be a simple model containing a
reciprocity effect only, but it could also include more effects),
and then -- using the parameters estimated under Model Type 1 --
change the specification to Model Type 2, and use the
\hyperlink{T_S_cond}{unconditional estimation method}
(see Section~\ref{S_cond}) (instead of the conditional method which is the
default). It is likely that the very first model estimated under
Model Type 2 will have results with poor
\hyperlink{T_convergence}{convergence properties}, but in such
cases it is advised just to estimate the same model another time,
now using the parameter values obtained under the previous Model
Type 2 run as the initial values for the estimation.

To obtain a good model specification with respect to the rates of
change in dependence of the out-degrees, three effects can be
included:
\begin{enumerate}
\item the out-degrees effect
\item the factorial out-degree effect
\item the logarithmic out-degree effect.
\end{enumerate}
These are the effects defined in formula (18) of \citet{Snijders03}
and indicated with the parameters $\alpha_1$, $\alpha_2$,
and $\alpha_3$, respectively.
The user has to see from the estimation results which, or which two,
out of these effects
should be included to yield a good fit for the out-degrees.
% klopt dit wel met de mintekens??????????
\medskip
\fi

\iffalse
In addition these types, there is
Model Type 6 which implements the reciprocity model of \citet{Wasserman79}
and \citet{Leenders95}  \citep[also see][]{Snijders99, Snijders05} ---
provided that no other effects are chosen than
the outdegree effect, the reciprocity effect and perhaps
the reciprocity endowment effect,
and possible also effects of actor covariates or dyadic covariates.
This model is meaningful only as a ``straw man" model to provide a test
of the null hypothesis that the dynamics of the dyads are mutually
independent, against the alternative hypothesis
that there do exist network effects (which make the dyad processes
mutually dependent).
For this purpose, Model Type 6 can be chosen,
while for one or more network effects such as the effects
representing transitivity, the null hypothesis is tested that their
coefficients are zero (see Section~\ref{S_gof}).


\fi

\iffalse
\subsubsection{Model Type: non-directed networks}
\label{S_modeltype_nd}

Non-directed networks are an undocumented option (there currently
only is the presentation \citet{Snijders07}, and therefore
mentioned here reluctantly for those users who want to use
this option anyway.

\SI detects automatically when the networks all are non-directed, and then employs a model for this
special case. For non-directed networks, the Model Type has seven possible values,
as described in \citet{Snijders07}.

\begin{enumerate}
\item Forcing model: \\
      one actor takes the initiative and unilaterally
      imposes that a tie is created or dissolved.
\item Unilateral initiative and reciprocal confirmation:\\
      one actor takes the initiative and proposes a new tie
      or dissolves an existing tie; if the actor proposes a new tie, the other
      has to confirm, otherwise the tie is not created;
      for dissolution, confirmation is not required.
\item Tie-based model:\\
      a random pair of actors is chosen (actor-specific rate functions
      are not used here),
      and the average change in objective function (\ref{u_net})
      for toggling $(i,j)$ and $(j,i)$
      is the log-odds of the probability of changing the tie variable.
\item Pairwise conjunctive model:\\
      a pair of actors is chosen and reconsider
      whether a tie will exist between them;
      the tie will exist if both agree,
      it will not exist if at least one does not choose for it.
\item Pairwise disjunctive (forcing) model:\\
      a pair of actors is chosen and reconsider
      whether a tie will exist between them;
      the tie will exist if at least one of them chooses for the tie,
      it will not exist if both do not want it.
\item Pairwise compensatory (additive) model:\\
      a pair of actors is chosen and reconsider
      whether a tie will exist between them;
      this is based on the sum of their utilities
      for the existence of this tie.
\end{enumerate}
In Models 1-2, where the initiative is one-sided,
the rate function is comparable to the rate function in directed models.
In Models 4-6, however, the pair of actors is chosen at a rate
which is the \emph{product} of the rate functions
$\lambda_i$ and $\lambda_j$ for the two actors.
This means that opportunities for change of the single tie variable $x_{ij}$
occur at the rate $\lambda_i \times \lambda_j$.
The numerical interpretation is different from that in Models 1-2.
\fi
\hypertarget{T_int_eff}{
\subsection{Additional interaction effects}
}
\label{S_int_eff}

It is possible for the user to define additional interaction effects for the
network. % and the behavior.
The basis is provided by the initial definition, by \si, of `unspecified
interaction effects'.  Modifying two or three of the columns named `effect1',
`effect2', and `effect3' of the effects dataframe
allows the definition of two-way
or three-way interactions. The \emph{effectNumber} of the effects between which
an interaction is required should be entered in the `effect1' and `effect2',
and for three-way effects, the `effect3' columns. The interaction effect must
also be `included', but the underlying effects need only be `included' if
they are also required individually.

\sfn{includeInteraction} is an \R function provided to facilitate the definition
of interaction effects. Such effects can be specified simply by short names and
the names of any variables required to identify the underlying effects: it is
not necessary to know the effectNumbers of the effects. (The effectNumbers would
change if new effects are introduced to \rs.) Information about short names of
effects can be found in the file `effects.pdf' in the doc directory of the
library, accessible from within \R using the command

\verb|RShowDoc("effects", package="RSiena")|

Alternatively a new version of this list can be displayed in a browser by using
the function:

\verb|effectsDocumentation()|

\subsubsection{Interaction effects for network dynamics}

The following kinds of user-defined interactions are possible
for the network dynamics.
\begin{description}
\item[a.]
  Ego effects of actor variables can interact with all effects.
  \item[b.] Dyadic effects can interact with each other.
\end{description}
(The column ``InteractionType'' in the effects data frame indicates which
effects are `ego' effects and which are `dyadic' effects.)

Thus a two-way interaction must be between two dyadic effects or between one
ego effect and another effect. A three-way interaction may be between three
dyadic effects, two dyadic effects and an ego effect, or two ego effects and
another effect.

All effects used in interactions must be defined on the same network
(in the role of dependent variable): that for
which the ``unspecified
interaction effects'' is defined.  And either all must be evaluation effects or
all must be endowment effects.
\iffalse

  b. Further, interaction effects are permitted
  which are combinations of actor variables, dyadic variables, and reciprocity.\\
  c. The transitive triplets effect can interact with the reciprocity effect in two ways,
  in four ways with similarity between actor variables, and in four ways with dyadic covariates.\\
  d. The transitive triplets, 3-cycles, and transitive ties effects
  can be restricted to triplets having the same value on an actor covariate;
  or triplets in which all pairs have the value 1 on a dyadic covariate.
The specification is made by changing the
\hyperlink{T_effpar}{\emph{internal effect parameter}}
for the interaction effects.
The values of these internal parameters
can be changed in the \textsf{{\em pname}.mo} file
described in Section~\ref{S_mo3file}.

For interaction effects, this parameter represents a code
for the two or three interacting effects.
Each effect is represented by its index number
in three digits (including leading zeros)
as reported in the file called \textsf{{\em pname}.eff}
(recall that \textsf{{\em pname}} stands for your project name).
Thus, two-way interactions are represented by twice three digits: e.g., the code 020003
refers to the interaction between the effects numbered 20 and 3,
where the numbers are the rank numbers in this list of all effects.
Leading zeros of the total parameter can be skipped, so that the code 020003
can also be represented by 20003 (but not by 203!). The order does not matter, so that
the codes 020003, 003020, 20003, and 3020 all are equivalent.
Three-way interactions similarly are represented by thrice three digits.
For example, the code 020028003 represents the interaction effects
between the effects numbered 20, 28, and 3.
E.g., to implement the interaction effect between the effects numbered 20 and 3,
in the \textsf{{\em pname}.mo} file the lines that initially are
\begin{verbatim}
unspecified interaction effect
0 0 0 0   0.000000 0
0 0 0 0   0.000000 0
0 0 0 0   0.000000 0
\end{verbatim}
must be changed into
\begin{verbatim}
unspecified interaction effect
0 0 0 0   0.000000 020003
0 0 0 0   0.000000 0
0 0 0 0   0.000000 0
\end{verbatim}
After this is done, \SI will automatically replace the name
by the suitable interaction effect name.
(This is done by \textsf{Siena04};
and also when successfully exiting \textsf{Siena03} and \textsf{Siena07}.)
\medskip

One of the uses of interaction effects is non-homogeneity in time:
interactions with an actor variable that depends only on time
(the observation number).

For example, if there are four observations, two cumulative
dummy variables could be used for the periods,
defined by one data file with all rows equal to\\
\texttt{0 \ 1 \ 1 \ 1}\\
and another data file with all rows equal to\\
\texttt{0 \ 0 \ 1 \ 1}\\
where these data files are used as changing actor covariates,
called, e.g., \textsf{dum1} and \textsf{dum2}.
For the detailed interpretation, it is important to realize that
all covariates are centered internally in \si.
To avoid misinterpretation, one can look up in the file
\textsf{{\em pname}.dac} (see Section~\ref{S_datafiles})
what are the values used by \si.
If there are no other time-changing actor covariates,
then in this example all lines in the .dac files are\\
\texttt{-0.6667   0.3333   0.3333  -0.3333  -0.3333   0.6667}
\smallskip \\
Now suppose that the model specification includes a parameter
$\beta_r$ for reciprocity, $\beta_{r1}$ for the interaction of reciprocity
with dummy variable \textsf{dum1}, and $\beta_{r2}$ for the interaction of reciprocity
with  \textsf{dum2}.
Then the total effect of reciprocity is given by
\[
\beta_r + \beta_{r1}\,\textsf{dum1} + \beta_{r2}\,\textsf{dum2} \ ;
\]
this is then equal to \\
$\beta_r - 0.6667 \beta_{r1} - 0.3333 \beta_{r2}$\\
for period 1 (from observation 1 to observation 2),\\
$\beta_r + 0.3333 \beta_{r1} - 0.3333 \beta_{r2}$\\
for period 2, and
\\
$\beta_r + 0.3333 \beta_{r1} + 0.6667 \beta_{r2}$\\
for period 3.
\medskip


The *.dac file made internally by Siena stores the values of
time-changing covariates (see Siena manual section 21.3).
Here the variables have been centered, and they are as used
internally by Siena. For four observations / three periods,
the number of elements of each row is three times the number
of changing actor covariates. The order of the variables
is the same as the order in the output file (initial section).
This means that, if for the network process, the interaction
with dum1, and the interaction with dum2, I get parameter
estimates bn, bnd1, bnd2, then the resulting values for
the network process are
bn - 0.6667*bnd1 - 0.3333*bnd2 for period 1
bn + 0.3333*bnd1 - 0.3333*bnd2 for period 2
bn + 0.3333*bnd1 + 0.6667*bnd2 for period 3.


Interactions are also possible between reciprocity and transitive triplets.
Here it must be taken into account that several ways are possible for
such an interaction.
The two following interactions are available.
\bigskip

\noindent
\hfill
\begin{minipage}[t]{.35\textwidth}
\linethickness{0.3pt}
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 0 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\arrow <2mm> [.2,.6]  from 2.2 1.1 to 3.8 1.1   % i => j
\arrow <2mm> [.2,.6]  from 3.8 0.9 to 2.2 0.9   % j => i
\arrow <2mm> [.2,.6]  from 3.1 2.559 to 3.9 1.1732 % h => j
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559   % i => h
\endpicture
\medskip

\noindent
Interaction reciprocity $\times$ \\ transitive triplets, type 1.\\
Parameter \texttt{1002003}.
\end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{.35\textwidth}
\linethickness{0.3pt}
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 0 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\arrow <2mm> [.2,.6]  from 2.2 1 to 3.8 1   % i => j
\arrow <2mm> [.2,.6]  from 1.95 1.2 to 2.75 2.55   % i => h
\arrow <2mm> [.2,.6]  from 2.95 2.55 to 2.15 1.27   % h => i
\arrow <2mm> [.2,.6]  from 4.05 1.2 to 3.25 2.55   % j => h
\arrow <2mm> [.2,.6]  from 3.05 2.55 to 3.85 1.27   % h => j
\endpicture
\medskip

\noindent
Interaction reciprocity $\times$ \\ transitive triplets, type 2.\\
Parameter \texttt{2002003}.
\end{center}
\end{minipage}
\hfill
\bigskip

\noindent
To interpret these interactions, keep in mind that the existence of
the tie $i \rightarrow j$ is the `dependent variable'.
The usual condition for this tie in the transitive triplets effect
is the number of two-paths $i \rightarrow h \rightarrow j$.
For the type 1 interaction, this condition is extended with
the extra requirement that the `dependent' tie is already
reciprocated, i.e., there already is the tie $j \rightarrow i$.
For the type 2 interaction, the condition on each two-path
is extended with the extra requirement that the
two-path is reciprocated, i.e., the two-path
$j \rightarrow h \rightarrow i$ also exists.
To specify these interaction effects, simply change the internal
effect parameter into \texttt{1002003} or \texttt{2002003}, respectively.
\bigskip

\noindent
\hfill
\begin{minipage}[t]{.24\textwidth}
\linethickness{0.3pt}
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 1 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\put{sim} at 3 0.4
\arrow <2mm> [.2,.6]  from 2.2 1.0 to 3.8 1.0   % i => j
\arrow <2mm> [.2,.6]  from 3.1 2.559 to 3.9 1.1732 % j => j
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559   % i => h
\endpicture
\medskip

\noindent
Interaction similarity $\times$ \\ transitive triplets, \\ type 1.\\
Parameter \texttt{1003def}.
\end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{.24\textwidth}
\linethickness{0.3pt}
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 1 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\put{sim} at 1.8 1.9
\arrow <2mm> [.2,.6]  from 2.2 1.0 to 3.8 1.0   % i => j
\arrow <2mm> [.2,.6]  from 3.1 2.559 to 3.9 1.1732 % j => j
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559   % i => h
\endpicture
\medskip

\noindent
Interaction similarity $\times$ \\ transitive triplets, \\ type 2.\\
Parameter \texttt{2003def}.
\end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{.24\textwidth}
\linethickness{0.3pt}
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 1 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\put{sim} at 4.2 1.9
\arrow <2mm> [.2,.6]  from 2.2 1.0 to 3.8 1.0   % i => j
\arrow <2mm> [.2,.6]  from 3.1 2.559 to 3.9 1.1732 % j => j
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559   % i => h
\endpicture
\medskip

\noindent
Interaction similarity $\times$ \\ transitive triplets, \\ type 3.\\
Parameter \texttt{3003def}.
\end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{.24\textwidth}
\linethickness{0.3pt}
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 1 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\put{sim} at 1.8 1.9
\put{sim} at 4.2 1.9
\arrow <2mm> [.2,.6]  from 2.2 1.0 to 3.8 1.0   % i => j
\arrow <2mm> [.2,.6]  from 3.1 2.559 to 3.9 1.1732 % j => j
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559   % i => h
\endpicture
\medskip

\noindent
Interaction similarity $\times$ \\ transitive triplets, \\ type 4.\\
Parameter \texttt{4003def}.
\end{center}
\end{minipage}
\hfill
\bigskip

In addition, interaction effects can be specified between transitive
triplets and (1) similarity between actor variables
and (2) dyadic covariates.
Here the variables are represented by the number \textsf{001} for the
first variable, \textsf{002} for the second variable, etc., so the numbers are
not from the \textsf{{\em pname}.eff} file but just
the order in which the variables occur in all of the files.
Generally, \texttt{def} stands for the 3-digit representation
(with leading zeros) of the number of the actor variable
and also the number of the dyadic covariate; these
sets of variables are numbered separately, so the first dyadic
covariate is represented also by \textsf{001}.

The parameter \texttt{1003def}, \texttt{2003def}, \texttt{3003def},
and \texttt{4003def},
specifies transitive triplet effects where the transitive triplet
is weighted by the similarity between two actors
on actor variable number \textsf{def}:
for code \texttt{1003def} between actors $i$ and $j$,
for \texttt{2003def} between actors $i$ and $h$,
for \texttt{3003def} between actors $j$ and $h$, and
for \texttt{4003def} by the product of the similarity between actors $i$ and $h$
and the similarity between actors $j$ and $h$.
Analogously, the parameter \texttt{11003def}, \texttt{12003def}, \texttt{13003def},
and \texttt{14003def},
specifies transitive triplet effects where the transitive triplet
is weighted by dyadic variable number \textsf{def}:
for code \texttt{11003def} for actors $i$ and $j$,
for \texttt{12003def} for actors $i$ and $h$, and
for \texttt{13003def} for actors $j$ and $h$, and
for \texttt{14003def} by the product of the dyadic variable for actors $i$ and $h$
and for actors $j$ and $h$.
The dyadic variables here are not centered!!
For example, for the first actor variable,
code \texttt{1003001} will define the transitive triplets
effect weighted by the similarity between actors $i$ and $j$
on the first actor variable.
\bigskip

Further, three triadic effects: transitive triplets, 3-cycles,
and transitive ties, can be restricted to triplets which all have the
same value of an actor variable; or triplets in which all pairs
have the value 1 on a dyadic covariate.
This is achieved by the following codes:
\begin{description}
\item[\texttt{8003def}]\ : \ transitive triplets restricted to triplets of actors
                        having the same value on actor covariate number \texttt{def};
\item[\texttt{8005def}]\ : \  3-cycles restricted to triplets of actors
                        having the same value on actor covariate number \texttt{def};
\item[\texttt{8006def}]\ : \  transitive ties restricted to triplets of actors
                        having the same value on actor covariate number \texttt{def};
\item[\texttt{18003def}]\ : \  transitive triplets restricted to triplets of actors
                        where all pairs have the value 1 on dyadic covariate number \texttt{def};
\item[\texttt{18005def}]\ : \  3-cycles restricted to triplets of actors
                        where all pairs have the value 1 on dyadic covariate number \texttt{def};
\item[\texttt{18006def}]\ : \  transitive ties restricted to triplets of actors
                        where all pairs have the value 1 on dyadic covariate number \texttt{def}.
\end{description}


The calculation of user-defined effects is slightly more time-consuming
than the calculation of internally defined effects. Therefore, when there
is the choice between two equivalent effects -- e.g.,
in longitudinal modeling, interactions
of actor covariates with reciprocity -- it is advisable to use
the predefined interaction effects.

\subsubsection{Interaction effects for behavior dynamics}

For behavior dynamics, interaction effects can be defined
by the user, for each dependent behavior variable separately,
as interactions of two or three actor variables.
The actor variables (changing and non-changing) are numbered
in the order in which they appear in the \textsf{{\em pname}.mo} file:
first the dependent variables, then the non-changing actor variables,
then the other changing actor variables.
For the internal effect parameter defining the interaction effect,
each actor variable is represented by its index number
in this order, in two digits (including leading zero
if the number is less than 10).
E.g., the parameter 0203 represents the interaction between variables
number 2 and number 3, and parameter 010404 represents the interaction
between variables 1, 4 and 4. The interactions are represented
by products of the centered variables.

In addition, there are interactions available between actor variables
and influence, as described in Section~\ref{S_f_b}.


\begin{screen}
\newpage
\end{screen}
\begin{print}
%\newpage
\end{print}
\subsection{Random effects models: unobserved actor heterogeneity}

The network (and behavior) evolution may be affected by the fact that actors are
heterogeneous.
If all relevant actor heterogeneity is observed in the form of actor covariates,
then actor heterogeneity can be taken into account by including covariates in the model.
If not all relevant actor heterogeneity is observed,
then more complex models are required, such as random effects models.
Random effects models \citep[see][]{SchweinbergerSnijders07b} allow to take
unobserved actor heterogeneity into account
by assuming that the network (and behavior) evolution is affected by unobserved
outcomes of actor-dependent random variables (random effects),
which represent the combined effect of the unobserved actor
heterogeneity on the network (and behavior) evolution.

These models can be estimated only using the maximum likelihood
estimation option, see Sections~\ref{S_Est} and~\ref{S_options}.
The maximum likelihood estimation of random effects models requires MCMC-based data
imputation of the unobserved random effects (which can be regarded as missing data).
\SI supplies three alternative MCMC algorithms for the MCMC-based
data imputation of the random effects:
\begin{itemize}
\item[(1)] random walk M-H,
\item[(2)] autoregressive M-H,
\item[(3)] independence sampler (default).
\end{itemize}
The algorithms require the determination of the scale factor of the so-called proposal distribution,
which may affect the efficiency of the algorithms and
the accuracy of the results (for a given number of iterations).
It is recommended to choose the default algorithm (3);
and to choose as scale factor of the proposal distribution $0$---which
would be a pointless scale factor, but which communicates to \SI
that the user wishes to leave the determination of the scale factor to
the defaults within the algorithm,
which features an adaptive method for determining suitable scale factors.
In the ideal case, the choice of algorithm does not affect the
parameter estimates---though the efficiency of
the algorithms and the accuracy of the results (for a given number of iterations) may be affected.

The estimation of random effects models (i.e., the estimation of the parameters,
including the variances of the random effects) may be done by either Maximum Likelihood
or Bayesian estimation (see Section~\ref{S_Bayes}).

%The interpretation of the parameter estimates is straightforward;
%the estimates of the variances of the random effects indicate the magnitude
%of the unobserved actor heterogeneity.
\fi

\begin{print}
\newpage
\end{print}
\section{Estimation}
\label{S_Est}

The model parameters are estimated under the specification given
during the model specification part, using a stochastic
approximation algorithm.
%Three estimation procedures are implemented:
Only one estimation procedure is currently implemented:
the Method of Moments (MoM) \citep*{Snijders01, SnijdersEA07};
% the Method of Maximum Likelihood (ML) \citep{SnijdersEA10};
% and a Bayesian method \citep{Koskinen04, KoskinenSnijders07,
% SchweinbergerSnijders07c).
% For non-constant rate functions, currently only
% MoM estimation is available.
% The Method of Moments is the default;
% the other two methods require much more computing time.
% Given the greater efficiency but longer required computing time
% for the ML and Bayesian methods, these can be useful especially for smaller data sets
% and relatively complicated models (networks and behavior; endowment effects).


In the following, the number of
parameters is denoted by $p$. The algorithm %s are
is based on repeated
(and repeated, and repeated...) simulation of the evolution
process of the network. These repetitions are called `runs' in the
following. The MoM estimation algorithm is based on comparing the
observed network (obtained from the data files)
to the hypothetical networks generated in the simulations.

Note that the estimation algorithm is of a stochastic nature, so
the results can vary! This is of course not what you would like.
For well-fitting combinations of data set and model, the
estimation results obtained in different trials will be very
similar. It is good to repeat the estimation process at least once
for the models that are to be reported in papers or presentations,
to confirm that what you report is a stable result of the
algorithm.

The initial value of the parameters normally is the current value (that is,
the value that the parameters have immediately before you start
the estimation process);
as an alternative, it is possible to start instead with
a standard initial value.
Usually, a sequence of models can be
fitted without problems, each using the previously obtained estimate
as the starting point for the new estimation procedure.
Sometimes, however,
problems may occur during the estimation process, which will
be indicated by some kind of warning in the output file
or by parameter estimates being outside a reasonably expected range.
In such cases the current parameter estimates may be
unsatisfactory, and using them as initial
values for the new estimation process might again lead to
difficulties in estimation. Therefore,
when the current parameter values are unlikely and also
when they were obtained after a divergent estimation algorithm,
it is advisable to start the estimation algorithm
with a \emph{standard initial value}.
The use of standard initial values is one of the
\hyperlink{T_S_options}{model options}.
If this has successfully led to a model with convergent
parameter estimates and model fitting is continued,
then the option can be reset to the
current initial values.

\begin{screen}
\newpage
\end{screen}
\subsection{\label{algorithm}Algorithm} %MS

The estimation algorithm
is an implementation of the Robbins-Monro \citeyearpar{RobbinsMonro51}
 algorithm,
described in \citet{Snijders01, Snijders02}, and
has %for both the MoM and ML method
three phases:
\begin{enumerate}
\item In phase 1, the parameter vector is held constant at its
initial value.
      This phase is for
      having a first rough estimate of the matrix of derivatives.
\item Phase 2 consists of several subphases.
      More subphases means a greater precision. The default
      number of subphases is 4.
      The parameter values change from run to run, reflecting
      the deviations between generated and observed values of the
      statistics. The changes in the parameter values are smaller
      in the later subphases.\\
      The program searches for parameter values where these deviations
      average out to 0. This is reflected by what is called the
      \hypertarget{T_quasiac}{`quasi-auto\-cor\-relations'}
      in the output screen.
      These are averages
      of products of successively generated deviations between
      generated and observed statistics. It is a good sign
      for the convergence of the process when the
      \hyperlink{T_quasiac}{quasi-auto\-correlations}
      are negative (or positive but close to 0),
      because this means the generated values are jumping
      around the observed values.
\item In phase 3, the parameter vector is held constant again,
      now at its final value.
      This phase is for estimating the covariance matrix and the
      matrix of derivatives used for the computation of standard errors.\\
      The default number of runs in phase 3 is 1000. This requires a lot
      of computing time, but when the number of phase 3 runs is too low,
      the standard errors computed are rather unreliable.
\end{enumerate}

The number of subphases in phase 2, and the number of runs in
phase 3, can be changed in the \hyperlink{T_S_options}{model}
options.

\begin{screen}
\newpage
\end{screen}
The user can break in and modify the estimation process in three
ways:
\begin{enumerate}
\item it is possible to terminate the estimation;
\item in phase 2, it is possible to terminate phase 2
      and continue with phase 3;
%\item in addition, it is possible to change the current
%      parameter values and restart the whole estimation process.
\end{enumerate}

% For the ML estimation option and for the non-longitudinal case,
% tuning the `multiplicaton factor' and the `initial gain parameter'
% can be important for getting good results;
% for Bayesian estimation the `multiplicaton factor' can likewise
% be important; this is briefly described in Section~\ref{S_exec}.


\subsection{Output}
\label{S_output}

%There are three output files.
The output file is an ASCII (`text') file which can be
read by any text editor.
%The main output is given in the
It is called
\textsf{{\em pname}.out} (recall that {\sf pname} is the
project name defined by the user).
%A brief history of what the
%program does is written to the file \textsf{{\em pname}.log}.
%The latter file also contains some supplementary output
%that usually is disregarded but sometimes is helpful.
%Some diagnostic output
%containing a history of the estimation algorithm
%which may be informative when there are convergence problems is
%written to the file \textsf{{\em pname}.cck} (for `check').
%This file is overwritten for each new estimation. Normally, you
%only need to look at  \textsf{{\em pname}.out}.

The output is divided into sections indicated by a line {\tt @1},
subsections indicated by a line {\tt @2}, subsubsections indicated
by {\tt @3}, etc. For getting the main structure of the output, it
is convenient to have a look at the {\tt @1} marks first.

The primary information in the output of the estimation process
consists of the following three parts. Results are presented here
which correspond to Table 2, column ``$t_1$, $t_3$" of \citet{Snijders01}.
The results were obtained in an independent repetition of
the estimation for this data set and this model specification;
since the repetition was independent, the results are slightly
different, illustrating the stochastic nature of the estimation
algorithm.\medskip

\begin{screen}
\newpage
\end{screen}
\noindent{\em 1. Convergence check}\smallskip

In the first place, a
\hypertarget{T_convergence}{convergence check}
is given, based on Phase 3 of the algorithm. This check
considers the deviations between simulated values of the
statistics and their observed values (the latter are called the
`targets'). Ideally, these deviations should be 0. Because of the
stochastic nature of the algorithm, when the process has properly
converged the deviations are small but not exactly equal to 0.
The program calculates the averages and standard deviations of the
deviations and combines these in a $t$-ratio (in this case,
average divided by standard deviation). For longitudinal modeling,
convergence is excellent when these $t$-ratios are less than 0.1
in absolute value, good when they are less than 0.2, and
moderate when they are less than 0.3.
For published results, it is suggested that estimates presented come from runs
in which all $t$-ratios for convergence are less than 0.1 in absolute value
-- or nearly so.
(These bounds are indications only, and
are not meant as severe limitations.)
The corresponding part
of the output is the following.

{\footnotesize
\begin{verbatim}
Total of 1954 iterations.
Parameter estimates based on 954 iterations,
basic rate parameter as well as
convergence diagnostics, covariance and derivative matrices based on 1000 iterations.

Information for convergence diagnosis.
Averages, standard deviations, and t-ratios for deviations from targets:
  1.     -0.236    7.006   -0.034
  2.      0.204    7.059    0.029
  3.     -1.592   22.242   -0.072

Good convergence is indicated by the t-ratios being close to zero.
\end{verbatim}
}

In this case, the $t$-ratios are -0.034, -0.029, and -0.072,
which is less than 0.1 in absolute value, so the convergence is
excellent. In data exploration, if one or more of these
$t$-ratios are larger in absolute value than 0.3, it is
advisable to restart the estimation process. For results that are
to be reported, it is advisable to carry out a new estimation when
one or more of the $t$-ratios are larger in absolute value
than 0.1. Large values of the averages and standard deviations are
in themselves not at all a reason for concern.

For maximum likelihood estimation, the convergence
of the algorithm is more problematic than for longitudinal
modeling. A sharper value of the $t$-ratios must be found
before the user may be convinced of good convergence. It is
advisable to try and obtain $t$-values which are less than 0.15.
If, even with repeated trials, the algorithm does not succeed in
producing $t$-values less than 0.15, then the estimation results
are of doubtful value.
\medskip

\begin{screen}
\newpage
\end{screen}
\noindent{\em 2. Parameter values and standard errors}\smallskip

The next crucial part of the output is the list of estimates and
standard errors. For this data set and model specification, the
following result was obtained.

{\footnotesize
\begin{verbatim}
@3
Estimates and standard errors

 0. Rate parameter                                 5.4292  (   0.6920)
Other parameters:
 1. eval:  outdegree (density)                    -0.7648  (   0.2957)
 2. eval:  reciprocity                             2.3071  (   0.5319)
 3. eval:  number of actors at distance 2         -0.5923  (   0.1407)
\end{verbatim}
}

The rate parameter is the
\hyperlink{T_rho}{parameter called $\rho$}
in section \ref{S_r} below. The value 5.4292 indicates
that the estimated number of changes per actor (i.e., changes in
the choices made by this actor, as reflected in the row for this
actor in the adjacency matrix) between the two observations is
5.43 (rounded in view of the standard error 0.69). Note that this
refers to unobserved changes, and that some of these changes may
cancel (make a new choice and then withdraw it again), so the
average observed number of differences per actor will be somewhat
smaller than this estimated number of unobserved changes.

The other three parameters are the weights in the \emph{evaluation function}.
The terms in the evaluation function in this model specification are
the \hyperlink{T_density}{out-degree effect} defined as $s_{i1}$ in
Section \ref{S_f}, the
\hyperlink{T_reci}{reciprocity effect}
$s_{i2}$, and the
\hyperlink{T_dist2}{number of distances 2}
(indirect relations) effect, defined as $s_{i5}$. Therefore the
estimated evaluation function here is
\[
\min 0.76\, s_{i1}(x) \+ 2.31\, s_{i2}(x) \min 0.59\, s_{i5}(x)~.
\]

\begin{screen}
\newpage
\end{screen}
The standard errors can be used to test the parameters. For the rate
parameter, testing the hypothesis that it is 0 is meaningless
because the fact that there are differences between the two observed
networks implies that the rate of change must be positive. The
weights in the evaluation function can be tested by $t$-statistics,
defined as estimate divided by its standard error. (Do not confuse
this $t$-test with \hyperlink{T_convergence}{the $t$-ratio for}
checking convergence; these are completely different although both
are $t$ ratios!) Here the $t$-values are, respectively,
-0.7648/0.2957 = -2.59, 2.3071/0.5319 = 4.34, and -0.5923/0.1407 =
-4.21. Since these are larger than 2 in absolute value, all are
significant at the 0.05 significance level. It follows that there is
evidence that the actors have a preference for reciprocal relations
and for networks with a small number of other actors at a distance
2. The value of the density parameter is not very important; it is
important that this parameter is included to control for the density
in the network, but as all other statistics are correlated with the
density, the density is difficult to interpret by itself.

When for some effects the parameter estimate as well as the
standard error are quite large, say, when both are more than 2,
and certainly when both are more than 5, then it is possible that
this indicates poor convergence of the algorithm: in particular,
it is possible that the effect in question does have to be
included in the model to have a good fit, but the precise
parameter value is poorly defined (hence the large standard error)
and the significance of the effect cannot be tested with the
$t$-ratio. This can be explored by estimating the model without
this parameter, and also with this parameter
\hyperlink{T_fix}{fixed at some large value}
(see section~\ref{S_model}) -- whether the value is large positive or
large negative depends on the direction of the effect. For the
results of both model fits, it is advisable to check the fit by
simulating the resulting model and considering the statistic
corresponding to this particular parameter.
(The indicative sizes of 2 and 5 result from experience with
network effects and with effects of covariates on usual scales
with standard deviations ranging between, say, 0.4 and 2. These numbers have
to be modified for covariates with different standard errors.)
\medskip

\begin{screen}
\newpage
\end{screen}
\noindent{\em 3. Collinearity check}\smallskip

After the parameter estimates, the covariance matrix
of the estimates is presented. In this case it is

{\footnotesize
\begin{verbatim}
Covariance matrix of estimates (correlations below diagonal):
     0.087     -0.036      0.003
    -0.230      0.283     -0.033
     0.078     -0.440      0.020

\end{verbatim}
}

The diagonal values are the variances, i.e., the squares of the
standard errors (e.g., 0.087 is the square of 0.2957). Below the
diagonal are the correlations. E.g., the correlation between the
estimated density effect and the estimated reciprocity effect is
-0.230. These correlations can be used to see whether there is an
important degree of collinearity between the effects. Collinearity
means that several different combinations of parameter values
could represent the same data pattern, in
this case, the same values of the network statistics. When one or
more of the correlations are very close to -1.0 or +1.0, this is a
sign of near collinearity. This will also lead to large standard errors
of those parameters. It is then advisable to omit one of the
corresponding effects from the model, because it may be redundant
given the other (strongly correlated) effect. It is possible that
the standard error of the retained effect becomes much smaller by
omitting the other effect, which can also mean a change of the
$t$-test from non-significance to significance.

However, correlations between parameter estimates close to -1.0 or +1.0
should not be used too soon in themselves as reasons to exclude effects
from a model. This is for two reasons.
In the first place, network statistics often are highly correlated
(for example, total number of ties and number of transitive triplets)
and these correlations just are one of the properties of networks.
Second, near collinearity is not a problem in itself,
but the problem (if any) arises when standard errors are high,
which may occur
because the value of the parameters of highly correlated variables
is very hard to estimate with any precision. The problem resides in the
large standard errors, not in itself in the strong correlation between
the parameter estimates. If for both parameters
the ratio of parameter estimate to standard error,
i.e., the $t$-ratio, is larger than 2 in absolute value,
in spite of the high correlations between the parameter estimates, then
the significance of the $t$-test is evidence anyway that both
effects merit to be included in the model.
In other words, in terms of the `signal-to-noise ratio':
the random noise is high but the signal is strong enough
that it overcomes the noise.

As a rule of thumb for parameter correlations,
usually for correlations of estimated structural network effects there is no
reason for concern even when these correlations
are as strong as .9.

\begin{screen}
\newpage
\end{screen}
\begin{print}
%\newpage
\end{print}

\iffalse
\subsection{Maximum Likelihood and Bayesian estimation}
\label{S_ML}
\label{S_Bayes}

\SI can estimate models by three estimation methods: the (unconditional or conditional)
Method of Moments \citep*[`MoM', the default;][]{Snijders01; SnijdersEA07},
the Maximum Likelihood method \citep[`ML', see][]{SnijdersEA10},
and Bayesian methods
\citep[see][]{Koskinen04, KoskinenSnijders07, SchweinbergerSnijders07c}.
The maximum likelihood and Bayesian procedures are not yet
implemented in RSiena.
In nice situations (relatively small and large network data sets,
and large network and behavior data sets),
the three methods tend to agree
and there seems not to be no reason to use the more time-consuming
ML or Bayesian methods.
In not-so-nice situations (very small network data sets, small network and behavior
data sets in combination with complex models),
however, ML and Bayesian methods tend to produce more accurate results
than MoM.
Statistical theory suggests that ML is a more efficient estimation method
than MoM in the sense of producing estimates with smaller standard errors.
But in the `nice situations' the efficiency advantage of ML is very small.
Bayesian estimation is based on a different statistical paradigm, and
assumes and requires that the uncertainty about parameters is expressed
itself in a probability distribution.

\SI supplies three alternative MCMC algorithms for the
Bayesian estimation of the objective function parameters:
\begin{itemize}
\item[(1)] random walk M-H (default),
\item[(2)] autoregressive M-H,
\item[(3)] independence sampler.
\end{itemize}
The algorithms require the determination of the scale factor of
the so-called proposal distribution, which may affect the efficiency
of the algorithms and the accuracy of the results (for a given number of iterations).
It is recommended to make a short run with the default algorithm (1),
and then to make a longer run with algorithm (3);
and to choose as scale factor of the proposal
distribution $0$---which would be a pointless scale factor,
but which communicates to \SI that the user wishes to
leave the determination of the scale factor to the defaults provided in the algorithm,
which features an adaptive method for determining suitable
scale factors.
In the ideal case, the choice of algorithm does not affect the
results of primary interest, the parameter estimates---though the
efficiency of the algorithms and the accuracy of the results (for a
given number of iterations) may be affected.

Bayesian estimation gives rise to more results than the parameter
estimates printed in the output file.
The additional results can be best inspected by using {\tt R} and the
{\tt R} function {\tt siena}\_{\tt bayes} written by Michael
Schweinberger (see Section \ref{R_functions}).

\begin{screen}
\newpage
\end{screen}
\begin{print}
%\newpage
\end{print}
\subsection{Supplementing {\tt R} functions}
\label{R_functions}

To examine the MCMC output of \SI for Maximum Likelihood (ML) and Bayesian estimation,
the {\tt R} functions {\tt siena}\_{\tt mle} and {\tt siena}\_{\tt bayes} can be used,
respectively,
which were programmed by Michael Schweinberger.
The {\tt R} functions input files generated by {\tt Siena} and output,
among other things,
trace plots and MCMC lag $1, \dots, 100$ autocorrelations of sampled entities
\citep[see][]{SchweinbergerSnijders07b, SchweinbergerSnijders07c},
and,
in the Bayesian case,
in addition $95\%$ posterior intervals, histograms, and Gaussian kernel density
estimates of the marginal posterior densities of the parameters.

The {\tt R} functions {\tt siena}\_{\tt mle} and {\tt siena}\_{\tt bayes}
can be downloaded from the website \\
{\tt http://stat.gamma.rug.nl/stocnet},
and can be used in {\tt R} as follows:

\begin{itemize}
\item[(1)] Load the {\tt R} function:
\begin{itemize}
\item[---] ML estimation: \verb@source("siena_mle.r")@.
\item[---] Bayesian estimation: \verb@source("siena_bayes.r")@.
\end{itemize}
\item[(2)] Call the {\tt R} function:
\begin{itemize}
\item[---] ML estimation: \verb@siena_mle(project_name, full_output,@

\verb@no_random_effects, no_actors)@.

\item[---] Bayesian estimation: \verb@siena_bayes(project_name, full_output,@

\verb@no_random_effects, no_actors)@.

\end{itemize}
\end{itemize}

The arguments are:
\begin{itemize}
\item[---] \verb@project_name@ (string): the name of the {\tt Siena} project
that is to be examined;
note that calling {\tt siena}\_{\tt mle} or {\tt siena}\_{\tt bayes} presumes
that {\tt Siena} carried out ML or Bayesian estimation of the
project \verb@project_name@, respectively.
\item[---] \verb@full_output@ ($0$ or $1$): $1$ indicates that the full output is desired,
while $0$ indicates that selected output is desired.
\item[---] \verb@no_random_effects@ (non-negative integer): the number of
actor-dependent weights (parameters) in the model.
\item[---] \verb@no_actors@ (positive integer): the number of actors.
\end{itemize}
Examples are provided by \verb@siena_mle("alcohol", 1, 3, 50)@ and\\
\verb@siena_bayes("alcohol", 1, 3, 50)@.

\begin{screen}
\newpage
\end{screen}
\begin{print}
%\newpage
\end{print}
\subsection{Other remarks about the estimation algorithm}

\subsubsection{Changing initial parameter values for estimation}
\label{S_st}

When you wish to change initial parameter values for running a new
estimation procedure, this can be done by `breaking in' into the \SI program.
\fi

\subsubsection{Fixing parameters}
\label{S_fixingparameters}

\hypertarget{T_fix}{Sometimes an effect must be present in the
model, but its precise numerical value is not well-determined.}
E.g., if the network at time $t_2$ would contain only reciprocated
choices, then the model should contain a large positive
reciprocity effect but whether it has the value 3 or 5 or 10 does
not make a difference. This will be reflected in the estimation
process by a large estimated value and a large standard error, a
derivative which is close to 0, and sometimes also by
\hyperlink{T_convergence}{lack of convergence of the algorithm}.
(This type of problem also occurs in maximum likelihood estimation
for logistic regression and certain other generalized linear
models; \label{LargeFix} see \citet[section 1.6]{GeyerThompson92},
\citet{AlbertAnderson84, HauckDonner77}.)
In such cases this effect
should be fixed to some large value and not left free to be
estimated. This can be specified in the model specification
under the {\sf{Edit Effects}} %{\sf Advanced}
button. As another example, when the network
observations are such that ties are formed but not dissolved (some
entries of the adjacency matrix change from 0 to 1, but none or
hardly any change from 1 to 0), then it is possible that the
density parameter must be fixed at some high positive value.

\subsubsection{Automatic fixing of parameters}
\label{S_fixing}

If the algorithm encounters computational
problems, sometimes it tries to solve them automatically by fixing
one (or more) of the parameters. This will be noticeable because a
parameter is reported in the output as being fixed without your
having requested this. This automatic fixing procedure is used,
when in phase 1 one of the generated statistics seems to be
insensitive to changes in the corresponding parameter.

This is a sign that there is little information in the data about
the precise value of this parameter, when considering the
neighborhood of the initial parameter values. However, it is
possible that the problem is not in the parameter that is being
fixed, but is caused by an incorrect starting value of this
parameter or one of the other parameters.

When the warning is given that the program automatically fixed one
of the parameter, try to find out what is wrong.

In the first place, check that your data were entered correctly
and the coding was given correctly, and then re-specify the model
or restart the estimation with other (e.g., 0) parameter values.
Sometimes starting from different parameter values (e.g., the
default values implied by the
\hyperlink{T_S_options}{model option}
of ``standard initial values") will lead to a good result.
Sometimes, however, it works better to delete this effect
altogether from the model.

It is also possible that the parameter does need to be included in
the model but its precise value is not well-determined. Then it is
best to give the parameter a large (or strongly negative) value
and indeed
\hyperlink{T_fix}{require it to be fixed}
(see Section~\ref{S_model}).


\subsubsection{Conditional and unconditional estimation}
\label{S_cond}

\SI has two methods for MoM estimation and simulation:
\hypertarget{T_S_cond}{conditional and unconditional}. They differ
in the {\em stopping rule} for the simulations of the network
evolution. In unconditional estimation, the simulations of the
network evolution in each time period (and the co-evolution of the
behavioral dimensions, if any are included) carry on until the
predetermined time length (chosen as 1.0
for each time period between consecutive observation moments) has elapsed.

In conditional estimation, in each period
the simulations run on until a stopping
criterion is reached that is calculated from the observed data.
Conditioning is possible for each of the dependent variables
(network, or behavior), where `conditional' means `conditional on
the observed number of changes on this dependent variable'.

Conditioning on the network variable means running simulations
until the number of different entries between the initially
observed network of this period and the simulated network
\hypertarget{T_distance_stop}{is equal to the number} of entries
in the adjacency matrix that differ between the initially and the
finally observed networks of this period.

Conditioning on a behavioral variable means running simulations
until the sum of absolute score differences on the behavioral
variable between the initially observed behavior of this period
and the simulated behavior is equal to the sum of absolute score
differences between the initially and the finally observed
behavior of this period.

Conditional estimation is slightly more stable and efficient,
because the corresponding rate parameters are not estimated by the
Robbins Monro algorithm, so this method decreases the number of
parameters estimated by this algorithm.
% Therefore, it is the
% default for models that do not include any dependent behavior
% variables. For models including dependent behavior variables,
% the default estimation type is unconditional (because in most
% applications, there will be no straightforward choice for the
% conditioning variable).
The possibility to choose between
unconditional and the different types of conditional estimation is
one of the \hyperlink{T_S_options}{model options}.

If there are changes in network composition (see
Section~\ref{S_comp}), only the unconditional estimation procedure
is available.

\begin{print}
%\newpage
\end{print}

\subsubsection{Required changes from conditional to unconditional estimation}

Even though conditional estimation is slightly more efficient than
unconditional estimation, there is one kind of problem that
sometimes occurs with conditional estimation and which is not
encountered by unconditional estimation.

It is possible (but luckily rare) that the initial parameter
values were chosen in an unfortunate way such that the conditional
simulation does not succeed in ever attaining the condition required
by \hyperlink{T_distance_stop}{its stopping rule} (see
Section~\ref{S_cond}).
The solution is either to use standard initial values or to
to unconditional estimation.

\begin{print}
%\newpage
\end{print}
\hypertarget{T_se}{
\section{Standard errors}
}
\label{S_se}

The estimation of standard errors of the MoM estimates requires the estimation of derivatives,
which indicate how sensitive the expected values of the statistics
(see Section~\ref{algorithm}) are with respect to the parameters.
The derivatives can be estimated by three methods:
\begin{itemize}
\item[(0)] finite differences method with common random numbers,
\item[(1)] score function method 1 (default),
\item[(2)] score function method 2 (not currently implemented).
\end{itemize}
\citet{SchweinbergerSnijders07a} point out that the finite differences method is
associated with a bias-variance dilemma, and proposed the unbiased and
consistent score function methods.  These methods demand less computation time
than method (0).
\iffalse

Method 1 estimates the derivatives per observation
period separately by the simulated sample covariance of the complete data score
function and the generated statistics; this is then added over the observation
periods.  Especially for more than 2 observations, method 1 has a much smaller
standard error of the estimated standard errors than the other methods.  \fi It
is recommended to use at least 1000 iterations (default) in phase 3.  For
published results, it is recommended to have 2000 or 4000 iterations in phase 3.

\begin{print}
\newpage
\end{print}
\section{Tests}
\label{S_gof}

%Three
Two types of tests are available in \si.
\begin{enumerate}
\item $t$-type tests of single parameters can be carried out by
dividing the parameter estimate by its standard error.
Under the null hypothesis that the parameter is 0,
these tests have approximately a standard normal distribution.

\item Score-type tests of single and multiple parameters
      are described in the following section.

\iffalse
\item
In the maximum  likelihood estimation method
it is possible to request likelihood ratio tests.
The log likelihood ratio is computed
by bridge sampling \citep{GelmanMeng98, HandcockHunter06}.
This can be requested (a bit deviously) by the number of runs in phase 3
(defined in the  \hyperlink{T_S_options}{specification options}):
\begin{enumerate}
\item If the number of phase 3 runs is a multiple of 100 plus 1
      (e.g., 101, 501, etc.), then
      the log likelihood ratio is calculated comparing
      the estimates obtained with the standard initial values.
\item If the number of phase 3 runs is a multiple of 100 plus 2
      (e.g., 102, 502, etc.), then
      the log likelihood ratio is calculated comparing
      the estimates obtained with the initial values
      used in the current estimation procedure.
\end{enumerate}
The first option will be the most frequently useful, because it
yields log likelihood ratios which,
for different models fitted to a given data set,
all are comparable.
\fi
\end{enumerate}

\subsection{Score-type tests}
\label{howtodo}

A generalized Neyman-Rao score test
is implemented for the MoM estimation method
in \SI (see Schweinberger, 2005).
\iffalse
For the ML estimation method,
following the same steps produces the \citet{Rao47} efficient score test.
\fi

Most goodness-of-fit tests will have the following form: some model
is specified and one or more parameters are restricted to some
constant, in most cases $0$ -- these constant values
define the null hypothesis being tested.
This can be obtained in \RS by appropriate choices in the effects dataframe
(called \sfn{myeff} in Section~ \ref{S_Rscript}).
Parameters can be restricted by
putting 1 in the \sfn{fix} and \sfn{test} columns when editing the effects, and
the tested value in the \sfn{initialValue} column.
For example, to request a score test for the reciprocity evaluation effect for
the first network: Suppose this effect has \sfn{effectNumber} equal to 10, the
commands can be as follows.

\begin{verbatim}
myeff[10, 9] <- TRUE
myeff[10, 'fix'] <- TRUE
myeff[10, 'test'] <- TRUE
myeff[10, 'initialValue'] <- ((value to be used for test))
## or, more easily
myeff <- setEffect(myeff, recip, fix=TRUE, test=TRUE,
initialValue=(value to be used for test))
\end{verbatim}




The goodness-of-fit test
proceeds by simply estimating the restricted model (not the unrestricted model,
with unrestricted parameters) by the standard \SI estimation algorithm. No more
information needs to be communicated.%  When the model is restricted, \SI by
% default assumes that the restricted model is to be tested against the
% unrestricted model, and by default \SI evaluates the generalized Neyman-Rao
% score test statistic.

\subsection{Example: one-sided tests, two-sided tests, and one-step estimates}
\label{example}

Suppose that it is desired to test the goodness-of-fit of the model
restricted by the null hypothesis that the reciprocity parameter is zero.
The following output may be obtained:

%\newpage

\begin{verbatim}
@2
Generalised score test <c>
--------------------------

Testing the goodness-of-fit of the model restricted by

 (1)   eval:  reciprocity                                 =  0.0000
________________________________________________

   c =   3.9982   d.f. = 1   p-value =   0.0455
   one-sided (normal variate):   1.9996
________________________________________________

One-step estimates:

l: constant network rate (period 1)                 6.3840
l: constant network rate (period 2)                 6.4112
eval:  outdegree (density)                          0.9404
eval:  reciprocity                                  1.2567
\end{verbatim}
To understand what test statistic {\tt <c>} is about, consider the case
where the network is observed at two time points, and let $R$
be the number of reciprocated ties at the second time point. Then it
can be shown that the test statistic is some function of
\[
  \mbox{Expected $R$ under the restricted model } - \mbox{ observed } R.
\]
Thus, the test statistic has some appealing interpretation in terms
of goodness-of-fit: when reciprocated ties do have added value for
the firms---which means that the reciprocity parameter is not 0,
other than the model assumes---then the deviation of the observed
$R$ from the $R$ that is expected under the model will be large
(large misfit), and so will be the value of the test statistic.
Large values of the test statistic imply low $p$-values, which, in
turn, suggests to abandon the model in favor of models incorporating
reciprocity.

The null distribution of the test statistic $c$ tends,
as the number of observations increases, to the chi-square
distribution, with degrees of freedom equal to the
number of restricted parameters. The corresponding $p$-value is
given in the output file.

In the present case, one parameter is restricted (reciprocity),
hence there is one degree of freedom {\tt d.f. = 1}. The value of
the test statistic {\tt c = 3.9982} at one degree of freedom
gives {\tt p = 0.0455}.
That is, it seems that reciprocity
should be included into the model and estimated as the other
parameters.

The one-sided test statistic, which can be regarded as normal variate, equals {\tt 1.9996}
indicating that the value of the transitivity parameter is positive.

The one-step estimates are approximations of the unrestricted estimates (that is,
the estimates that would be obtained if the model were estimated once again,
but without restricting the reciprocity parameter).
The one-step estimate of reciprocity, {\tt 1.2567},
hints that this parameter is positive,
which agrees with the one-sided test.

\subsubsection{Multi-parameter tests}

In the case where $K > 1$ model parameters are restricted, \SI
evaluates the test statistic with $K$ degrees of freedom. A low
$p$-value of the joint test would indicate that the
goodness-of-fit of the model is intolerable. However, the joint
test with $K$ degrees of freedom gives no clue as to what parameters
should be included into the model: the poor goodness-of-fit could be
due to only one of the $K$ restricted parameters, it could be due to
two of the $K$ restricted parameters, or due to all of them. Hence
\SI carries out, in addition to the joint test with $K$ degrees of
freedom, additional tests with one degree of freedom that test the
single parameters one-by-one. The goodness-of-fit table looks as
follows:

% I do not understand this, but the verbatim environment
% does something with < and > signs.
% Unpaired < signs lead to errors later on.
% Therefore I changed < and > to [ and ]. TS.
\begin{verbatim}
@2
Generalised score test <c>
--------------------------

Testing the goodness-of-fit of the model restricted by

 (1)   eval:  covariate_ij (centered)                     =  0.0000
 (2)   eval:  covariate_i alter                           =  0.0000
 (3)   eval:  covariate_i similarity                      =  0.0000
________________________________________________

Joint test:
-----------
   c =  92.5111   d.f. = 3   p-value [ 0.0001

(1) tested separately:
----------------------
 - two-sided:
   c =  62.5964   d.f. = 1   p-value [ 0.0001
 - one-sided (normal variate):   7.9118

(2) tested separately:
----------------------
 - two-sided:
   c =  16.3001   d.f. = 1   p-value [ 0.0001
 - one-sided (normal variate):   4.0373

(3) tested separately:
----------------------
 - two-sided:
   c =  23.4879   d.f. = 1   p-value [ 0.0001
 - one-sided (normal variate):   4.8464
________________________________________________

One-step estimates:

l: constant network rate (period 1)                 7.4022
l: constant network rate (period 2)                 6.4681
eval:  outdegree (density)                         -0.4439
eval:  reciprocity                                  1.1826
eval:  transitive triplets                          0.1183
eval:  covariate_ij (centered)                      0.4529
eval:  covariate_i alter                            0.1632
eval:  covariate_i similarity                       0.4147
\end{verbatim}


In the example output, three parameters are restricted.
The joint test has test statistic $c$, which has under the
null hypothesis a chi-squared distribution with d.f.\ = 3.
The $p$-value corresponding to the joint test indicates
that the restricted model is not tenable. Looking at the separate
tests, it seems that the misfit is due to all three parameters.
Thus, it is sensible to improve
the goodness-of-fit of the baseline model by including all of these parameters,
and estimate them.

\subsection{Alternative application: convergence problems}
\label{alternative}

An alternative use of the score test statistic is as follows. When
convergence of the estimation algorithm is doubtful, it is sensible
to restrict the model to be estimated. Either "problematic" or
"non-problematic" parameters can be kept constant at preliminary
estimates (estimated parameters values). Though such strategies may
be doubtful in at least some cases, it may be, in other cases, the
only viable option besides simply abandoning "problematic" models.
The test statistic can be exploited as a guide in the process of
restricting and estimating models, as small values of the test
statistic indicate that the imposed restriction on the parameters is
not problematic.

\newpage
\subsection{Testing differences between independent groups}

Sometimes it is interesting to test differences between parameters
estimated for independent groups. For example, for work-related support networks
analyzed in two different firms, one might wish to test whether the
tendency to reciprocation of work-related support, as reflected by the reciprocity parameter,
is equally strong in both firms.
Such a comparison is meaningful especially if the total model is the same in
both groups, as control for different other effects would compromise
the basis of comparison of the parameters.

If the parameter estimates in the two networks are $\hat\beta_a$ and $\hat\beta_b$,
with standard errors \textit{s.e}$_a$ and  \textit{s.e}$_b$, respectively,
then the difference can be tested with the test statistic
\begin{equation}
    \frac{\hat\beta_a  - \hat\beta_b}{\sqrt{s.e_a^2 + s.e_b^2}} \ ,
\end{equation}
which under the null hypothesis of equal parameters has an approximating
standard normal distribution.



\begin{print}
\newpage
\end{print}
\section{Simulation}

The simulation option still must be made available in a clear way for \SI
version 4.

The simulation option simulates the network evolution for fixed
parameter values. This is meaningful mainly at the point that you
have already estimated parameters, and then either want to check
again whether the statistics used for estimation have expected
values very close to their observed values, or want to compute
expected values of other statistics.
%The statistics to be simulated
%can be specified in the file \textsf{\em pname}.si,
%as documented in Section~\ref{S_sifile}.

The number of runs is set at a default value of 1,000, and can be
changed in the \hyperlink{T_S_simoptions}{simulation options}. The
user can break in and terminate the simulations early.
When only 1 run is requested, an entire data set is generated
and written to file in \SI format and also in Pajek format.
%When exactly 10 runs are requested and the maximum likelihood option is chosen,
%then the sequence of changes
%from each observation to the next is written to file \textsf{{\em pname}.cha}
%in the format described in Section~\ref{S_lalgo}.

The output file contains means, variances, covariances, and
correlations of the selected statistics. The output file also
contains $t$-statistics for the various statistics; these can be
regarded as tests for the simple null hypothesis that the model
specification with the current parameter values is correct.

For simulating networks and behavior, the output includes
the autocorrelation statistics known as Moran's $I$ and Geary's $c$.
For formulae and interpretation see, e.g., \citet[98--99]{Ripley81}.
These measure the extent to which the value of the variable
in question is similar between tied actors.
This similarity is expressed by relatively high values for Moran's $I$
and by relatively low values for Geary's $c$.
The null values, which are the expected values for variables
independent of the network, are given by $-1/(n-1)$ for Moran's $I$
and by 1 for Geary's $c$.

(The output of the descriptive statistics, which can be obtained
from \textsf{Siena02}, also contains Moran's $I$ and Geary's $c$,
computed for the observed data, together with their
null means and standard deviations.)


The simulation feature can be used in the following way. Specify a
model and estimate the parameters. After this estimation
(supposing that it converged properly), add a number of potential
effects. This number might be too large for the estimation
algorithm. Therefore, do not {\sf Estimate} but choose {\sf
Simulate} instead. The results will indicate which are the
statistics for which the largest deviations (as measured by the
$t$-statistics) occurred between simulated and observed values.
Now go back to the model specification, and return to the
specification for which the parameters were estimated earlier. The
effects corresponding to the statistics with large $t$-values are
candidates for now being added to the model. One should be aware,
however, that such a data-driven approach leads to capitalization
on chance. Since the selected effects were chosen on the basis of
the large deviation between observed and expected values, the
$t$-tests, based on the same data set, will tend to give
significant results too easily.
The tests described in Section~\ref{S_gof} do not have this
problem of chance capitalization.

The generated statistics for each run are also written to the file
\textsf{{\em pname}.sdt} (`sdt' for `simulation data'), so you can
inspect them also more precisely. This file is overwritten each
time you are simulating again. A brief history of what the program
does is again written to the file \textsf{{\em pname}.log}.

\subsection{Conditional and unconditional simulation}

The distinction between conditional and unconditional simulation
is the same for the simulation as for
\hyperlink{T_S_cond}{the estimation option}
of \si, described in Section~\ref{S_cond}.

If the conditional simulation option was chosen (which is the
default) and the simulations do not succeed in achieving the
condition required by
\hyperlink{T_distance_stop}{its stopping rule}
(see Section~\ref{S_cond}), then the simulation is
terminated with an error message, saying {\em This distance is not
achieved for this parameter vector}. In this case, you are advised
to change to unconditional simulation.

\begin{print}
\newpage
\end{print}
\section[Options for model type, estimation and simulation]{Options for model type, estimation and simulation}
\label{S_options}

\hypertarget{T_S_options}{}
There are several options available in \si. The main options
concern the model type and the estimation procedure used.


\begin{enumerate}
\item There is a choice between conditional (1) and unconditional (0)
Method of Moments estimation. If there are dependent action variables, the default for
conditional estimation is to condition on the observed distance
for the network variable; but it then is possible also to condition
on the distances observed for the dependent action variables.\\
%In addition, there are options for maximum likelihood (2)
%and Bayesian (3) estimation; these are beginning to be documented.
%\item \hypertarget{T_modelcode}{The Model Code}.\\
%   This defines the Model Type and an associated output option.\\
%   In the longitudinal case, the meaning of this code is as follows.\\
%   Model Codes 10 or more give extra output for evaluating the fit of
%   the out-degree distribution and for the explained variation
%   \citet{Snijders04};\\
%   the integer Model Code in the unit position (i.e.,
%   Model Code itself if it is less than 10, and Model Code - 10 if the code is more than 10)
%   defines the Model Type defined in Section~\ref{S_modeltype}.\\[0.5ex]
\item The number of subphases in phase 2 of the estimation algorithm.\\
      This determines the precision of the estimate.
      Advice: 3 for quick preliminary investigations,
      4 or 5 for serious estimations.
\item The number of runs in phase 3 of the estimation algorithm.\\
      This determines the precision of the estimated standard errors
      (and covariance matrix of the estimates),
      and of the $t$-values reported as diagnostics of the convergence.
      Advice: 200 for preliminary investigations when precise standard errors
      and $t$-values are not important,
      1000 for serious investigations,
      2000 to 4000 for estimations of which results are to be reported
      in publications.\\
      (These numbers can be twice as low if, instead of the
      new (from Version 2.3) default option of estimation by the
      Score Function method, the older method of
      Finite Differences is used. The latter method has runs
      that take more time, but needs fewer runs.)
%\item A constant used in other estimation procedures.\\
%      In the ML case, this is the multiplication
%      factor $r$ for the \hyperlink{T_runlength}{run length} used in the
%      MCMC algorithm.
\item The initial gain value, which is the step size in the starting
      steps of the Robbins-Monro procedure, indicated in
      \citet{Snijders01} by $a_1\,$.
\item The choice between standard initial values (suitable
estimates for the density and reciprocity parameters and zero
values for all other parameters) or
the current parameter values as initial values for estimating new
parameter values.
%\item The selection of the period for which a goodness-of-fit
%       on period homogeneity is to be carried out.
%\item The selection of the effect for which a goodness-of-fit
%       on actor homogeneity is to be carried out
%       (1 for the out-degree effect, 2 for the reciprocity effect);
%       if this is selected, a list of actors also has to be supplied.
\item A random number seed. If the value 0 is chosen, the program
      will randomly select a seed. This is advised to obtain truly
      random results. If results from an earlier run are to be
      exactly replicated, the random number seed from this earlier
      run can be used.
\item The method to estimate derivatives;
      0 is the older finite differences method
      %(this is the method used in
      %\SI versions 1 and 2, which has a bias);
      1 is the more efficient and unbiased
      method proposed by \citet{SchweinbergerSnijders07a};
      this is the preferred method. See Section~\ref{S_se}.
\end{enumerate}

\hypertarget{T_S_simoptions}
There is one option for simulations that can be chosen here.
\begin{enumerate}
\setcounter{enumi}{\value{savenumi}}
\item The number of runs in the straight simulations.\\
      Advice: the default of 1000 will usually be adequate.
\setcounter{savenumi}{\value{enumi}}
\end{enumerate}
Depending on the choice for conditional or unconditional
estimation in the estimation options, also the simulations are run
conditionally or unconditionally.\medskip


\begin{print}
\newpage
\end{print}
\section{Getting started}
\label{S_getting}

For getting a first acquaintance with the model, one may use the
data set collected by Gerhard van de Bunt, discussed extensively in
\citet*{vanBunt99, vanBuntEA99},
and used as example also in \citet{Snijders01} and \citet{Snijders05}.
The data files are provided with the program
and at the \SI website. The digraph data files
used are the two networks {\sf vrnd32t2.dat}, {\sf vrnd32t4.dat}.
The networks are coded as 0 = unknown, 1 = best friend, 2 = friend,
3 = friendly relation, 4 = neutral, 5 = troubled relation, 6 = item
non-response, 9 = actor non-response.
Choose the values 1, 2, and 3 as the values to be coded
as 1 for the first as well as the second network. Choose 6 and 9 as
missing data codes.

The actor attributes are in the file {\sf vars.dat}. Variables
are, respectively, gender (1 = $F$, 2 = $M$), program, and smoking
(1 = yes, 2 = no). See the references mentioned above for further
information about this network and the actor attributes.

At first, leave the specification of the rate function as
it is by default (see Section \ref{S_modspec}):
a constant rate function).

Then let the program estimate the parameters. You will see a
screen with intermediate results: current parameter values, the
differences (`deviation values') between simulated and observed
statistics (these should average out to 0 if the current
parameters are close to the correct estimated value), and the
\hyperlink{T_quasiac}{quasi-autocorrelations} discussed in Section
\ref{S_Est}.

It is possible to intervene in the algorithm by clicking on the
appropriate buttons: %the current parameter values may be altered
%or
the algorithm may be restarted or terminated. In most cases
this is not necessary.

\begin{screen}
\newpage
\end{screen}
Some patience is needed to let the machine complete its three
phases.
%How this depends on the data set and the number of parameters
%in the model is indicated in Section~\ref{S_timeuse}.
After having obtained the outcomes of the estimation
process, the model can be respecified: non-significant effects may
be excluded (but it is advised always to retain the out-degree and
the reciprocity effects) and other effects may be included.

\begin{print}
%\newpage
\end{print}
\subsection{Model choice}
\label{S_model}

For the selection of an appropriate model for a given data set it
is best to start with a simple model (including, e.g., 2 or 3
effects), delete non-significant effects, and add further effects
in groups of 1 to 3 effects. Like in regression analysis, it is
possible that an effect that is non-significant in a given model
may become significant when other effects are added or deleted!

When you start working with a new data set, it is often helpful first
to investigate the main endogenous network effects (reciprocity,
transitivity, etc.) to get an impression of what the network
dynamics looks like, and later add effects of covariates.
The most important effects are discussed in Section~\ref{S_modspec};
the effects are defined mathematically
in Section~\ref{S_math}.

\iffalse
Here we give a more qualitative description.
\begin{enumerate}
\item The \emph{out-degree effect} which always must be included.
\item The \emph{reciprocity effect} which practically always must be included.
\item There is a choice of four network closure effects.
      Usually it will be sufficient to express the tendency to network
      closure by including one or two of these. They can be selected
      by theoretical considerations and/or by their empirical
      statistical significance.
      Some researchers may find the last effect (distances two)
      less appealing because it expresses network closure
      inversely.
      \begin{enumerate}
      \item[a.]
      \begin{minipage}[t]{.6\textwidth}
      The \emph{transitive triplets effect}, which is
               the classical representation of network closure by the number of transitive
               triplets.
               For this effect the contribution
               of the tie $i \rightarrow j$ is proportional to the total number
               of transitive triplets that it forms -- which can be transitive triplets
               of the type
               $\{i \rightarrow j \rightarrow h ;\ i \rightarrow h \}$
               as well as $\{i \rightarrow h \rightarrow j ;\ i \rightarrow j \}$;
      \end{minipage}
\hfill
\begin{minipage}[t]{.2\textwidth}
\linethickness{0.3pt}
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 2 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\arrow <2mm> [.2,.6]  from 2.2 1 to 3.8 1
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559
\arrow <2mm> [.2,.6]  from 3.9 1.1732 to 3.1 2.559
\endpicture
\end{center}
\end{minipage}
      \item[b.] The \emph{balance effect}, which may also be called \emph{structural equivalence
                with respect to outgoing ties}.
                This expresses a preference of actors to have ties to those other actors
                who have a similar set of outgoing ties as themselves.
                Whereas the transitive triplets effect focuses on how many same choices
                are made by ego (the focal actor) and alter (the other actor)
                --- the number of $h$ for which
                $i \rightarrow h$ and $j \rightarrow h $, i.e., $x_{ih} = x_{jh} = 1$
                where $i$ is ego and $j$ is alter --- ,
                the balance effect considers in addition how many the same
                non-choices are made --- $x_{ih} = x_{jh} = 0$.
      \item[c.] The \emph{transitive ties effect} is similar to the transitive
                triplets effect, but instead of considering for each other actor $j$
                how many two-paths $i \rightarrow h \rightarrow j $ there are,
                it is only considered whether there is at least one such indirect connection.
                Thus, one indirect tie suffices for the network embeddedness.
      \item[d.] The \emph{number of actors at distance two effect} expresses network closure inversely:
                stronger network closure (when the total number of ties is fixed)
                will lead to fewer geodesic distances equal to 2.
                When this effect has a negative parameter, actors will have a preference
                for having few others at a geodesic distance of 2 (given their
                out-degree, which is the number of others at distance 1);
                this is one of the ways for expressing network closure.
      \end{enumerate}
\item
      \begin{minipage}[t]{.7\textwidth}
      The \emph{three-cycles effect}, which can be regarded as
      generalized reciprocity (in an exchange interpretation
      of the network) but also as the opposite of hierarchy
      (in a partial order interpretation of the network).
      A negative three-cycles effect sometimes may be
      interpreted as a tendency toward hierarchy.
      The three-cycles effect also contributes to
      network closure.\\
      In a non-directed network, the three-cycles effect is identical
      to the transitive triplets effect.
      \end{minipage}
\hfill
\begin{minipage}[t]{.2\textwidth}
\linethickness{0.3pt}
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 2 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\arrow <2mm> [.2,.6]  from 2.2 1 to 3.8 1
\arrow <2mm> [.2,.6]  from 2.9 2.559  to 2.1 1.1732
\arrow <2mm> [.2,.6]  from 3.9 1.1732 to 3.1 2.559
\endpicture
\end{center}
\end{minipage}
\end{enumerate}
\fi

\subsubsection{Exploring which effects to include}

The present section describes an exploratory approach to model
specification. A more advanced approach to testing model
specifications is described in Section~\ref{S_gof}.

For an exploration of further effects to be included, the
following steps may be followed:
\begin{enumerate}
\item Estimate a model which includes a number of basic effects;
\item Simulate the model for these parameter values but
      also include some other relevant statistics
      among the simulated statistics;
\item Look at the $t$-values for these other statistics;
      effects with large $t$-values are candidates for inclusion
      in a next model.
\end{enumerate}
It should be kept in mind, however, that this exploratory approach
may lead to capitalization on chance, and also that the $t$-value
obtained as a result of the straight simulations is conditional on
the fixed parameter values used, without taking into account the
fact that these parameter values are estimated themselves.

It is possible that for some model specifications the data set
will lead to divergence, e.g., because the data contains too
little information about this effect, or because some effects are
`collinear' with each other. In such cases one must find out which
are the effects causing problems, and leave these out of the
model. Simulation can be helpful to distinguish between the
effects which should be fixed at a high positive or negative value
and the effects which should be left out because they are
superfluous.

When the distribution of the out-degrees is fitted poorly % (which can
% be investigated by the extra output requested
% by selecting \hyperlink{T_modelcode}{Model Code} larger than 10
% in the \hyperlink{T_S_options}{model options}),
an improvement
usually is possible either by including non-linear effects of the
out-degrees in the evaluation function.
%, or by changing to Model Type 2
%(see Section~\ref{S_modeltype}).

\subsection{Convergence problems}

If there are convergence problems, this may have several reasons.
\begin{itemize}
\item The data specification was incorrect (e.g., because the coding
      was not given properly).
\item The starting values were poor.
      Try restarting from the standard initial values
      (a certain non-zero value for the
      density parameter, and zero values for the other parameters);
      or from values obtained as the estimates for a simpler model
      that gave no problems.
      The initial default parameter values can be obtained
      by choosing the  \hyperlink{T_S_options}{model option}
      ``standard initial values".   \\
\iffalse
      When starting estimations with Model Type 2
      (see Section~\ref{S_modeltype}), there may be some problems to
      find suitable starting values.
      For Model Type 2, it is advised to start with unconditional estimation
      (see the \hyperlink{T_S_options}{model options})
      and a simple model,
      and to turn back to conditional estimation, using the current parameter
      values as initial estimates for new estimation runs, only when
      satisfactory estimates for a simple model have been found.
\fi
\item The model does not fit well in the sense that even with well-chosen
      parameters it will not give a good representation of the data.

      This can be the case, e.g., when there is a large heterogeneity
      between the actors which is not well represented by effects
      of covariates.
      The out-degrees and in-degrees are given in the begin of the \SI output
      to be able to check whether there are outlying actors having very high
      in- or out-degrees, or a deviating dynamics in their degrees.
      Strong heterogeneity between the actors will have to be
      represented by suitable covariates; if these are not available,
      one may define one or a few dummy variables each representing
      an outlying actor, and give this dummy variable an ego effect
      in the case of deviant out-degrees, and an alter effect in the
      case of deviant in-degrees.

      Another possibility is that there is time heterogeneity.
      Indications about this can be gathered also from the descriptives
      given in the start of the output file: the number of changes
      upward and downward, in the network and also -- if any -- in the
      dependent behavioral variable. If these do not show a smooth
      or similar pattern across the observations, then it may be useful
      to include actor variables representing time trends. These
      could be smooth -- e.g., linear -- but they also could be dummy variables
      representing one or more observational periods; these must be included
      as an ego effect to represent time trends in the tendency to make ties
      (or to display higher values of the behavior in question).
\item Too many weak effects are included. Use a smaller number of effects,
      delete non-significant ones, and increase complexity step by step.
      Retain parameter estimates from the last
      (simpler) model as the initial values for the new estimation procedure,
      provided for this model the algorithm converged
      without difficulties.
\item Two or more effects are included that are almost collinear
      in the sense that they can both explain the same observed structures.
      This will be seen in high absolute values of
      correlations between parameter estimates.
      In this case it may be better to exclude one of these effects from the model.
\item An effect is included that is large but of which the precise
      value is not well-determined (see above:
      \hyperlink{T_fix}{section on fixing parameters}).
      This will be seen in estimates and standard errors both being large
      and often in divergence of the algorithm.
      Fix this parameter to some large value.
      (Note: large here means, e.g., more than 5 or less than -5; depending
      on the effect, of course.)
\end{itemize}

If the algorithm is unstable, with parameter values (the left hand list
in the \SI window) changing too wildly, or with the algorithm
suddenly seeming stuck and not moving forward, the a solution may be
to simplify the model (perhaps later on making it more complex again
in forward parameter estimation steps); another solution may be
to decrease the initial gain parameter (see Section~\ref{S_options}).

\iffalse
If there are problems you don't understand, but you do know
something about the operation of {\SI}, you could take a look at
the file \textsf{{\em pname}.log\/}; and, if the problems occur in
the estimation algorithm, at the file \textsf{{\em pname}.cck}.
These files give information about what the program did, which may
be helpful in diagnosing the problem. E.g., you may look in the
\textsf{{\em pname}.cck} file to see if some of the parameters are
associated with positive values for the so-called
\hyperlink{T_quasiac}{quasi-auto\-correlations}.
If this happens from
subphase 2.2 onward for some parameters, these may be the parameters
that led to problems
in the estimation algorithm (e.g., because the corresponding
effect is collinear with other effects; or because they started
from unfortunate starting values; or because the data set contains
too little information about their value).
\fi

\iffalse
\begin{print}
%\newpage
\end{print}
\subsection{Composition change}

Example data files for a network of changing composition are also
provided with the program. These files are called {\sf
vtest2.dat}, {\sf vtest3.dat}, and {\sf vtest4.dat}. They contain
the same network data as the friendship data files of van de Bunt
(for these three observation times and with the same coding),
except that in these data some joiners and leavers were
artificially created. These actors were given the code
`\texttt{9}' for the observation moment at which they were not
part of the network. The attribute file {\sf vtestexo.dat}
contains the times at which the network composition changes (see
also the example in Section~\ref{S_comp}). This file is necessary
for the program to correctly include the times at which actors
join or leave the network. For example, the first line of the file
contains the values
\begin{verbatim}
1 0.7 3 0.0
\end{verbatim}
\noindent which indicates that the first actor joins the network
at fraction 0.7 of period 1 (the period between the first and
second observation moments) and leaves the network right after the
beginning of the third period, i.e., he/she does not leave the
network before the last observation at the third time point. Thus,
the first actor joins the network and then stays in during the
whole period being analyzed.
\fi

\newpage
\section{Multilevel network analysis}
\label{S_mulev}

For combining \SI results of several independent networks,
there are three options.
(`Independent'  networks here means that the sets of actors are
disjoint, and it may be assumed that there are no direct influences
from one network to another.)
The first two options assume that the parameters
of the actor-based models for the different
networks are the same -- except
for the basic rate parameters and for
those differences that are explicitly modeled by interactions
with dummy variables indicating the different networks.
The first and third options require that the number of observations is the same
for the different networks. This is not required for the
second option.
These methods can be applied for two or more networks.
\medskip

\noindent
The three options are:
\begin{enumerate}
\item Combining the different networks in one large network,
      indicating by structural zeros that ties between the
      networks are not permitted. This is explained in Section~\ref{S_struct}.\\
      The special effort to be made here is the construction
      of the data files for the large (combined) network.
\item Combining different sub-projects
      into one \emph{multi-group} project.
      The `sub-projects' are the same as the `different networks'
      mentioned here.
      This is explained in Section~\ref{S_multigroup}.\\
      A difference between options 1 and 2 is that the use
      of structural zeros (option 1) will lead to a default specification
      where the rate parameters are equal across networks
      (this can be changed by making the rate dependent upon dummy actor
      variables that indicate the different networks)
      whereas the multi-group option yields rate parameters
      that are distinct across different networks.
\item Analyzing the different networks separately, without any assumption
      that parameters are the same but using the same model specification,
      and post-processing the output files by a meta-analysis
      using \textsf{Siena08}.
      This is explained in Section~\ref{S_Siena08}.
\end{enumerate}
The first and second options will yield nearly the same results, with the
differences depending on the basic rate (and perhaps other) parameters that are
allowed to differ between the different networks, and of course
also depending on the randomness of the estimation algorithm.
The second option is more `natural' given the design of \SI and
will normally run faster than the first.
Therefore the second option seems preferable to the first.

The third option makes much less assumptions because parameters are not
constrained at all across the different networks.
Therefore the arguments usual in statistical modeling apply:
as far as assumptions is concerned, option 3 is safer;
but if the assumptions are satisfied (or if they are a good approximation),
then options 1 and 2 have higher power and are simpler.
However, option 3 requires that each of the different network data sets
is informative enough to lead to well-converged estimates;
this will not always be the case for small data sets,
and then options 1 or 2 are preferable.

When the data sets for the different networks are not too small
individually,
then a middle ground might be found in the following way.
Start with option 3. This will show for which parameters there are
important differences between the networks.
Next follow option 2, with interactions between the sub-project dummies
and those parameters for which there were important between-network
differences.
This procedure may work less easily when
the number of different networks is relatively high, because it may
then lead to too many interactions with dummy variables.

\subsection{Multi-group Siena analysis}
\label{S_multigroup}

The multi-group option `glues' several projects
(further referred to as \emph{sub-projects}) after each other
into one larger multi-group project.
These sub-projects
must have the same sets of variables of all kinds:
that is, the list of dependent networks, dependent behavioral variables,
actor covariates, and dyadic covariates must be the same
for the various sub-projects. The number of actors
and the number of observations can be different, however.
These sub-projects then are combined into one project
where the number of actors is the largest of the number of
actors of the sub-projects, and the number of observations
is the sum of the observations of the sub-projects.
As an example, suppose that three projects with names {\tt sub1}, {\tt sub2}, and
{\tt sub3} are combined. Suppose {\tt sub1} has 21 actors and
2 observations, {\tt sub2} has 35 actors and 4 observations,
and {\tt sub3} has 24 actors with 5 observations.
Then the combined multi-group project has 35 actors and 11 observations.
The step from observation 2 to 3 switches from sub-project {\tt sub1}
to sub-project {\tt sub2}, while
the step from observation 6 to 7 switches from sub-project {\tt sub2}
to {\tt sub3}. These switching steps do not correspond to simulations
of the actor-based model, because that would not be meaningful.

The different sub-projects are considered to be unrelated
except that they have the same model specification and the same
parameter values.

Given the potentially large number of periods that can be implied
by the multi-group option, it probably is advisable,
when using Method of Moments estimation, to use
the conditional estimation option.

In \SI version 4 the groups can be specified directly.



\subsection{Meta-analysis of Siena results}
\label{S_Siena08}

The program \textsf{Siena08.exe} is a relatively simple
multilevel extension to \si.
   This program must be run independently,
   after having obtained estimates for a common model
   estimated for several data sets.
   \textsf{Siena08} combines
   the estimates in a meta-analysis or multilevel analysis
   according to the methods of \citet{SnijdersBaerveldt03},
   and according to a Fisher-type combination of one-sided $p$-values.
   This combination method of \citet{Fisher32} is described in
\citet{HedgesOlkin85}
   and (briefly) in \citet[Chapter 3]{SnijdersBosker99}).
   Some more information is at the \SI website.

   For \SI version 4 the program \textsf{Siena08.exe} still must be
   checked and adapted. We hope that its role will be taken over
   by new functions in \rs.

   All \SI output files to be used must already exist,
   and the \emph{last estimation results} in these output files will be used.
   It is required that all these last estimation runs
   have the same set of estimated parameters, and of
   parameters tested by score tests.
   The program does not check that the score tests (if any)
   in the output files
   refer to the same parameters.
   It is also required that the decimal separator is a point, not a comma.
   (This depends on your Windows settings; if your output files have commas,
   just change all commas into points using an editor.)
   The \textsf{Siena08} project is the collection of output files
   to be combined, which is defined in the project \textsf{.mli} file.

   An easy way to operate \textsf{Siena08} is
%   to make a shortcut in Windows,
%   right-click on the shortcut and open the ``properties" tab,
%   and in the ``Target" -- which already contains the path and filename
%   of the \textsf{Siena08.exe} file -- add the projectname after the filename
%   (separated by a space).
%   An alternative is
    to make a batch file containing the single line
     \smallskip \\
     {\tt Siena08 ABC} \smallskip \\
   where \textsf{ABC} is the projectname.

   E.g., suppose the projectname is \textsf{ABC}.
   Then there must be a project file with the name \textsf{ABC.mli}
   (the root name ``\textsf{ABC}" can be chosen by the user,
   the extension name ``\textsf{mli}" is prescribed.)
   If the number of network evolution projects combined in this \textsf{Siena08} run
   is given by $K$, e.g. the $K=3$ projects with names A, B and C,
   then the file \textsf{ABC.mli} must give the project names
   on separate lines and in addition the options, as indicated
   in the following example file:

\begin{verbatim}
[This file contains specifications for the meta-analysis of Siena projects.]
[It serves as input for the Siena08 program.]

@1 [general information about the Siena project list ]
10 [number of projects, names follow:]
A
B
C

@2 [options for estimation of projects]
5 [upper bound for standard error in meta-analysis]
1 [code 0=estimate, 1=aggregate from .out-files, 2=generate .dsc-file]
1 [code 1=extra output]
0 [number of score tests]
\end{verbatim}

   \noindent
   Executing the batch file (e.g.\ by double clicking) will execute \textsf{Siena08}.
   To get started, try this out with a small data set.
   Some further explanation and example data are provided on the \SI website.
%   the data set included in the zip file \textsf{Siena08.zip} with project \textsf{CB}
%   and the subprojects \textsf{CB1\ CB3\ CB4}~.
\bigskip

\iffalse

<h4>
New version Siena08 (August 29, 2006)
</h4>

New in this version:<br>

<ul>
<li>A correction of the standard error for the first estimation stage<br>
    (not an important error in the earlier version because this is something we never use...).
<li>A plot of standard errors versus estimates;<br>
    this is important for two reasons:<br>
    1. it allows to see easily how many positive and negative individually significant
     parameter values are contained in the combined data set;<br>
    2. an assumption of the Snijders-Baerveldt \citeyearpar{SnijdersBaerveldt03}
    method for meta-analysis is that standard errors and true parameter values
    are uncorrelated; this can be visually checked from this plot.
<li>An extra method for combining the various classes, which does not make this assumption.<br>

    This method is based on Fisher's method for combining independent p-values.
    It is a double test:
    <ol>
    <li> for detecting if there are any networks with a positive parameter value, the null hypothesis
         tested is <br>
         H0: For all networks, the value of this parameter is zero or less than zero;
    <li> for detecting if there are any networks with a negative parameter value, the null hypothesis
         tested is <br>
         H0: For all networks, the value of this parameter is zero or greater than zero.
    </ol>
    For each of these combined tests, the p-value is given.
    It is advisable to use for each the significance level of alpha/2 (e.g., 0.025 if alpha = 0.05)
    which yields an overall combined test at significance level alpha.<br>

    Note that four different overall results are possible.
    Indicating the right-sided and the left-sided p-values by p_r and p_l, respectively,
    these possible results are (">=" means "greater than or equal to"):
    <ol>
    <li>p_r > &nbsp alpha/2, p_l > &nbsp alpha/2: &nbsp No evidence for any nonzero parameter values;
    <li>p_r <=   alpha/2, p_l > &nbsp alpha/2: &nbsp Evidence that some networks have a positive parameter value,
        no evidence for any negative parameter values;
    <li>p_r > &nbsp alpha/2, p_l <= alpha/2: &nbsp Evidence that some networks have a negative parameter value,
        no evidence for any positive parameter values;
    <li>p_r <=  alpha/2, p_l <=  alpha/2: &nbsp Evidence that some networks have a negative parameter value,
        and some others have a positive parameter value.
    </ol>

    If <b>all</b> networks have a zero parameter value, then the probability of result (1)
    is less than or equal to alpha.
</ul>
The .mli file is a bit different from the preceding version; when you look at
the .mli file in the zipped file below, you see immediately how it has to be made.
This means that you have to change all your earlier .mli files.<p>
<a href="Siena08_dec07.zip" target="_top">Download zipped file contain source, executable,
and example input files.</a>
<p>
Siena08 works on the basis of t-tests (t-ratio = estimate divided by standard error).
At this moment, I do not trust the t-tests for estimates which are large with also
a large standard error. The precise value of a lower threshold for
what constitutes a large standard error is not (yet) well determined. <br>
I propose to work with a threshold of 4 for the standard error;
if a tested parameter has a standard error larger than 4, then it is advisable
to redo the analysis in a specification where this parameter only is fixed to 0
and a score test is carried out for this parameter.
The result of the score test can be added as follows to the
Fisher-combination results of Siena08:
<ol>
<li> if the one-step estimate is positive, calculate
     c_r = -2*ln(0.5*p) and c_l = -2*ln(1 - 0.5*p)
     where p is the p-value obtained for the score test;<br>

     (the "*" symbol denotes multiplication)<br>
     (it may be noted that these are chi-squared values with d.f. = 2);
<li> if the one-step estimate is negative, calculate
     c_r = -2*ln(1.0 - 0.5*p) and c_l = -2*ln(0.5*p)
     where p is the p-value obtained for the score test;
<li> add c_r to the right-sided chi-squared value
     and c_l to the left-sided chi-square value reported by Siena08;
<li> these are again chi-squared values, but the degrees of freedom are 2 higher.
</ol>
A disadvantage of this procedure is that if there are two or more
tested parameters having large standard errors, this procedure including the estimation
must be carried out SEPARATELY for each tested parameter, because in testing each parameter
you wish to control for all other effects and therefore not fix any other
effects to 0.
<hr>
<h4>
Versies voor Liesbeth
</h4>

<ol>
<li> <a href="Siena3beta1s.zip" target="_top">Siena 3 beta 1s</a>: moet grote netwerken aankunnen (tot 500 actoren)
   en ook geen floating point errors meer bij Eggerslevmagleskolen enz.</a>
</ol>
<hr>
<p>
<a href="http://stat.gamma.rug.nl/snijders/siena.html" target="_top">SIENA website</a>
<p>
<a href="http://stat.gamma.rug.nl/stocnet/" target="_top">StOCNET website</a><p>
<p>
<a href="http://www.ppsw.rug.nl/~steglich/dynamics/index.htm" target="_top">Research Program
<i>Networks and Behavior</i> website</a>.

</body>
</html>
\fi


\begin{print}
\newpage
\end{print}
\section[Formulas for effects]{Mathematical definition of effects}
\label{S_math}


Here, the mathematical formulae for the definition of the effects
are given. In \citet{Snijders01,Snijders05} and \citet*{SteglichEA10},
further background to these formulae can be found.
The effects are grouped into effects for modelling network
evolution and effects for modelling behavioral evolution (i.e.,
the dynamics of dependent actor variables). Within each group of
effects, the effects are listed in the order in which they appear
in \si.

For two-mode (bipartite) networks, only a subset of the effects is
meaningful, since the first node set has only outgoing ties
and the second only incoming; for example, the reciprocity effect
is meaningless because there cannot be any reciprocal ties;
the out-degree popularity effect is meaningless because it refers to
incoming ties of actors with high out-degrees; and there are no similarity
effects of actor covariates.
There is one additional effect for two-mode networks, viz.,
the four-cycle effect.

\hypertarget{T_effpar}{
Some of the effects contain a number which is denoted in this section
by $c$, and called in this manual an \emph{internal effect parameter}.
}
(These are totally different from the statistical parameters which are
the weights of the effects in the objective function.)
%These numbers can be determined by the user
%by changing the \textsf{{\em pname}.mo} file
%described in Section~\ref{S_mo3file}.

\subsection{Network evolution}
The model of network evolution consists of the model of actors'
decisions to establish new ties or dissolve existing ties
(according to {\it evaluation} and {\it endowment functions}) and the
model of the timing of these decisions (according to the {\it rate
function}).
The objective function of the actor is the sum of the
network evaluation
function and the network endowment function
\begin{equation}
u^{\rm net}(x) \, = \, f^{\rm net}(x) + g^{\rm net}(x)  \ , \label{u_net}
\end{equation}
and a random term; where the evaluation function $f^{\rm net}(x)$ and the endowment
function $g^{\rm net}(x)$ are as defined in the following subsections.

For some effects %(those for which the function $f1i $ in Section~\ref{S_efdef} is non-zero)
the endowment function is implemented not for estimation by the Method of Moments
but only by the Maximum Likelihood or Bayesian method;
this is indicated below by ``endowment effect only likelihood-based''.

(It may be noted that the network evaluation function was called objective function,
and the endowment function was called gratification function, in
\citet{Snijders01}.)

\subsubsection{Network evaluation function}
\label{S_f}

The network evaluation function for actor $i$ is defined as
\begin{equation}
f^{\rm net}(x) \, = \, \sum_k \beta^{\rm net}_k s^{\rm net}_{ik}(x)   \label{f_net}
\end{equation}
where $\beta^{\rm net}_k$ are parameters and $s^{\rm net}_{ik}(x)$
are effects as defined below.

The potential effects in the \hypertarget{T_objective}{network
evaluation function}
are the following. Note that in all
effects where a constants $c$ occurs, this constant can be chosen
and changed by the user;
this is the internal effect parameter mentioned above.
\iffalse
Also note that the evaluation effects which
are a function only of the out-degree of actor $i$ are excluded for
Model Type 2.
\fi
For non-directed networks, the same formulae are used,
unless a different formula is given explicitly.
\begin{enumerate}
 \item {\em out-degree effect} or \emph{density effect},
 \hypertarget{T_density}{defined by the out-degree} \\
 $s^{\rm net}_{i\vit}(x) = x_{i+} = \sum_j x_{ij}$,\\
 where $x_{ij}=1$ indicates presence of a tie from $i$ to $j$
 while $x_{ij}=0$ indicates absence of this tie;

 \item {\em reciprocity effect},
 \hypertarget{T_reci}{defined by the number of reciprocated ties}\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij}\,x_{ji}$;

 \item {\em transitive triplets effect}, defined by the number of transitive
 patterns in $i$'s relations (ordered pairs of actors
 $(j,h)$ to both of whom $i$ is tied, while also $j$ is tied to $h$),\\
 for directed networks,
  $s^{\rm net}_{i\vit}(x) =  \sum_{j, h} x_{ij}\, x_{ih}\, x_{jh}\,$;\\
 and for non-directed networks,
 $s^{\rm net}_{i\vit}(x) =  \sum_{j < h} x_{ij}\, x_{ih}\, x_{jh}\,$;\\
 there was an error here until version 3.313,
 which amounted to combining the transitive triplets and transitive
 mediated triplets effects;

 \item {\em transitive mediated triplets effect}, defined by the number of transitive
 patterns in $i$'s relations where $i$ has the
 mediating position (ordered pairs of actors
 $(j,h)$ for which $j$ is tied to $i$ and $i$ to $h$, while also $j$ is tied to $h$),
 which is different from the transitive triplets effect only for directed networks,\\
  $s^{\rm net}_{i\vit}(x) =  \sum_{j, h} x_{ji}\, x_{ih}\, x_{jh}\,$;\\
 this cannot be used together with the transitive triplets effect in
 Method of Moments estimation, because of perfect collinearity
 of the fit statistics;

 \item {\em number of 3-cycles},\\
 $s^{\rm net}_{i\vit}(x) =  \sum_{j, h} x_{ij}\, x_{jh}\, x_{hi}\,$;


 \item for two-mode networks: the {\em number of 4-cycles},\\
 $s^{\rm net}_{i\vit}(x) =  \sum_{i_1, i_2, j_1, j_2}
            x_{i_1 j_1}\, x_{i_1 j_2}\, x_{i_2 j_1}\, x_{i_2 j_2}\,$;


 \item {\em transitive ties effect} (earlier called \emph{(direct and indirect ties) effect}),
 defined by
 the number of actors to whom $i$ is directly as well as indirectly tied,\\
 $s^{\rm net}_{i\vit}(x) =  \sum_j x_{ij} \, \max_h (x_{ih}\, x_{hj}) $;

 \item {\em betweenness count},\\
 $s^{\rm net}_{i\vit}(x) =  \sum_{j, h} x_{hi}\, x_{ij}\, (1-x_{hj})\,$;

 \item {\em balance}, defined by the similarity between the outgoing ties
 of actor $i$ and the outgoing ties of the other actors $j$ to whom
 $i$ is tied,
 \[ s^{\rm net}_{i\vit}(x) = \sum_{j=1}^n x_{ij} \neqsum{h}{h}{i,j}
 \left( b_0 - \mid x_{ih} - x_{jh} \mid \right)\, , \]
 where $b_0$ is a constant included to reduce the correlation
 between this effect and the density effect,
 \hypertarget{T_meanbal}{defined by}
 \[ b_0 = \frac{1}{(M-1)n(n-1)(n-2)} \sum_{m=1}^{M-1}
 \sum_{i, j=1}^n \neqsum{h}{h}{i,j}
 \mid x_{ih}(t_m) - x_{jh}(t_m) \mid \,.\]
 (In \SI versions before 3.324, this was divided by $n-2$, which for larger networks
 tended to lead to quite large estimates and standard errors.
 Therefore in version 3.324, the division by $n-2$
 -- which had not always been there -- was dropped.)

 \item {\em number of distances two effect},
 \hypertarget{T_dist2}{defined by}
 the number of actors to whom $i$ is indirectly tied
 (through at least one intermediary, i.e., at sociometric distance 2),\\
 $s^{\rm net}_{i\vit}(x) =  \#\{j \mid x_{ij} = 0,\, \max_h (x_{ih}\, x_{hj}) > 0 \}$;\\
 endowment effect only likelihood-based;

 \item {\em number of doubly achieved distances two effect},
 defined by
 the number of actors to whom $i$
 is not directly tied, and tied through twopaths via at least two intermediaries,\\
 $s^{\rm net}_{i\vit}(x) =  \#\{j \mid x_{ij} = 0,\, \sum_h (x_{ih}\, x_{hj}) \geq 2 \}$;\\
 endowment effect only likelihood-based;


 \item {\em number of dense triads}, defined as triads containing at least $c$ ties,\\
 $s^{\rm net}_{i\vit}(x) =  \sum_{j,h} x_{ij} \, I\{ x_{ij}\, + x_{ji}\, + x_{ih}\, + x_{hi}\,
 + x_{jh}\, + x_{hj}\,)\geq c \}\,$,\\
 where the `indicator function' $I\{A\}$ is 1 if the condition
 $A$ is fulfilled and 0 otherwise, and where $c$ is either 5 or 6;\\
  (this effect is superfluous and undefined for symmetric networks);

 \item {\em number of (unilateral) peripheral relations to dense triads},\\
 $s^{\rm net}_{i\vit}(x) =  \sum_{j,h,k} x_{ij} (1-x_{ji})(1-x_{hi})(1-x_{ki})
 I\{ (x_{jh}\,  + x_{hj}\, + x_{jk}\, + x_{kj}\, + x_{hk}\, + x_{kh}\,)\geq c \} \,$,\\
 where $c$ is the same constant as in the {\it dense triads} effect;\\
 for symmetric networks, the `unilateral' condition is dropped, and the definition is\\
 $s^{\rm net}_{i\vit}(x) =  \sum_{j,h,k} x_{ij} (1-x_{hi})(1-x_{ki})
 I\{ (x_{jh}\,  + x_{hj}\, + x_{jk}\, + x_{kj}\, + x_{hk}\, + x_{kh}\,)\geq c \} \,$;

 \item {\em in-degree related popularity effect}
 (earlier called {\em popularity} or {\em popularity of alter effect}), defined by
  the sum of
 the in-degrees of the others to whom $i$ is tied,\\
 $s^{\rm net}_{i\vit}(x) =  \sum_j x_{ij}\, x_{+j} =
 \sum_j x_{ij} \sum_h x_{hj} $;\\
 until version 3.313, this effect was multiplied by a factor $1/n$;

 \item {\em in-degree related popularity (sqrt) effect}
 (earlier called {\em popularity of alter (sqrt measure) effect}), defined by the sum of
 the square roots of the in-degrees of the others to whom $i$ is tied,\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij}\, \sqrt{x_{+j}} =
  \sum_j x_{ij} \sqrt{\sum_h x_{hj}} $;\\
 this often works better in practice than the raw popularity effect;
 also it is often reasonable to assume that differences between high in-degrees are
 relatively less important than the same differences between low
 in-degrees;

 \item {\em out-degree related popularity effect}
 (earlier called {\em activity} or {\em activity of alter effect}), defined by
  the sum of the out-degrees
 of the others to whom $i$ is tied,\\
 $s^{\rm net}_{i\vit}(x) =  \sum_j x_{ij}\, x_{j+} =
  \sum_j x_{ij} \sum_h x_{jh} $; \\
 until version 3.313, this effect was multiplied by a factor $1/n$;


 \item {\em out-degree related popularity (sqrt) effect}
 (earlier called {\em activity of alter (sqrt measure) effect}), defined by the sum of
 the square roots of the out-degrees of the others to whom $i$ is tied,\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij}\, \sqrt{x_{j+}} =
  \sum_j x_{ij} \sqrt{\sum_h x_{jh}} $;\\
 this often works better in practice than the raw activity effect
 for the same reasons as mentioned above for the sqrt measure of the popularity effect;

 \item[{\hspace*{-1ex}$\bigodot$}] for non-directed networks, the popularity and activity
 effects are taken together as ``degree effects'',
 since in-degrees and out-degrees are the same in this case;

 \item {\em in-degree related activity effect}, defined as
   the cross-product  of the actor's in- and out-degrees,\\
 $s^{\rm net}_{i\vit}(x) = x_{i+}\, x_{+i}$;\\
 endowment effect only likelihood-based;

 \item {\em in-degree related activity (sqrt) effect}, defined by  \\
 $s^{\rm net}_{i\vit}(x) = x_{i+}\,\sqrt{x_{+i}}$ ;

 \item {\em out-degree related activity effect}, defined as
   the squared out-degree of the actor,
 $s^{\rm net}_{i\vit}(x) = x^2_{i+}$;\\
 endowment effect only likelihood-based;

 \item {\em out-degree related activity (sqrt) effect}
 (earlier called {\em out-degree$\,\hat{\ }$(1.5)}), defined by  \\
 $s^{\rm net}_{i\vit}(x) = x^{1.5}_{i+} = x_{i+}\,\sqrt{x_{i+}}$ \\
 endowment effect only likelihood-based;

 \item {\em out-degree up to $c$}, where $c$ is some constant
 (internal effect parameter, see above),
 defined by  \\
 $s^{\rm net}_{i\vit}(x) = \max(x_{i+}\,,\, c)$;\\
 this is left out in later versions of \si;

 \item {\em square root out-degree}, defined by  \\
 $s^{\rm net}_{i\vit}(x) = \sqrt{x_{i+}}$;\\
 this is left out in later versions of \si;

 \item {\em squared (out-degree -- $c$)}, where $c$ is some constant,
 defined by  \\
 $s^{\rm net}_{i\vit}(x) = (x_{i+} - c)^2$,\\
 where $c$ is chosen to diminish the collinearity between this
 and the density effect;\\
 this is left out in later versions of \si;

 \item {\em sum of (1/(out-degree + $c$)}, where $c$ is some constant,
 defined by  \\
 $s^{\rm net}_{i\vit}(x) = 1/(x_{i+} + c)$;\\
 endowment effect only likelihood-based;

 \item {\em sum of (1/(out-degree + $c$)(out-degree + $c+1$))}, where $c$ is some constant,
 defined by  \\
 $s^{\rm net}_{i\vit}(x) = 1/(x_{i+} + c)(x_{i+} + c + 1)$;\\
 endowment effect only likelihood-based.

 \item {\em out-out degree$\,\hat{\ }$(1/c) assortativity},
 which represents the differential tendency for actors with high out-degrees
 to be tied to other actors who likewise have high out-degrees,\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij}\, {x_{i+}^{1/c}} {x_{j+}^{1/c}} $;\\
 $c$ can be 1 or 2 (the latter value is the default);

 \item {\em out-in degree$\,\hat{\ }$(1/c) assortativity},
 which represents the differential tendency for actors with high out-degrees
 to be tied to other actors who have high in-degrees,\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij}\, {x_{i+}^{1/c}} {x_{+j}^{1/c}} $;\\
 $c$ can be 1 or 2 (the latter value is the default);

 \item {\em in-out degree$\,\hat{\ }$(1/c) assortativity},
 which represents the differential tendency for actors with high in-degrees
 to be tied to other actors who have high out-degrees,\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij}\, {x_{+i}^{1/c}} {x_{j+}^{1/c}} $;\\
 $c$ can be 1 or 2 (the latter value is the default);

 \item {\em in-in degree$\,\hat{\ }$(1/c) assortativity},
 which represents the differential tendency for actors with high in-degrees
 to be tied to other actors who likewise have high in-degrees,\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij}\, {x_{+i}^{1/c}} {x_{+j}^{1/c}} $;\\
 $c$ can be 1 or 2 (the latter value is the default);

\setcounter{savenumi}{\value{enumi}}
\end{enumerate}
\smallskip
The effects for a dyadic covariate $w_{ij}$ are
\begin{enumerate}
\setcounter{enumi}{\value{savenumi}}

 \item {\em covariate (centered) main effect},\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij} \, (w_{ij} - \bar{w}) $\\
 where $\bar{w}$ is the mean value of $w_{ij}\,$;

 \item {\em covariate (centered) $\times$ reciprocity},\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij} x_{ji} \, (w_{ij} - \bar{w}) $.

 \item[{\hspace*{-1ex}$\bigodot$}]
 Three different ways can be modeled in which
 a triadic combination can be made between
 the dyadic covariate and the network.
 In the explanation, the dyadic covariate
 is regarded as a weighted network
 (which will be reduced to a non-weighted network if $w_{ij}$ only
 assumes the values 0 and 1).
 By way of exception, the dyadic covariate
 is not centered in these three effects
 (to make it better interpretable as a network).
 In the text and the pictures, an arrow with the letter $W$
 represents a tie according to the weighted network $W$.

 \item
\begin{minipage}[t]{.7\textwidth}
 {\em $WW=>X$ closure of covariate},\\
 $s^{\rm net}_{i\vit}(x) = \sum_{j \neq h} x_{ij}\, w_{ih}\, w_{hj}\,$;\\
 this refers to the closure of $W-W$ two-paths;
 each $W-W$ two-path
 $i \stackrel{W}{\rightarrow} h \stackrel{W}{\rightarrow} j$
 is weighted by the product $w_{ih} w_{hj}$
 and the sum of these product weights measures the
 strength of the tendency toward closure of
 these $W-W$ twopaths by a tie.
      \end{minipage}
\hfill
\begin{minipage}[t]{.15\textwidth}
\linethickness{0.3pt}
\vfill
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 2 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\put{{\small $W$}} at 2.1 1.9
\put{{\small $W$}} at 3.9 1.9
\arrow <2mm> [.2,.6]  from 2.2 1 to 3.8 1
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559
\arrow <2mm> [.2,.6]  from 3.1 2.559 to 3.9 1.1732
\endpicture
\end{center}
\vfill
\end{minipage}
\smallskip

 Since the dyadic covariates are represented by square arrays
 and not by edgelists, this will be a relatively time-consuming effect
 if the number of nodes is large.

\begin{minipage}[t]{.7\textwidth}
 \item {\em $WX=>X$ closure of covariate},\\
 $s^{\rm net}_{i\vit}(x) = \sum_{j \neq h} x_{ij}\, w_{ih}\, x_{hj}\,$;\\
 this refers to the closure of mixed $W-X$ two-paths;
 each $W-X$ two-path $i \stackrel{W}{\rightarrow} h \rightarrow j$
 is weighted by $w_{ih} $
 and the sum of these  weights measures the
 strength of the tendency toward closure of
 these mixed $W-X$ twopaths by a tie;
      \end{minipage}
\hfill
\begin{minipage}[t]{.15\textwidth}
\linethickness{0.3pt}
\vfill
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 2 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\put{{\small $W$}} at 2.1 2.0
\arrow <2mm> [.2,.6]  from 2.2 1 to 3.8 1
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559
\arrow <2mm> [.2,.6]  from 3.1 2.559 to 3.9 1.1732
\endpicture
\end{center}
\vfill
\end{minipage}

\begin{minipage}[t]{.7\textwidth}
 \item {\em $XW=>X$ closure of covariate},\\
 $s^{\rm net}_{i\vit}(x) = \sum_{j \neq h} x_{ij}\, x_{ih}\, w_{hj}\,$;\\
 this refers to the closure of mixed $X-W$ two-paths;
 each $X-W$ two-path $i \rightarrow h \stackrel{W}{\rightarrow} j$
 is weighted by $w_{hj} $
 and the sum of these  weights measures the
 strength of the tendency toward closure of
 these mixed $X-W$ twopaths by a tie.
      \end{minipage}
\hfill
\begin{minipage}[t]{.15\textwidth}
\linethickness{0.3pt}
\vfill
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 2 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\put{{\small $W$}} at 3.9 2.0
\arrow <2mm> [.2,.6]  from 2.2 1 to 3.8 1
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559
\arrow <2mm> [.2,.6]  from 3.1 2.559 to 3.9 1.1732
\endpicture
\end{center}
\vfill
\end{minipage}

\setcounter{savenumi}{\value{enumi}}
\end{enumerate}
\smallskip
For actor-dependent covariates $v_j$ (recall that these are
centered internally by \si) as well as for dependent behavior
variables (for notational simplicity here also denoted $v_j$;
these variables also are centered),
the following effects are available:
\begin{enumerate}
\setcounter{enumi}{\value{savenumi}}

 \item {\em covariate-alter} or {\em covariate-related popularity},
 defined by the sum of the covariate over all actors to whom $i$ has a tie,\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij}\, v_j$;

 \item {\em covariate squared - alter} or {\em squared covariate-related popularity},
 defined by the sum of the squared centered covariate over all actors to whom $i$ has a tie,
 (not included if the variable has range less than 2)\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij}\, v_j$;

 \item {\em covariate-ego} or {\em covariate-related activity},
 defined by $i$'s out-degree weighted by his covariate value,\\
 $s^{\rm net}_{i\vit}(x) = v_i \, x_{i+} $;

 \item {\em covariate-related similarity}, defined by the
 sum of centered similarity scores ${\rm sim}^v_{ij}$ between $i$
 and the other actors $j$ to whom he is tied,\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij} ({\rm sim}^v_{ij} - \widehat{{\rm sim}^v}) $,\\
 where $\widehat{{\rm sim}^v}$ is the mean of all similarity scores, which are defined as
 ${\rm sim}^v_{ij}=\frac{\Delta-\vert v_i - v_j \vert}{\Delta}$ with
 $\Delta=\max_{ij}\vert v_i - v_j \vert$ being the observed range of the covariate $v$
 (this mean is given in the output file just before the
 ``initial data description'');

 \item {\em covariate-related similarity $\times$ reciprocity}, defined by
 the sum of centered similarity scores for all
 reciprocal dyads in which $i$ is situated,\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij}x_{ji} ({\rm sim}^v_{ij} - \widehat{{\rm sim}^v}) $;

 \item \emph{same covariate}, which can also be called {\em covariate-related identity},
 defined by the
 number of ties of $i$ to all other actors $j$ who have
 exactly the same value on the covariate,\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij}\, I\{v_i = v_j \} $,\\
 where the indicator function $I\{v_i = v_j \} $ is 1 if the condition $\{v_i = v_j \} $
 is satisfied, and 0 if it is not;

 \item {\em same covariate $\times$ reciprocity}, defined by the
 number of reciprocated ties between $i$ and all other actors $j$ who have
 exactly the same value on the covariate,\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij} x_{ji}\, I\{v_i = v_j \} $;

 \item {\em covariate-ego $\times$ alter},
 defined by the product of $i$'s covariate and the sum of those of his alters,\\
 $s^{\rm net}_{i\vit}(x) = v_i \, \sum_j x_{ij}\, v_j $;

 \item {\em covariate-ego $\times$ alter $\times$ reciprocity},
 defined by the product of $i$'s covariate and the sum of those of his reciprocated alters,\\
 $s^{\rm net}_{i\vit}(x) = v_i \, \sum_j x_{ij}\,x_{ji}\, v_j $;

\iffalse
 \item {\em covariate-related similarity $\times$ popularity alter}, defined by
 the sum of centered similarity scores between $i$ and the
 other actors $j$ to whom he is tied, weighted by the indegree of
 these other actors,\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij}x_{+j} ({\rm sim}^v_{ij} - \widehat{{\rm sim}^v}) $.
\fi

 \item {\em ego $>$ alter for covariate},
 defined by the number of ties where $i$'s covariate
 is larger than alter's, while equality counts for half,\\
 $s^{\rm net}_{i\vit}(x) =  \sum_j x_{ij}\, \text{dsign}(v_i - v_j) $,\\
 where $\text{dsign}(d) = 0$ for $d < 0$, 0.5 for $d = 0$,
 and 1 for $d > 0$.

 \item {\em covariate of indirect ties}, defined by
 the sum of the covariate over the actors
 to whom $i$ is tied indirectly (at a geodesic distance of 2),\\
 $s^{\rm net}_{i\vit}(x) = \sum_j (1 -x_{ij})
                      \big( \max_h x_{ih}x_{hj} \big) v_j $.

\iffalse
 \item {\em user-defined interaction effects}
 as described in Section~\ref{S_int_eff}. The internal effect parameter
 is decomposed by \SI into its two or three constituents, see
 in the mentioned section. The interaction is defined on a tie basis:
 if two interacting effects are defined by
 $s^{\rm net}_{ia}(x) = \sum_j s^a_{ij}(x)$ and
 $s^{\rm net}_{ib}(x) = \sum_j s^b_{ij}(x)$
 (where $a$ and $b$ are calculated from the internal effect parameter $c$),
 then the interaction is defined by\\
 $s^{\rm net}_{i\vit}(x) = \sum_j s^a_{ij}(x) s^b_{ij}(x)$ .
\fi

\setcounter{savenumi}{\value{enumi}}
\end{enumerate}



\subsubsection{Multiple network effects}
\label{S_MultiNet}

If there are multiple dependent networks, the definition of
cross-network effects is such that always, one network has the
role of the dependent variable, while the other network, or
networks, have the role of explanatory variable(s).
In the following list the network in the role of dependent variable
is denoted by the tie variables $x_{ij}$, while the
tie variables $w_{ij}$ denote the network that is the
explanatory variable.

In the \SI output for projects with multiple networks,
the dependent network in each given effect is indicated by
the first part of the effect name.
In the list below, a more or less normally formulated name is given first, then the
name used in \SI between parentheses,
using $X$ as the name for the dependent network and $W$
as the name for the explanatory network.
Since this is a co-evolution model, \SI will include also the effects
where the roles of $X$ and $W$ are reversed.

The first three effects are dyadic. The first can be regarded
as a main effect; the reciprocity and mutuality effects
will require rather big data sets to be empirically distinguished
from each other.
\begin{enumerate}
 \item {\em Effect of W on X} ($X$: $W$),\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij} \, w_{ij}  $ ;\\
 $i \stackrel{W}{\rightarrow} j$ leads to  $i \stackrel{X}{\rightarrow} j$;

 \item {\em Effect of incoming W on X} ($X$: reciprocity with $W$),\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij} \, w_{ji}  $ ;\\
 this can be regarded as generalized exchange:
 $j \stackrel{W}{\rightarrow} i$ leads to  $i \stackrel{X}{\rightarrow} j$;

 \item {\em Effect of mutual ties in W on X} ($X$: mutuality with $W$),\\
 $s^{\rm net}_{i\vit}(x) = \sum_j x_{ij} \, w_{ij} \, w_{ji}  $ ;\\
 $j \stackrel{W}{\leftrightarrow} i$ leads to  $i \stackrel{X}{\rightarrow} j$;
\setcounter{savenumi}{\value{enumi}}
\end{enumerate}
\smallskip
The following five are degree-related effects, where nodal degrees
in the $W$ network have effects on popularity or activity in the
$X$  network. They use an internal effect parameter $p$, which
mostly will be 1 or 2.

To decrease correlation with other effects, the
$W$-degrees are centered by subtracting the value $\bar w$,
which is the average degree of $W$ across all observations.\\
THIS VALUE SHOULD BE GIVEN AS THE AVERAGE DEGREE IN THE INITIAL PART
OF THE OUTPUT.

\begin{enumerate}
\setcounter{enumi}{\value{savenumi}}
 \item {\em Effect of in-degree in W on X-popularity } ($X$: indegree$^{1/p}$ $W$ popularity)
 defined by   the sum of  the $W$-in-degrees of the others to whom $i$ is tied,
 for parameter $p = 2$ the square roots of the $W$-in-degrees:\\
 $s^{\rm net}_{i\vit}(x) =  \sum_j x_{ij}\, (w_{+j} - \bar w)^{1/p}  $;\\

 \item {\em Effect of in-degree in W on X-activity } ($X$: indegree$^{1/p}$ $W$ activity)
 defined by the $W$-in-degrees of $i$ (for $p = 2$ its square root)
 times $i$'s $X$-out-degree:\\
 $s^{\rm net}_{i\vit}(x) =  \sum_j x_{ij}\, (w_{+i} - \bar w)^{1/p}
                 =  x_{i+}\, (w_{+i} - \bar w_{+.}) ^{1/p} $;\\

 \item {\em Effect of out-degree in W on X-popularity } ($X$: outdegree$^{1/p}$ $W$ popularity)
 defined by   the sum of  the $W$-out-degrees of the others to whom $i$ is tied,
 for parameter $p = 2$ the square roots of the $W$-out-degrees:\\
 $s^{\rm net}_{i\vit}(x) =  \sum_j x_{ij}\, (w_{j+} - \bar w)^{1/p}  $;\\

 \item {\em Effect of out-degree  in W on X-activity } ($X$: outdegree$^{1/p}$ $W$ activity)
 defined by the $W$-out-degrees of $i$ (for $p = 2$ its square root)
 times $i$'s $X$-out-degree:\\
 $s^{\rm net}_{i\vit}(x) =  \sum_j x_{ij}\, (w_{i+} - \bar w)^{1/p} =
                           x_{i+}\, (w_{i+} - \bar w)^{1/p} $;\\

 \item {\em Effect of both in-degrees in W on X-popularity } ($X$: both indegrees$^{1/p}$ $W$ )
 defined by   the sum of  the $W$-in-degrees of the others to whom $i$ is tied
 multiplied by the centered $W$-in-degree of $i$,
 for parameter $p = 2$ the square roots of the $W$-in-degrees:\\
 $s^{\rm net}_{i\vit}(x) =  \sum_j x_{ij}\, (w_{+i} - \bar w)^{1/p} \, (w_{+j} - \bar w)^{1/p}  $;\\
 this can be regarded as an interaction between the effect of $W$-in-degree on $X$-popularity
 and the effect of $W$-in-degree on $X$-activity.
\setcounter{savenumi}{\value{enumi}}
\end{enumerate}
\smallskip
The betweenness effect is another positional effect:
a positional characteristic in the $W$ network affects the
ties in the $X$ network, but now the position is the betweenness count,
defined as the number of pairs of nodes that are not directly connected:
 $j \stackrel{W}{\nrightarrow} h$ ,
but that are connected through $i$:
 $j \stackrel{W}{\rightarrow} i  \stackrel{X}{\rightarrow} h$ .
 Again there is an internal effect parameter $p$, usually
1 or 2.
\begin{enumerate}
\setcounter{enumi}{\value{savenumi}}
 \item {\em Effect of W-betweenness on X-popularity } ($X$: betweenness$^{1/p}$ $W$ popularity)
 defined by   the sum of  the $W$-betweenness counts of the others to whom $i$ is tied:\\
 $s^{\rm net}_{i\vit}(x) =  \sum_j x_{ij}\, \left(\sum_{h,k; h \neq k}w_{hj}\,w_{jk}\,(1-w_{hk})\right)^{1/p}  $;\\
\setcounter{savenumi}{\value{enumi}}
\end{enumerate}
\smallskip
Finally there are four mixed triadic effects.
\begin{enumerate}
\setcounter{enumi}{\value{savenumi}}
\item
\begin{minipage}[t]{.7\textwidth}
 {\em agreement about W leading to X}, ($X$: from $W$ agreement)\\
 $s^{\rm net}_{i\vit}(x) = \sum_{j \neq h} x_{ij}\, w_{ih}\, w_{jh}\,$;\\
 this refers to agreement of actors with respect to their $W$-choices
 (structural equivalence with respect to outgoing $W$-choices)
 the contribution of the tie $i \stackrel{X}{\rightarrow} j$
 is proportional to
 the number of joint $W$ choices of others,
 $i \stackrel{W}{\rightarrow} h \stackrel{W}{\leftarrow} j$.
      \end{minipage}
\hfill
\begin{minipage}[t]{.15\textwidth}
\linethickness{0.3pt}
\vfill
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 2 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\put{{\small $W$}} at 2.1 1.9
\put{{\small $W$}} at 3.9 1.9
\put{{\small $X$}} at 3   0.6
\arrow <2mm> [.2,.6]  from 2.2 1 to 3.8 1
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559
\arrow <2mm> [.2,.6]  from  3.9 1.1732 to 3.1 2.559
\endpicture
\end{center}
\vfill
\end{minipage}
\smallskip
\item
\begin{minipage}[t]{.7\textwidth}
 {\em agreement in mutual W-ties leading to X}, ($X$: from $W$ mutual agreement)\\
 $s^{\rm net}_{i\vit}(x) = \sum_{j \neq h} x_{ij}\, w_{ih}\,  w_{hi}\, w_{jh}\, w_{hj}\,$;\\
 this refers to agreement of actors with respect to their mutual $W$-choices
 (structural equivalence with respect to mutual $W$-choices)
 the contribution  of the tie $i \stackrel{X}{\rightarrow} j$
 is proportional to
 the number of joint mutual $W$ choices of others,
 $i \stackrel{W}{\leftrightarrow} h \stackrel{W}{\leftrightarrow} j$.
      \end{minipage}
\hfill
\begin{minipage}[t]{.15\textwidth}
\linethickness{0.3pt}
\vfill
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 2 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\put{{\small $W$}} at 2.1 1.9
\put{{\small $W$}} at 3.9 1.9
\put{{\small $X$}} at 3   0.6
\arrow <2mm> [.2,.6]  from 2.2 1 to 3.8 1
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559
\arrow <2mm> [.2,.6]  from  3.9 1.1732 to 3.1 2.559
\arrow <2mm> [.2,.6]  from 2.9 2.559 to  2.1 1.1732
\arrow <2mm> [.2,.6]  from  3.1 2.559 to  3.9 1.1732
\endpicture
\end{center}
\vfill
\end{minipage}
\smallskip
 \item
\begin{minipage}[t]{.7\textwidth}
 {\em  W leading to agreement in X}, ($X$: $W$ to agreement)\\
 $s^{\rm net}_{i\vit}(x) = \sum_{j \neq h} x_{ij}\, w_{ih}\, x_{hj}\,$;\\
 this refers to the closure of mixed $W-X$ two-paths;
 the contribution of the tie $i \stackrel{X}{\rightarrow} j$
 is proportional to
 the number of mixed $W-x$ two-paths
 $i \stackrel{W}{\rightarrow} h \stackrel{X}{\rightarrow} j$.\\
 Note that since this is the evaluation function for actor $i$ with
 respect to network $X$, only the $x_{ij}$ tie indicator in the formula,
 corresponding to  the tie $i \stackrel{X}{\rightarrow} j$,
 is the dependent variable here.\\
 The interpretation is that actors have the tendency to make the same
 outgoing $X$-choices as those to whom they have a $W$-tie.
      \end{minipage}
\hfill
\begin{minipage}[t]{.15\textwidth}
\linethickness{0.3pt}
\vfill
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 2 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\put{{\small $W$}} at 2.1 1.9
\put{{\small $X$}} at 3.9 1.9
\put{{\small $X$}} at 3   0.6
\arrow <2mm> [.2,.6]  from 2.2 1 to 3.8 1
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559
\arrow <2mm> [.2,.6]  from 3.1 2.559 to 3.9 1.1732
\endpicture
\end{center}
\vfill
\end{minipage}
 \item
\begin{minipage}[t]{.7\textwidth}
 {\em mixed $WW=>X$ closure}, ($X$: closure of $W$)\\
 $s^{\rm net}_{i\vit}(x) = \sum_{j \neq h} x_{ij}\, w_{ih}\, w_{hj}\,$;\\
 this refers to the closure of $W-W$ two-paths;
 the contribution of the tie $i \stackrel{X}{\rightarrow} j$
 is proportional to
 the number of $W-W$ two-paths
 $i \stackrel{W}{\rightarrow} h \stackrel{W}{\rightarrow} j$.\\
 The interpretation is that actors have the tendency to make
 and maintain $X$-ties to those to whom they have an indirect
 (distance 2) $W$-tie: `$W$-ties of $W$-ties tend to become $X$-ties'.
      \end{minipage}
\hfill
\begin{minipage}[t]{.15\textwidth}
\linethickness{0.3pt}
\vfill
\begin{center}
\beginpicture
\setcoordinatesystem units <0.8cm,0.8cm> point at 4 3
\setplotarea x from 2 to 4, y from 0 to 3
\put{\large$\bullet$} at  2 1
\put{\large$\bullet$} at  4 1
\put{\large$\bullet$} at  3 2.732
\put{$i$} at 2 0.4
\put{$j$} at 4 0.4
\put{$h$} at 3 3.4
\put{{\small $W$}} at 2.1 1.9
\put{{\small $W$}} at 3.9 1.9
\put{{\small $X$}} at 3   0.6
\arrow <2mm> [.2,.6]  from 2.2 1 to 3.8 1
\arrow <2mm> [.2,.6]  from 2.1 1.1732 to 2.9 2.559
\arrow <2mm> [.2,.6]  from 3.1 2.559 to 3.9 1.1732
\endpicture
\end{center}
\vfill
\end{minipage}
\setcounter{savenumi}{\value{enumi}}
\end{enumerate}



\subsubsection{Network endowment function} \label{S_g}

The \hypertarget{T_gratification}{network endowment function}
is the way of modeling effects which operate in
different strengths for the creation and the dissolution of
relations.
The network endowment function is zero for creation of ties,
and is given by
\begin{equation}
g^{\rm net}(x) \, = \, \sum_k \gamma_k s^{\rm net}_{ik}(x)   \label{g_net}
\end{equation}
for dissolution of ties.
In this formula, the $\gamma_k$ are the parameters for the endowment function.
The potential effects $s^{\rm net}_{ik}(x) $ in this function, and their
formulae, are the same as in the evaluation function;
except that not all are available, as indicated in the preceding subsection.
For further explication, consult \citet{Snijders01, Snijders05};
(here, the `gratification function' is used rather than the endowment function),
\citet*{SnijdersEA07}, and \citet*{SteglichEA10}.

\begin{screen}
\newpage
\end{screen}
\subsubsection{Network rate function} \label{S_r}

The \hypertarget{T_rate}{network rate function} $\lambda^{\rm net}$
(lambda) is defined for Model Type 1 (which is the default Model
Type) as a product \[ \lambda^{\rm net}_i(\rho, \alpha, x, m) =
\lambda^{\rm net}_{i1} \lambda^{\rm net}_{i2} \lambda^{\rm net}_{i3}
\] of factors depending, respectively, on period $m$, actor
covariates, and actor position \citep[see][p.\ 383]{Snijders01}. The
corresponding factors in the rate function are the following:
\begin{enumerate}
 \item The dependence on the period can be represented by a simple factor
 \[ \lambda^{\rm net}_{i1} = \rho^{\rm net}_m \]
 for $m = 1, ..., M-1$. If there are only $M = 2 $ observations,
 \hypertarget{T_rho}{the basic rate parameter} is called $ \rho^{\rm net}$.

 \item The effect of actor covariates with values
 $v_{hi}$ can be represented by the factor
 \[ \lambda^{\rm net}_{i2} = \exp(\sum_h \alpha_h \, v_{hi})\,. \]

 \item The dependence on the position of the actor can be modeled
 as a function of the actor's out-degree, in-degree, and number
 of reciprocated relations, the `reciprocated degrees'.
 Define these by
 \[ x_{i+} = \sum_j x_{ij},\ x_{+i} = \sum_j x_{ji},\ x_{i(r)} = \sum_j x_{ij}x_{ji} \]
 (recalling that $x_{ii} = 0$ for all $i$).\\

 \iffalse
 Denoting the corresponding parameter by $\alpha_1$, the dependence
 on the out-degree is represented by
 \[ \lambda^{\rm net}_{i3} = \frac{x_{i+}}{n-1} \exp(\alpha_1) \+
 \left(1 - \frac{x_{i+}}{n-1}\right) \exp(- \alpha_1). \]
 This formula is motivated in \citet{SnijdersDuijn97}.
 This defines a linear function of the out-degree,
 parametrized in such a way that it is necessarily positive.\\
 For a general dependence on the out-degree, in-degree, and number
 of reciprocated relations, one can use an average of such terms, the
 second and third one depending in the analogous way on
 $x_{+i}$ and $x_{i(r)}$, respectively.\\
 \fi

The contribution of the out-degrees to $\lambda^{\rm net}_{i3}$
is a factor
 \[ \exp( \alpha_h \, x_{i+})\,, \]
if the associated parameter is denoted $\alpha_h$ for some $h$,
and similarly for the contributions of the in-degrees and the
reciprocated degrees.

 Also an exponential dependence on reciprocals of out-degrees can be specified;
 this can be meaningful because the rate effect of
 the out-degree becoming a value 1 higher might
 become smaller and smaller as the out-degree increases.
 Denoting again the corresponding parameter by $\alpha_h$
 (but always for different index numbers $h$),
 this effect multiplies the factor $\lambda^{\rm net}_{i3}$ by
% \[ 1 \+ \frac{\alpha_h}{1 \+ x_{i+}} \ . \]
 \[ \exp( \alpha_h / x_{i+} ) \ . \]
% This function was chosen so that for a parameter $\alpha_h = 0$,
% there is no effect (multiplication by a factor 1);
% and no problems (division by 0) occur when $ x_{i+} = 0$.
\end{enumerate}

\iffalse
\subsubsection{Network rate function for Model Type 2}

For Model Type 2 (see Section~\ref{S_modeltype}), the network rate
function is defined according to \citet{Snijders03} by
\begin{eqnarray*}
  \rho_m\, \lambda_{i+}(s) & = &  \rho_m\,\frac{\nu(s)\, \xi(s)}{1 \,+\, \xi(s)}\, , \\
  \rho_m\, \lambda_{i-}(s) & = &  \rho_m\, \frac{\nu(s-1)}{1 \,+\, \xi(s-1)} \ ,
\end{eqnarray*}
where $ \rho_m\,\lambda_{i+}(s)$ and $ \rho_m\,\lambda_{i-}(s)$
represent, respectively, the rate at which an actor of current
out-degree $s$ increases, or decreases, his out-degree by 1. The
parameter $\rho_m$ is a multiplicative effect of the observation
period.

Function $\xi$ (\emph{xi}) is called the distributional tendency
function and is represented according to \citet[formula (17)]{Snijders03} by
\[ \xi(s) \,=\, \exp\left(\alpha_1 \,-\, \alpha_2 \log(s+1) - \frac{\alpha_3}{s+1}\right)  \ . \]
where the names given in \SI are
\begin{itemize}
 \item $\alpha_1$ : out-degrees effect;
 \item $\alpha_2$ : logarithmic out-degree effect;
 \item $\alpha_3$ : factorial out-degree effect.
\end{itemize}
The reasons for these names and interpretation of the effects
can be found in \citet{Snijders03}.
To the exponent also effects of actor covariates can be added.

The so-called volatility function $\nu$ (\emph{nu}) is defined as
\[ \nu(s) \,=\, \left( 1 \,+\, \alpha_4 \, \frac{1}{s+1} \right) \ . \]
Also to this exponent effects of actor covariates can be added.
\fi

\subsection{Behavioral evolution}
The model of the dynamics of a dependent actor variable
consists of a model of actors' decisions (according to {\it
evaluation} and {\it endowment functions}) and a model of the timing
of these decisions (according to a {\it rate function}),
just like the model for the network dynamics. The
decisions now do not concern the creation or dissolution of
network ties, but whether an actor increases or decreases his
score on the dependent actor variable by one, or keeps it as it
is.

\subsubsection{Behavioral evaluation function}
\label{S_f_b}

Effects for the behavioral evaluation function $u^{\rm beh}$ can be
selected from the following.
Here the dependent variable is transformed to have an overall average value of 0;
in other words, $z$ denotes the original input variable
minus the overall mean, which is given in the output file under the heading
\emph{Reading dependent actor variables}.

\begin{enumerate}
 \item {\em behavioral shape effect},\\
 $s^{\rm beh}_{i\vit}(x) = z_i \,$,\\
 where $z_{i}$ denotes the value of the dependent behavior variable of actor $i$;

 \item {\em quadratic shape effect, or effect of the behavior upon itself},
 where the attractiveness of further steps up the behavior `ladder'
 depends on where the actor is on the ladder:\\
 $s^{\rm beh}_{i\vit}(x) =  z_i^2$.\\
 The position of this effect in the sequence of effects is different between
 versions 3 and 4 of \si.


 \item {\em average similarity effect}, defined by the
 average of centered similarity scores ${\rm sim}^z_{ij}$ between $i$
 and the other actors $j$ to whom he is tied,\\
 $s^{\rm beh}_{i\vit}(x) = x_{i+}^{-1} \, \sum_j x_{ij} ({\rm sim}^z_{ij} - \widehat{{\rm sim}^z}) $;\\
 (and 0 if $x_{i+} = 0$) ;

 \item {\em total similarity effect}, defined by the
 sum of centered similarity scores ${\rm sim}^z_{ij}$ between $i$
 and the other actors $j$ to whom he is tied,\\
 $s^{\rm beh}_{i\vit}(x) = \sum_j x_{ij} ({\rm sim}^z_{ij} - \widehat{{\rm sim}^z}) $;

 \item {\em indegree effect}, \\
 $s^{\rm beh}_{i\vit}(x) = z_i \sum_j x_{ji} $;

 \item {\em outdegree effect}, \\
 $s^{\rm beh}_{i\vit}(x) = z_i \sum_j x_{ij} $;

% \item {\em indegree up to $c$ effect}, where $c$ is a constant between 1 and $n-1$,\\
% $s^{\rm beh}_{i\vit}(x) = z_i I\{x_{+i} \leq c\}$,\\
% where again $I\{A\}$ denotes the indicator function of the condition $A$;

 \item {\em isolate effect}, the differential attractiveness of the behavior
  for isolates, \\
 $s^{\rm beh}_{i\vit}(x) = z_i I\{x_{+i} = 0 \}$,\\
 where again $I\{A\}$ denotes the indicator function of the condition $A$;


 \item
 {\em average similarity $\times$ reciprocity effect}, defined by the
 sum of centered similarity scores ${\rm sim}^z_{ij}$ between $i$
 and the other actors $j$ to whom he is reciprocally tied,\\
 $s^{\rm beh}_{i\vit}(x) = x_{i(r)}^{-1} \, \sum_j x_{ij} x_{ji} ({\rm sim}^z_{ij} - \widehat{{\rm sim}^z}) $;\\
 (and 0 if $x_{i(r)} = 0$) ;

 \item {\em total similarity $\times$ reciprocity effect}, defined by the
 sum of centered similarity scores ${\rm sim}^z_{ij}$ between $i$
 and the other actors $j$ to whom he is reciprocally tied,\\
 $s^{\rm beh}_{i\vit}(x) =  \sum_j x_{ij} x_{ji} ({\rm sim}^z_{ij} - \widehat{{\rm sim}^z}) $;

 \item {\em average similarity $\times$ popularity alter effect}, defined by the
 sum of centered similarity scores ${\rm sim}^z_{ij}$ between $i$
 and the other actors $j$ to whom he is  tied, multiplied by their indegrees, \\
 $s^{\rm beh}_{i\vit}(x) = x_{i+}^{-1} \, \sum_j x_{ij}  x_{+j} ({\rm sim}^z_{ij} - \widehat{{\rm sim}^z}) $;\\
 (and 0 if $x_{i+} = 0$) ;

 \item
 {\em total similarity  $\times$ popularity alter effect}, defined by the
 sum of centered similarity scores ${\rm sim}^z_{ij}$ between $i$
 and the other actors $j$ to whom he is tied, multiplied by their indegrees,\\
 $s^{\rm beh}_{i\vit}(x) =  \sum_j x_{ij} x_{+j} ({\rm sim}^z_{ij} - \widehat{{\rm sim}^z}) $;
\iffalse
 \\ if the parameter for this effect is equal to 1:\\
 {\em popularity alter effect}, defined by the
 average in-degrees of the other actors $j$ to whom $i$ is tied,\\
 $s^{\rm beh}_{i\vit}(x) =   x_{i+}^{-1} \, \sum_j x_{ij} x_{+j}  $;\\
 (and 0 if $x_{i+} = 0$) ;
\fi

 \item {\em average similarity $\times$ reciprocity $\times$ popularity alter effect}, defined by the
 sum of centered similarity scores ${\rm sim}^z_{ij}$ between $i$
 and the other actors $j$ to whom he is reciprocally tied, multiplied by their indegrees, \\
 $s^{\rm beh}_{i\vit}(x) = x_{i(r)}^{-1} \, \sum_j x_{ij} x_{ji} x_{+j} ({\rm sim}^z_{ij} - \widehat{{\rm sim}^z}) $;\\
 (and 0 if $x_{i(r)} = 0$) ;

 \item {\em total similarity $\times$ reciprocity $\times$ popularity alter effect}, defined by the
 sum of centered similarity scores ${\rm sim}^z_{ij}$ between $i$
 and the other actors $j$ to whom he is reciprocally tied, multiplied by their indegrees,\\
 $s^{\rm beh}_{i\vit}(x) =  \sum_j x_{ij} x_{ji} x_{+j} ({\rm sim}^z_{ij} - \widehat{{\rm sim}^z}) $;

 \item {\em average alter effect}, defined by the product of $i$'s
 behavior multiplied by the average behavior of his alters (a kind
 of ego-alter behavior covariance), \\
 $s^{\rm beh}_{i\vit}(x) =  z_i \big( \sum_j x_{ij}\, z_j \big)
                                / \big (\sum_j x_{ij}\big)  $\\
 (and the mean behavior, i.e. $0$, if the ratio is 0/0) ;

 \item {\em average reciprocated alter effect}, defined by the product of $i$'s
 behavior multiplied by the average behavior of his reciprocated alters, \\
 $s^{\rm beh}_{i\vit}(x) =  z_i \big( \sum_j x_{ij}\, x_{ji}\, z_j \big)
                                / \big (\sum_j x_{ij}\, x_{ji} \big)  $\\
 (and 0 if the ratio is 0/0) ;


 \item {\em dense triads effect}, defined by the number of dense triads in which actor $i$ is
 located, \\
 $s^{\rm beh}_{i\vit}(x) =  z_i \sum_{j,h} I\{ x_{ij}\, + x_{ji}\, + x_{ih}\, + x_{hi}\,
 + x_{jh}\, + x_{hj}\,)\geq c \}\,$,\\
 where $c$ is either 5 or 6; \\
 \emph{this is currently not correctly implemented in \SI 3 };

 \item {\em peripheral effect}, defined by the number of dense triads to which actor $i$ stands
 in a unilateral-peripheral relation,\\
 $s^{\rm beh}_{i\vit}(x) =  z_i \sum_{j,h,k} x_{ij} (1-x_{ji})(1-x_{hi})(1-x_{ki})
 I\{ x_{ij}\,  + x_{ji}\, + x_{ih}\, + x_{hi}\, + x_{jh}\, + x_{hj}\,)\geq c \} \,$,\\
 where $c$ is the same constant as in the {\it dense triads} effect;\\
 for directed networks, the unilateral condition is dropped, and the effect is\\
 $s^{\rm beh}_{i\vit}(x) =  z_i \sum_{j,h,k} x_{ij} (1-x_{hi})(1-x_{ki})
 I\{ x_{ij}\,  + x_{ji}\, + x_{ih}\, + x_{hi}\, + x_{jh}\, + x_{hj}\,)\geq c \} \,$;\\
 \emph{this is currently not correctly implemented in \SI 3 };

 \item {\em reciprocated degree effect}, \\
 $s^{\rm beh}_{i\vit}(x) = z_i \sum_j x_{ij}\,x_{ji} $;

 \item {\em average similarity $\times$ popularity ego effect}, defined by the
 sum of centered similarity scores ${\rm sim}^z_{ij}$ between $i$
 and the other actors $j$ to whom he is  tied, multiplied by ego's indegree, \\
 $s^{\rm beh}_{i\vit}(x) =  x_{+i} \, x_{i+}^{-1} \sum_j x_{ij} ({\rm sim}^z_{ij} - \widehat{{\rm sim}^z}) $;\\
 (and 0 if $x_{i+} = 0$) ;\\
 because of collinearity, under the Method of Moments this cannot be estimated together with the
  average similarity $\times$ popularity alter effect.

\setcounter{savenumi}{\value{enumi}}
\end{enumerate}
\smallskip
For each actor-dependent covariate $v_j$ (recall that these are
centered internally by \SI) as well as for each of the other
dependent behavior variables (for notational simplicity here also
denoted $v_j$), there is one main effect.
\iffalse
 and one interaction effect,
the latter of which is a choice among three, dependent on the
\hyperlink{T_effpar}{internal parameter} for this effect:
\begin{enumerate}
\setcounter{enumi}{\value{savenumi}}
 \item {\em covariate effect},\\
 $s^{\rm beh}_{i\vit}(x) = z_{i} v_{i}\,$;\\
 here too, the other dependent behavioral variables are centered so that they
 have overall mean 0;
 \item depending on the parameter value (1, 2, or 3):
 \begin{enumerate}
   \item[value 1:] {\em interaction of actor variable with average similarity},\\
 $s^{\rm beh}_{i\vit}(x) = (v_i / x_{i+}) \,
         \sum_j x_{ij} ({\rm sim}^z_{ij} - \widehat{{\rm sim}^z}) $;\\
    (and 0 if $x_{i+} = 0$) ;
   \item[value 2:] {\em interaction of actor variable with total similarity},\\
 $s^{\rm beh}_{i\vit}(x) = v_i \,
         \sum_j x_{ij} ({\rm sim}^z_{ij} - \widehat{{\rm sim}^z}) $;\\
   \item[value 3:] {\em interaction of actor variable with average alter},\\
 $s^{\rm beh}_{i\vit}(x) =  v_i \, z_i \big( \sum_j x_{ij}\, z_j \big)
                                / \big (\sum_j x_{ij}\big)  $\\
 (and the mean behavior, i.e. $0$, if the ratio is 0/0) ;
 \end{enumerate}
\item There are also {\em user-defined interaction effects}
      between actor variables, defined as the product of two
      or three grand mean centered variables.
      (If these include the dependent variable itself,
      special formulae are used for the change statistic.)


\setcounter{savenumi}{\value{enumi}}
\end{enumerate}
\fi

\subsubsection{Behavioral endowment function}
Also the behavioral model knows the distinction between evaluation and
endowment effects. The formulae of the effects that can be included
in the behavioral endowment function $e^{\rm beh}$ are the same as
those given for the behavioral evaluation function. However, they enter
calculation of the endowment function only when the actor considers
decreasing his behavioral score by one unit (downward steps), not
when upward steps (or no change) are considered. For more details,
consult
\citet*{SnijdersEA07} and
\citet*{SteglichEA10}.

The statistics reported as \emph{dec.\ beh.} (decrease in behavior)
are the sums of the changes in actor-dependent values
for only those actors who decreased in behavior.
More precisely, it is
\begin{equation}
\sum_{m=1}^{M-1} \sum_{i=1}^n I\{z_{i}(t_{m+1}) < z_{i}(t_m) \}\,
     \big( s^{\rm beh}_{ik}(x(t_{m+1})) -  s^{\rm beh}_{ik}(x(t_m))   \big) ,
\end{equation}
where $M$ is the number of observations, $x(t_m)$ is the observed situation
at observation $m$, and the indicator function $I\{A\}$ is 0 if event $A$ is true
and 0 if it is untrue.

\subsubsection{Behavioral rate function}
The behavioral rate function $\lambda^{\rm beh}$ consists of a
constant term per period, \[ \lambda^{\rm beh}_{i} = \rho^{\rm
beh}_m \] for $m = 1, ..., M-1$.




\newpage
\section{Parameter interpretation}
\label{S_interpret}

This section still is in development.

\subsection{Longitudinal models}

The main `driving force' of the actor-oriented model
is the evaluation function
\citep[in earlier publications called objective function,
see][]{Snijders01, Snijders05} given in formula (\ref{f_net})
(for the network) as
\[
f^{\rm net}(x) \, = \, \sum_k \beta^{\rm net}_k \, s^{\rm net}_{ik}(x)   \ .
\]
The objective function can be regarded as the ``attractiveness"
of the network (or behavior, respectively) for a given actor.
For getting a feeling of what are small and large values,
is is helpful to note that the objective functions are
used to compare how attractive various different tie changes are,
and for this purpose random disturbances are added
to the values of the objective function with standard deviations
equal\footnote{More exactly, the value is $\sqrt{\pi^2/6}$,
the standard deviation of the Gumbel
distribution; see \citet{Snijders01}.} to 1.28.

An alternative interpretation is that when actor $i$ is making
a `ministep', i.e., a single change in his outgoing ties
(where no change also is an option), and
$x_a$ and $x_b$ are two possible results of this ministep,
then $f^{\rm net}(x_b) - f^{\rm net}(x_a)$ is the log odds ratio
for choosing between these two alternatives -- so that the ratio
of the probability of $x_b$ and $x_a$ as next states is
\[
  \exp(f^{\rm net}(x_b) - f^{\rm net}(x_a)) \ .
\]
Note that, when the current state is $x$, the possibilities
for $x_a$ and $x_b$ are $x$ itself (no change), or $x$ with one extra
outgoing tie from $i$, or $x$ with one fewer outgoing tie from $i$.
Explanations about log odds ratios can be found
in texts about logistic regression and loglinear models.

The evaluation function is a weighted sum of `effects'
$s^{\rm net}_{ik}(x)$.
Their formulae can be found in Section~\ref{S_f}.
These formulae, however, are defined as a function of the whole
network $x$, and in most cases the contribution of a single tie
variable $x_{ij}$ is just a simple component of this formula.
The contribution to $s^{\rm net}_{ik}(x)$
of adding the tie $i \rightarrow h$ minus the
contribution of adding the tie $i \rightarrow j$ is the log odds ratio
comparing the probabilities of $i$ sending a new tie to $h$ versus
sending the tie to $j$, if all other effects $s^{\rm net}_{ik}(x)$
yields the same values for these two hypothetical new configurations.

For example, suppose that actors $j$ and $h$,
actual or potential relation partners of actor $i$,
have exactly the same network
position and the same values on all variables included in the model,
except that for some actor variable $V$ for which only the
popularity (alter) effect is included in the model,
actor $h$ is one unit higher than actor $j$: $v_h = v_j + 1$.
It can be seen in Section~\ref{S_f} that
the popularity (alter) effect is defined as
\[
s^{\rm net}_{ik}(x) \, = \,  \sum_j x_{ij}\, v_j \ .
\]
The contribution to this formula made by a single tie variable,
i.e., the difference made by filling in $x_{ij} = 1$ or $x_{ij} = 0$
in this formula, is just $v_j$.
Let us denote the weight of the $V$-alter effect by $\beta_k$.
Then, the difference between extending a tie to $h$ or to $j$
that follows from the $V$-alter effect is
$\beta_k \times (v_h - v_j) = \beta_k \times 1 = \beta_k$.

Thus, in this situation, $\beta_k$ is the log odds ratio of the probability
that $h$ is chosen compared to the probability that $j$ is chosen.
E.g., if $i$ currently has a tie neither to $j$ nor to $h$,
and supposing that $\beta_k = 0.3$, the probability for $i$ to
extend a new tie to $h$ is $e^{0.3} = 1.35$ times as high
as the probability for $i$ to extend a new tie to $j$.

\subsubsection{Ego -- alter selection tables}

When some variable $V$ occurs in several effects in the model,
then its effects can best be understood
by considering all these effects simultaneously.
For example, if in a network dynamics model the
ego, alter, and similarity effects of a variable $V$ are specified,
then the formulae for their contribution can be obtained
from the components listed in Section~\ref{S_f} as
\begin{equation}
 \beta_{\rm ego}\, v_i \, x_{i+} \, + \, \beta_{\rm alter}  \sum_j x_{ij}\, v_j \, + \,
        \beta_{\rm sim}  \sum_j x_{ij} ({\rm sim}^v_{ij} - \widehat{{\rm sim}^v}) \ ,
        \label{eq_contr_V}
\end{equation}
where the similarity score is
${\rm sim}^v_{ij}= 1 - \frac{\vert v_i - v_j \vert}{\Delta_V}$, with
$\Delta_V=\max_{ij}\vert v_i - v_j \vert$ being the observed range of the covariate $v$
and where $\widehat{{\rm sim}^v}$ is the mean of all similarity scores.
The superscript $^{\rm net}$ is left out of the notation for the parameters
in order not to clutter the notation.

Similarly to how it was done above, the contribution to~(\ref{eq_contr_V})
of the tie from $i$ to $j$, represented by the
single tie variable $x_{ij}$ -- i.e., the difference
between the values of~(\ref{eq_contr_V}) for $x_{ij}=1$
and $x_{ij}=0$  --  can be calculated from this formula.
It should be noted that all variables are internally centered by \si,
and that the mean values used for the centering
are given near the beginning of the input file.
This is made explicit in the following by the subtraction
of the mean $\bar v$. The contribution of
\begin{eqnarray}
  & & \beta_{\rm ego}\, (v_i - \bar v)  \, + \, \beta_{\rm alter}\,  (v_j - \bar v) \, + \,
        \beta_{\rm sim} \, ({\rm sim}^v_{ij} - \widehat{{\rm sim}^v} ) \nonumber \\
 && = \, \beta_{\rm ego}\, (v_i - \bar v)  \, + \, \beta_{\rm alter}\,  (v_j - \bar v) \, + \,
        \beta_{\rm sim} \,  \Big( 1 - \frac{\vert v_i - v_j \vert}{\Delta_V} - \widehat{{\rm sim}^v} \Big) \ .
                 \label{eq_sel}
\end{eqnarray}
%Since constant additive terms do not make a difference, we can just as well work with
%\begin{equation}
% \beta_{\rm ego}\, v_i  \, + \, \beta_{\rm alter}\,  v_j \,
%         - \, \beta_{\rm sim} \,  \frac{\vert v_i - v_j \vert}{\Delta_V}  \ .
%                 \label{eq_sel}
%\end{equation}
From this equation a table can be made that gives the
outcome of (\ref{eq_sel}) for some values of $v_i$ and $v_j$.


This can be concretely carried using the data set {\sf s50}
which is an excerpt of 50 girls in the data set used in
\citet{PearsonMichell00, PearsonWest03,
SteglichEA06} and \citet{SteglichEA10}.
We refer to any of these papers for a further description of the data.
The friendship network data over 3 waves are in
the files {\sf s50-network1.dat}, {\sf s50-network2.dat},
and {\sf s50-network3.dat}.
We also use the attribute data
for alcohol use, {\sf s50-alcohol.dat}, as a dependent variable.
It can be seen from the \SI output file using these data that
the alcohol use variable assumes values from 1 to 5, with overall mean
equal to $\bar v = 3.113$, and mean of the similarity variable $\widehat{{\rm sim}^v} = 0.6983$.
Drug use is used as a changing actor variable, with
range 1--4, average $\bar v = 1.5$ and average dyadic similarity $\widehat{{\rm sim}^v} = 0.7533$.

Suppose that we fit a model of network-behavior co-evolution to this data set
with for the network evolution the effects of outdegree, reciprocity,
transitive ties, number of distances two,
the ego, alter, and similarity effects of alcohol use,
as well as the ego, alter, and similarity effects of drug use;
and for the behavior (i.e., alcohol) dynamics
the shape effect,
the effect of alcohol on itself (quadratic shape effect),
and the average similarity effect.

The results obtained are given in the following
part of the output file.

\begin{verbatim}
Network Dynamics
 1. rate:  constant network rate (period 1)              8.2357  (   1.6225)
 2. rate:  constant network rate (period 2)              5.6885  (   0.8434)
 3. eval:  outdegree (density)                          -2.1287  (   0.1565)
 4. eval:  reciprocity                                   2.3205  (   0.2132)
 5. eval:  transitive ties                               0.2656  (   0.2025)
 6. eval:  number of actors at distance 2               -0.9947  (   0.2173)
 7. eval:  drink alter                                   0.0899  (   0.1184)
 8. eval:  drink ego                                    -0.0100  (   0.1087)
 9. eval:  drink similarity                              0.8994  (   0.5864)
10. eval:  drug use alter                               -0.1295  (   0.1282)
11. eval:  drug use ego                                  0.1362  (   0.1253)
12. eval:  drug use similarity                           0.6650  (   0.3381)

Behavior Dynamics
13. rate:  rate drink period 1                           1.3376  (   0.3708)
14. rate:  rate drink period 2                           1.8323  (   0.4546)
15. eval:  behavior drink shape                          0.3618  (   0.1946)
16. eval:  behavior drink average similarity             3.9689  (   2.2053)
17. eval:  behavior drink: effect from drink            -0.0600  (   0.1181)
\end{verbatim}

\noindent
We interpret here the parameter estimates for the effects of drinking behavior
and drug use without being concerned with the significance, or lack thereof.
For the drinking behavior, formula (\ref{eq_sel}) yields (rounded to two decimals)
\[
  -0.01 \, (v_i - \bar v) \,+\, 0.09 \, (v_j - \bar v) \,+\,
     0.90 \Big( 1 \, - \, \frac{\vert v_i - v_j \vert}{ \Delta_V }  - 0.70 \Big) \ .
\]
The results can be tabulated as follows.
\bigskip

\begin{center}
\begin{tabular}{l  r@{.}l  r@{.}l  r@{.}l  r@{.}l  r@{.}l }
$ z_i \ \  \backslash  \ \ z_j $   &  \mcc{2}{ 1} & \mcc{2}{ 2} & \mcc{2}{ 3} & \mcc{2}{ 4} &  \mcc{2}{ 5}
\separationb
 1        &    0&10    &  --0&03    &  --0&17    &  --0&30    &  --0&44   \\
 2        &  --0&13    &    0&18    &    0&05    &  --0&09    &  --0&22   \\
 3        &  --0&37    &  --0&05    &    0&26    &    0&13    &  --0&01   \\
 4        &  --0&60    &  --0&29    &    0&03    &    0&34    &    0&21   \\
 5        &  --0&84    &  --0&52    &  --0&21    &    0&11    &    0&42   \\
\hline
\end{tabular}
\end{center}


% see program calctable
This table shows the preference for similar alters: in all rows,
the highest value is at the diagonal ($v_j = v_i$).
The ego and alter parameters are close to 0, therefore the similarity
effect is dominant. However, note that the formula uses raw values for $v_i$
and $v_j$ but divides the values for the absolute difference
$ \vert v_i - v_j \vert$ by $\Delta_V$ which here is $5-1=4$.
Therefore the weight of 0.09 for the alter effect is not
completely negligible compared to the weight of 0.90
for the similarity effect. The positive alter effect leads to a preference
for ties to alters with a high $v_j$ value which
goes against the similarity effect for $v_i = 1$ but strengthens
the similarity effect for $v_i = 5$. The table shows that the net resulting
preference for similar others is strongest for actors (egos) high on drinking behavior,
and weakest for actors in the middle and low range of drinking behavior.
\medskip

For drug use, the formula yields
\[
  0.14 \, (v_i - \bar v) \,-\, 0.13 \, (v_j - \bar v) \,+\,
       0.67 \Big( 1 \, - \, \frac{ \vert v_i - v_j \vert }{ \Delta_V }  \, -\, 0.7533  \Big) \ ,
\]
which leads to the following table.
\bigskip

\begin{center}
%\begin{tabular}{l| r@{.}l  r@{.}l  r@{.}l  r@{.}l }
\begin{tabular}{l r@{.}l  r@{.}l  r@{.}l  r@{.}l }
$ z_i \ \  \backslash  \ \ z_j $   &  \mcc{2}{ 1} & \mcc{2}{ 2} & \mcc{2}{ 3} &  \mcc{2}{ 4}
\separationb
 1        &    0&16    &  --0&19    &  --0&54    &  --0&89   \\
 2        &    0&08    &    0&17    &  --0&18    &  --0&53   \\
 3        &  --0&01    &    0&08    &    0&17    &  --0&18   \\
 4        &  --0&10    &  --0&00    &    0&09    &    0&18   \\
\hline
\end{tabular}
\end{center}
% see program calctable
In each row the highest value is at the diagonal, which shows that
indeed everybody prefers to be friends with similar others also
with respect to drug use.
The negative alter effect supports this for low $v_i$ values
and counteracts it for high $v_i$ values.
This is seen in the table in the strong preference of low drug users
($v_i = 1$) for others who are low on drug use, and the very weak
preference for high drug users ($v_i = 4$) for others
also high on drug use.
\bigskip

An alternative specification uses the drink ego $\times$ drink alter interaction
together with the drink squared alter effect in the network dynamics model,
and similarly for drug use; for the behavior dynamics,
an alternative specification uses the average alter effect.
This leads to the following table of results.

\begin{verbatim}
Network Dynamics
 1. rate:  constant network rate (period 1)              8.0978  (   1.5118)
 2. rate:  constant network rate (period 2)              5.7781  (   0.9474)
 3. eval:  outdegree (density)                          -2.1333  (   0.2196)
 4. eval:  reciprocity                                   2.3033  (   0.2184)
 5. eval:  transitive ties                               0.2430  (   0.2059)
 6. eval:  number of actors at distance 2               -1.0011  (   0.2275)
 7. eval:  drink alter                                   0.1041  (   0.1348)
 8. eval:  drink squared alter                           0.0141  (   0.1329)
 9. eval:  drink ego                                     0.0078  (   0.1157)
10. eval:  drink ego x drink alter                       0.1655  (   0.1095)
11. eval:  drug use alter                               -0.2603  (   0.2436)
12. eval:  drug use squared alter                       -0.0249  (   0.1945)
13. eval:  drug use ego                                 -0.0214  (   0.1454)
14. eval:  drug use ego x drug use alter                 0.1976  (   0.1146)

Behavior Dynamics
15. rate:  rate drink period 1                           1.3218  (   0.3632)
16. rate:  rate drink period 2                           1.7884  (   0.5053)
17. eval:  behavior drink shape                          0.3820  (   0.2421)
18. eval:  behavior drink average alter                  1.1414  (   0.6737)
19. eval:  behavior drink: effect from drink            -0.5428  (   0.2839)
\end{verbatim}

For this specification, the formulae in Section~\ref{S_f} imply that the
components in the network objective function corresponding to the effects
of variable $V$ are
\begin{equation}
 \beta_{\rm ego}\, (v_i - \bar v) \, x_{i+} \, + \, \beta_{\rm alter}  \sum_j x_{ij}\, (v_j - \bar v)
 \, + \, \beta_{\rm sq.\ alter}  \sum_j x_{ij}\, (v_j - \bar v)^2  \, + \,
        \beta_{\rm e \times a}  \sum_j x_{ij}\,(v_i - \bar v) \,(v_j - \bar v)  \ .
        \label{eq_contr2_V}
\end{equation}
The contribution of the single tie variable $x_{ij}$ to this formula is equal to
\begin{equation}
  \beta_{\rm ego}\, (v_i - \bar v)  \, + \, \beta_{\rm alter}\,  (v_j - \bar v) \, + \,
      \beta_{\rm sq.\ alter}  \, (v_j - \bar v)^2  \,+\,
        \beta_{\rm e \times a} \, (v_i - \bar v)\, (v_j - \bar v) \ .
                 \label{eq_sel2}
\end{equation}
Filling in the estimates for the effects of drinking behavior yields
\[
  0.01 \, (v_i - \bar v)  \, + \,  0.10 \, (v_j - \bar v) \, + \,
      0.01 \, (v_j - \bar v)^2  \,+\,  0.17   (v_i - \bar v)\, (v_j - \bar v) \ .
\]
and this gives the following table.
\iffalse
\begin{center}
\begin{tabular}{l r@{.}l  r@{.}l  r@{.}l }
$ v_i \ \  \backslash  \ \ v_j $   &  \mcc{2}{ 1} & \mcc{2}{ 3} &  \mcc{2}{ 5}
\separationb
 1        &    0&54    &    0&01    &  --0&45   \\
 3        &  --0&15    &  --0&01    &    0&19   \\
 5        &  --0&83    &  --0&03    &    0&83   \\
\hline
\end{tabular}
\end{center}
\fi
\begin{center}
\begin{tabular}{l r@{.}l  r@{.}l  r@{.}l  r@{.}l  r@{.}l }
$ v_i \ \  \backslash  \ \ v_j $   &  \mcc{2}{ 1} & \mcc{2}{ 2} & \mcc{2}{ 3} & \mcc{2}{ 4} &  \mcc{2}{ 5}
\separationb
 1        &    0&54    &    0&27    &    0&01    &  --0&23    &  --0&45   \\
 2        &    0&20    &    0&09    &    0&00    &  --0&07    &  --0&13   \\
 3        &  --0&15    &  --0&09    &  --0&01    &    0&08    &    0&19   \\
 4        &  --0&49    &  --0&26    &  --0&02    &    0&24    &    0&51   \\
 5        &  --0&83    &  --0&44    &  --0&03    &    0&39    &    0&83   \\
\hline
\end{tabular}
\end{center}



For drug use we obtain the formula
\[
  -0.02 \, (v_i - \bar v)  \, - \,  0.26 \, (v_j - \bar v) \, - \,
      0.02 \, (v_j - \bar v)^2  \,+\,  0.20   (v_i - \bar v)\, (v_j - \bar v) \ .
\]
and the following table.
\begin{center}
\begin{tabular}{l r@{.}l  r@{.}l  r@{.}l  r@{.}l }
$ v_i \ \  \backslash  \ \ v_j $   &  \mcc{2}{ 1} & \mcc{2}{ 2} & \mcc{2}{ 3} &  \mcc{2}{ 4}
\separationb
 1        &    0&18    &  --0&18    &  --0&58    &  ---1&04   \\
 2        &    0&06    &  --0&10    &  --0&31    &  --0&57   \\
 3        &  --0&06    &  --0&02    &  --0&03    &  --0&10   \\
 4        &  --0&18    &    0&06    &    0&24    &    0&38   \\
\hline
\end{tabular}
\end{center}

The fact that we are using three variables
involving alter
(alter, alter squared, interaction) instead of two
(alter and similarity) leads to greater freedom in the curve that is fitted:
the top (or, in the rare case of a reversed pattern, bottom)
of the attractiveness of alters is not necessarily
obtained at the diagonal, i.e., at ego's value.
Straightforward calculus shows us that (\ref{eq_sel2}) is a quadratic
function and obtains its extreme value (a maximum if $\beta_{\rm sq.\ alter} $
is negative, a minimum if it is positive -- the latter is, in general,
less likely) for
\begin{equation}
  v_j \,=\, \bar v \,-\, \frac{\beta_{\rm alter}  \, + \,  \beta_{\rm e \times a} \, (v_i - \bar v)}
                              {2\, \beta_{\rm sq.\ alter}} \ .
                 \label{eq_extreme}
\end{equation}
If the effect $\beta_{\rm sq.\ alter}$ of the squared alter's value is negative
and the interaction effect $\beta_{\rm e \times a}$ is positive,
then this location of the maximum increases with ego's own value, $v_i$.
Of course the number given by (\ref{eq_extreme}) will usually not be an integer number,
so the actual value of $v_j$ for which attractiveness is maximized is
the integer in the range of $V$ closest to~(\ref{eq_extreme}).

For drinking there is a weak positive effect of squared drinking alter;
the effect of squared drug use alter is weak negative.
For drinking we see that the most attractive value
for egos with $v_i = 1$ or 2 is no drinking, $v_j = 1$,
whereas for egos with $v_i \geq 3$ the most attractive alters
are those who drink most, $v_j = 5$.
We also see that egos with the highest drinking
behavior are those who differentiate most strongly
depending on the drinking behavior of their potential friends.

For drug use the situation is different.
Actors with $v_i = 1$ or 2 prefer friends with drug use $v_j = 1$;
for actors with $v_i = 3$ the difference is hardly discernible,
but if we consider the differences even though they are tiny,
then they are most attracted to others with $v_j = 2$;
actors with the highest drug use ($v_i = 4$) differentiate most strongly,
and are attracted most to others with also the highest drug use.

The differences between the results with the similarity effects and the
interaction effects are minor. The extra degrees of freedom of the
latter model gives a slightly closer fit to the data.
However, the differences between the two fits are not significant,
as can be shown e.g.\ by score-type tests.

\subsubsection{Ego -- alter influence tables}

In quite a similar way as in the preceding section,
from the output tables and the formulae for the effects
we can construct tables indicating how attractive
various different values of the behavior are,
depending on the behavior of the actor's friends.

In the first model, the estimated coefficients in the
behavior evaluation function are as follows.
\begin{verbatim}
15. eval:  behavior drink shape                          0.3618  (   0.1946)
16. eval:  behavior drink average similarity             3.9689  (   2.2053)
17. eval:  behavior drink: effect from drink            -0.0600  (   0.1181)
\end{verbatim}
The dependent behavior variable now is indicated $Z$. (In the preceding
section the letter $V$ was used, but this referred to any actor variable
predicting network dynamics,
whether it was also a dependent variable or not.)
The formulae in Section \ref{S_f_b} show that the evaluation function
for this model specification is
\begin{equation}
   u^{\rm beh} \,=\, \beta_{\rm trend} \, (z_i - \bar z) \,+\, \beta_{\rm drink} \, (z_i - \bar z)^2 \,+\,
                   \beta_{\rm av.\ sim}\,  \frac{1}{x_{i+}} \,
                    \sum_j x_{ij} ({\rm sim}^z_{ij} - \widehat{{\rm sim}^z}) \ .
                    \label{eq_f_b1}
\end{equation}

\noindent
In the second model, the table gave the following results.
\begin{verbatim}
17. eval:  behavior drink shape                          0.3820  (   0.2421)
18. eval:  behavior drink average alter                  1.1414  (   0.6737)
19. eval:  behavior drink: effect from drink            -0.5428  (   0.2839)
\end{verbatim}
Here the evaluation function is
\begin{equation}
   u^{\rm beh} \,=\, \beta_{\rm trend} \, (z_i - \bar z) \,+\, \beta_{\rm drink} \, (z_i - \bar z)^2 \,+\,
                   \beta_{\rm av.\ alter}\,  (z_i - \bar z)(\bar z_{(i)} - \bar z)  \ ,
                    \label{eq_f_b2}
\end{equation}

\noindent
where $\bar z_{(i)} $ is the average $Z$ value of $i$'s
friends\footnote{If $i$ has no friends, i.e., $x_{i+} = 0$,  then $\bar z_{(i)} $ is defined
to be equal to $\bar z$.},
\[
  \bar z_{(i)}  =\frac{1}{x_{i+}} \, \sum_j x_{ij}\, z_j   \ .
\]
Equation (\ref{eq_f_b2}) is simpler than equation (\ref{eq_f_b1}), because
(\ref{eq_f_b2}) is a quadratic function of $z_i$, with coefficients depending
on the $Z$ values of $i$'s friends as a function of their average,
whereas (\ref{eq_f_b1}) depends on the entire distribution
of the $Z$ values of $i$'s friends.

Suppose that, in model (\ref{eq_f_b1}),
the similarity coefficient $\beta_{\rm av.\ sim}$ is positive,
and compare two focal actors,
$i_1$  all of whose friends have $z_j = 3$
and $i_2$ who has four friends, two of whom with
$z_j = 2$ and the other two with $z_j = 4$.
Both actors are then drawn toward the preferred value
of 3; but the difference between drinking behavior 3 on one hand
and 2 and 4 on the other hand will be larger for $i_1$
than for $i_2$.
In model (\ref{eq_f_b2}), on the other hand,
since the average is the same,
both actors would be drawn equally strongly toward
the average value 3.

For model (\ref{eq_f_b1}), consider actors
in the extreme situation
that all their friends have the same behavior $z_{ij}$.
For the parameters given above, the behavior
objective function then reads
\[
   u^{\rm beh} \,=\, 0.36 \, (z_i - \bar z) \,-\, 0.06 \, (z_i - \bar z)^2 \,+\,
                   3.97 \,  ({\rm sim}^z_{ij} - \widehat{{\rm sim}^z}) \ .
\]
This can be tabulated as follows.
\begin{center}
\begin{tabular}{l r@{.}l  r@{.}l  r@{.}l  r@{.}l  r@{.}l }
 $\bar z_{(i)}$ \ \ $ \backslash $ \ \ $z_i $   &  \mcc{2}{ 1} & \mcc{2}{ 2} & \mcc{2}{ 3} & \mcc{2}{ 4} &  \mcc{2}{ 5}
\separationb
 1        &  --0&05    &  --0&82    &  --1&71    &  --2&72    &  --3&84   \\
 2        &  --1&38    &    0&50    &  --0&39    &  --1&39    &  --2&52   \\
 3        &  --2&70    &  --0&82    &    0&94    &  --0&07    &  --1&20   \\
 4        &  --4&02    &  --2&14    &  --0&39    &    1&25    &    0&13   \\
 5        &  --5&35    &  --3&47    &  --1&71    &  --0&07    &    1&45   \\
\hline
\end{tabular}
\end{center}

For the other model, filling in the estimated parameters
in (\ref{eq_f_b2}) yields
\[
   u^{\rm beh} \,=\, 0.38 \, (z_i - \bar z) \,-\, 0.54 \, (z_i - \bar z)^2 \,+\,
                   1.14  \, (z_i - \bar z) ( \bar z_{(i)} - \bar z) \ .
\]
For a given average $Z$ values of $i$'s friends, this is a
quadratic function of $z_i$.
The following table indicates the behavior objective function
for $z_i$ (columns) as a function of the average drinking behavior
of $i$'s friends (rows).
\begin{center}
\begin{tabular}{l r@{.}l  r@{.}l  r@{.}l  r@{.}l  r@{.}l }
 $\bar z_{(i)}$ \ \ $\backslash $ \ \ $z_i $   &  \mcc{2}{ 1} & \mcc{2}{ 2} & \mcc{2}{ 3} & \mcc{2}{ 4} &  \mcc{2}{ 5}
\separationb
 1        &    1&87    &    1&59    &    0&22    &  --2&23    &  --5&76   \\
 2        &  --0&55    &    0&32    &    0&09    &  --1&22    &  --3&61   \\
 3        &  --2&96    &  --0&95    &  --0&04    &  --0&20    &  --1&46   \\
 4        &  --5&37    &  --2&22    &  --0&16    &    0&81    &    0&70   \\
 5        &  --7&78    &  --3&49    &  --0&29    &    1&82    &    2&85   \\
\hline
\end{tabular}
\end{center}
We see that, even though the squared function does not necessarily
draw the actors toward the average of their friends' behavior,
for these parameters the highest values of the
behavior objective function are obtained indeed
when the focal actor ($i$) behaves just like
the average of his friends.
It should be noted that no between-ego comparisons are made,
so comparisons are meaningful only within rows.
The values far away from the maximum contrast in this case more
strongly than in the case of the model with the average similarity
effect, but these differences here are not significant.

Another way to look at the behavior objective function is to consider
the location of its maximum. This function here can be written also as
\[
   u^{\rm beh} \,=\, \big( 0.38 + 1.14( \bar z_{(i)} - \bar z) \big) \, (z_i - \bar z)
       \,-\, 0.54 \, (z_i - \bar z)^2   \ .
\]
This function is maximal for
\[
   z_i = \bar z \,+\, 0.35 \,+ \,1.05 \, (z_i - \bar z) \ .
\]



%\begin{print}
%\newpage
%\end{print}
%\fi

%% \documentclass[a4paper,11pt,titlepage]{article}
% \usepackage{rotating}
% \usepackage{longtable, lscape}
% \usepackage[top=2.5cm, bottom=2.5cm, left=2cm , right=1.8cm]{geometry}
% \author{Paulina Preciado}
% \title{List of Functions for RSiena}

% \begin{document}
% \maketitle
% \tableofcontents
% \listoftables
\appendix
\newpage
\section{List of Functions in Order of Execution}

    This appendix, for which we are indebted to Paulina Preciado Lopez,
    provides a description of the functions that constitute the
    RSiena package. This is intended as a quick reference or catalogue for the
    user to employ Stochastic Actor Oriented Models (SAOM) to analyse network
    dynamics in R.

    The functions are presented in execution order (in the approximate order in
    which they would be used in real model estimation) A list of useful R
    functions to read and prepare the data set is also included at the
    beginning. In all cases examples on how to use these functions are provided.

    The descriptions provided are suitable for beginner and intermediate R and
    Siena users. For the advanced specifications of the functions the user
    should refer to the help by typing ``?funName'' in the R console, where
    ``funName'' is the name of the function.

    We consider that the model estimation is composed by 6 stages:
\begin{enumerate}
    \item Getting started
    \item Get the data the right format or check that it is in the correct
      format
    \item Data specification
    \item Model specification
    \item   Model estimation
    \item   Working with the results
\end{enumerate}

Tables \ref{tab:FuncExR} and \ref{tab:ListSienaExec} present the list of useful
R functions and the list of RSiena functions in execution order, respectively.
\begin{footnotesize}
\begin{sidewaystable}
\begin{threeparttable}
\centering
        \begin{tabular}{c | l | p{4.5cm} | p{5cm} | p{8cm} }
        \multicolumn{1}{c}{\textbf{Stage}} & \multicolumn{1}{c}{\textbf{Name}}
 &                          \multicolumn{1}{c}{\textbf{Syntax}} &
 \multicolumn{1}{c}{\textbf{Examples}} &
\multicolumn{1}{c}{\textbf{Description}} \\
        \cline{1-5}
        1 &help\tnote{*} &  help(funame) &  help(siena01Gui) &
    Opens the help on the function named ``funame'', this can also be done by
    typing ``?'' followed by ``funame'' in  the console\\
        \hline
        1   &getwd  &getwd()        & &
Returns the name of the current working directory.
                Does not require arguments\\
                \hline
        1 & list.files &    list.files(dir) &
list.files (``C:/User/ MyDocuments/ MySiena'') &    Returns a character vector
with the names of the files in the directory ``dir''.
If no argument is provided, ``dir'' is the current working directory.
Type ?list.files for more options\\
\hline
1   &setwd\tnote{*} &setwd(dir) &setwd(``C:/MyDocuments/ MySiena'') &
    Sets the working directory to ``dir''. In this context the working
 directory should be where the data is saved\\
\hline
1   &install.packages\tnote{*} &    install.packages() &
&It is used to install packages. If no arguments are provided it opens a GUI
 to select a mirror site and the packages that we want to download and
 install. This is not necessary if the package has already been installed.
 See ?install.packages for more options\\
\hline
1   &library\tnote{*} & library(package) &  library(RSiena) &
Loads the library named ``package''. See ?library for more options.\\
\hline
1   &read.table &   read.table(file, header = FALSE, sep = ``'',
quote = ``\'''',...) &  net1$<$--read.table(`network1. dat' , header = F)
&   Reads a file in table format and creates a data frame from it.
The argument ``file'' is the file containing the data. In the case of
adjacency matrices, the file should have the same number of columns and rows.
``header'' is a logical argument indicating whether the first row of the data
contains the column names. ``sep'' is the field separator character
 (such as space, comma, etc.). See the help on the function to specify
other arguments\\
\hline
2 &  as.matrix   &as.matrix(x,...)   &net1 $<$-- as.matrix(net1) &
Transforms an object ``x'' into a matrix. Siena works with matrices and
not with data frames\\
\hline
2&  class &   class(x)  & class(net1)  & Returns the type of object that
``x'' is\\
\hline
2&   dim  & dim(x)  & dim(net1) &  Returns the dimension of object ``x''\\
\hline
4 &   fix\tnote{*}  & fix(x) &  fix(effects) &  Allows editing the object ``x''
 by opening a window and it replaces the old object by the edited ``x''\\
\hline
\end{tabular}
\caption[Functions from R in order of execution]
{Useful functions from R in execution order}
\label{tab:FuncExR}
\begin{tablenotes}
\item [*] Also available via a menu option
\end{tablenotes}
\end{threeparttable}
\end{sidewaystable}
\end{footnotesize}


\begin{landscape}
\begin{footnotesize}
\begin{longtable}{c | p{3cm} | p{5.2cm} | p{4.2cm} | p{8.5cm} }
\caption[List of RSiena Functions: Execution]
{List of RSiena Functions in order of Execution}
\label{tab:ListSienaExec} \\
\hline

\multicolumn{1}{c}{\textbf{Stage}} &
\multicolumn{1}{c}{\textbf{Name}} &
\multicolumn{1}{c}{\textbf{Syntax}} &
\multicolumn{1}{c}{\textbf{Examples}} &
\multicolumn{1}{c}{\textbf{Description}} \\
\hline
\endfirsthead

\multicolumn{5}{c}%
{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\
\hline
\multicolumn{1}{c}{\textbf{Stage}} &
\multicolumn{1}{c}{\textbf{Name}} &
\multicolumn{1}{c}{\textbf{Syntax}} &
\multicolumn{1}{c}{\textbf{Examples}} &
\multicolumn{1}{c}{\textbf{Description}} \\
\hline
\endhead

\hline \multicolumn{5}{|r|}{{\tiny{Continued...}}} \\
\hline
\endfoot

\hline
\hline
\endlastfoot

1 & installGui &    installGui()    &
    &Starts the installer for the standalone version of RSiena.
Only for Windows. Does not require arguments\\
\hline

3 to 5& siena01Gui& siena01Gui()&   &   Does not require arguments.
 Opens a GUI to be used to run the model estimation or to create a session
 from which to work within R. Details on how to run the estimation under
 the GUI can be found in section \ref{thegui} and \ref{estgui}.\\
\hline

3   &sienaNodeSet   &sienaNodeSet (n, nodeSetName = ``Actors'',
names = NULL)   & & Creates a Siena node set which can be used as the nodes
 in a siena network. ``n'' is the number of actors or nodes; ``nodeSetName''
 is a character string to name the node set (defaults to ``Actors'') and
```names'' is a string vector with length n with the names of each node
(optional)\\
\hline

3 & sienaNet & sienaNet (netarray, type = c(``oneMode'', ``bipartite'',
``behavior''), nodeSet = ``Actors'', sparse = is.list (netarray)) &
sienaNet(array(c(net1,net2, net3), dim = c(dim(net1),3))) & Creates a Siena
network object by forming an array of network observations represented as
matrices, arrays or sparse matrices. ``netarray'' is a matrix (type=``behavior''
only) or array of values or list of sparse matrices of type
``dgTMatrix'';``type'' is either ``one mode'' (default), ``bipartite'' or
``behaviour''; ``nodeSet'' is the name of the node set.  It is a vector with two
strings for a bipartite network; ``sparse'' is logical and it is set to TRUE if
the data is in sparse matrix
format,  FALSE otherwise\\
\hline

3 &coCovar & coCovar(val, nodeSet =`Actors') & cons $<$-- as.matrix( read.table
('cons.DAT')) \newline cons1 $<$-- coCovar (cons[,1]) & Creates a constant
covariate object, where val is the vector of covariate values and nodeSet is the
name of the actors' set.  The dimension of val should be (1, \#
Actors)\\
\hline

3 & varCovar & varCovar(val, nodeSet =`Actors') & chan $<$-- as.matrix
(read.table ('chan.DAT')) \newline chan $<$-- varCovar (chan[,1]) & Creates a
changing covariate object where ``val'' is a matrix with the covariate values
with one row for each actor and one column for each period; ``nodeSet'' is the
name of the set of actors \\
\hline

3& coDyadCovar &coDyadCovar(val, nodeSets = c(``Actors'', ``Actors'')) & &
Creates a constant dyadic covariate object where ``val'' is a matrix of the same
dimension as the network observations and nodeSets are the sets of actors with
which the constant covariate is associated\\
\hline

3 &varDyadCovar & varDyadCovar(val, nodeSets = c(``Actors'', ``Actors'')) &
&Creates a changing dyadic covariate object where ``val'' is an array of
matrices. Each matrix has the same dimension of the actor set and ``val'' has
one less matrices than observations of the network; ``nodeSets'' are the sets
of actors to which the varying covariate object is associated\\
\hline

3 & sienaComposition Change & sienaCompositionChange(changelist, nodeSet =
"Actors", option = 1) & & Creates a list of events describing the moments in
which each actor is present in the network: ``changelist'' is a list with an
entry for each actor in the node set. Each entry is a vector indicating
intervals in which an actor is present in the network. ``nodeSet'' is the name
of the set of actors corresponding to these composition changes and ``option''
(defaults to 1) is an integer controlling the processing of the network entries
for the actors not currently present. See help(sienaCompositionChange) for
details on this\\
\hline

3 & sienaComposition ChangeFromFile & sienaCompositionChangeFromFile (filename,
nodeSet = "Actors", fileobj = NULL, option = 1) & & Creates a list of events
describing the changes over time in the actor set from a file. ``filename'' is
the name of the file containing change information (one line per actor) each
line is a series of space delimited numbers indicating intervals. ``fileobj'' is
the result of readLines on ``filename''. ``nodeSet'' is the name of the set of
actors. ``option'' (defaults to 1) has the same
description that in  sienaCompositionChange\\
\hline

3 & sienaDataCreate & sienaDataCreate(...,nodeSets=NULL, getDocumentation =
FALSE) & MyData $<$-- sienaDataCreate (net, cons1, cons2, cons3, chan, dyad) &
Creates a siena object from networks, covariates, composition and behaviour
objects: .``...''  represents the objects of class ``sienaNet'', ``coCovar'',
``varCovar'', ``coDyadCovar'', ``varDyadCovar'',
``compositionChange''. ``nodeSets'' is a list of Siena node sets. Default is a
single set named ``Actors'' with length equal to the number of rows in the first
object of class ``SienaNet'', it has to match the nodeSet supplied when the
arguments are created; ``getDocumentation'' is a flag to allow
documentation for internal functions,  not for use by users\\
\hline

3 & sienaDataCreateFrom Session & sienaDataCreateFromSession(file name = NULL,
session = NULL, modelName = ``Siena'', ...) & myobj $<$-- sienaDataCreate
FromSession(`Session.csv') & Reads a SIENA session from a file and creates a
Siena Data object or group. ``file'' is the session file; ``session'' is the
input session if the function is called from siena01Gui(); ``modelName'' is the
project's name; ``...'' refers to other
arguments used by siena01Gui()\\
\hline

3 & sienaGroupCreate & sienaGroupCreate (objlist, single OK = FALSE,
getDocumentation=FALSE) & sienaGroupCreate (list( MyData1, MyData2)) & Creates
an object of class ``sienaGroup'' from a list of Siena data objects: ``objlist''
is a list of objects of class ``siena''; ``singleOK'' is a boolean variable to
indicate if it is OK to have just one object; ``getDocumentation''
is a flag to  allow documentation of internal functions, not for use by users\\
\hline

4 & effectsDocumentation & effectsDocumentation() & & Prints a html or
\LaTeX\ table with the  effects details\\
\hline

4 & getEffects& getEffects(x, nintn = 10, behNintn = 4, getDocumentation =
FALSE) & MyEff$<$--getEffects (MyData, nint = 2, behNint = 1) & Creates a siena
effects objects (a data frame) that contains a list of the effects that can be
included in the model.  Type fix(MyEff) to edit the effects through a GUI
(e.g. Including them or excluding them, changing their names, initial values,
fixing them, etc.) The arguments are a siena or a siena group object ``x'', the
number of lines for user defined network interactions ``nint'' and the number of
lines for user defined behaviour interactions ``behNintn''. ``getDocumentation''
is a flag to allow documentation for
internal functions, not to  be used by users\\
\hline

4 & includeEffects & includeEffects(myeff, ..., include = TRUE, name =
myeff\$dollar name[1], type = ``eval'', interaction1 = ``'', interaction2 =
``'') & {MyEff$<$--includeEffects(MyEff, transTrip, balance) \flushleft
MyEff$<$--includeEffects(MyEff, sameX, sameXRecip, interaction1="gender")}
 &The function is a
way to select the effects to be included. ``myeff'' is an effects object, as
created by getEffects. It is necessary to indicate the short names to identify
the effects to be included (argument ...). Use myeff\$shortName to get a list of
the short names of possible effects to include and myeff\$effectName to get the
full name of the effects. This information can also be found in the
documentation
created by effectsDocumentation(). The ``include = TRUE'' indicates that we want
to
include the ``...'' effects in the model, it can be set to FALSE to exclude
effects from the model. ``name'' is the name of the network for which effects
are being included. ``type'' is to include ``eval'' (objective function effects)
or ``endow'' (endowment function effects). ``interaction1'' and ``interaction2''
are names of siena objects (where needed) to completely identify the effects
e.g. covariate name or behavior variable name. Use
myeff\$effectName[myeff\$include]
to get the names of the included effects. It returns a
new effects object, so it is important to assign it to a
name\\
\hline

4   & includeInteraction &  includeInteraction(myeff, ..., include = TRUE, name
= myeff\$name[1], type  = ``eval'', interaction1 = rep(``'', 3), interaction2 =
rep(``'', 3)) &  MyEff$<$--includeInteraction( MyEff, transTrip, egoX,
interaction1 = c(``'',``beh''))     &This function provides an interface to
allow easy update of  an unspecified interaction row in a Siena effects
object. ``myeff'' is  a Siena  effects object as created by getEffects. To
specify the effects to interact,  list their short names instead of ``...'';
``include'' is a boolean variable,  default TRUE to include the interaction, it
can be switched to FALSE to turn off  an interaction. ``name'' is the name of
network for which interactions are being  defined. Defaults to the first in the
effects object. ``type'' is the type of  effects to be interacted: currently
only ``eval'' or ``endow''. ``interaction1''  is a vector of siena objects where
needed to completely identify the effect e.g.  covariate name or behavior
variable name. Trailing blanks may be omitted.  ``interaction2''  is a vector of
siena objects where needed to completely  identify the effect e.g. covariate
name or behavior variable name.  Trailing blanks may be omitted\\
\hline

4 & setEffect &setEffect(myeff, shortName, parameter = 0, fix = FALSE, test =
FALSE, initialValue = 0, include = TRUE, name = myeff\$name[1], type = ``eval'',
interaction1 = ``'', interaction2 = ``'') & MyEff $<$-- setEffect(MyEff,
transTrip, initialValue = 3, include = T) & Interface to change the attributes
of a particular effect. The required arguments are an effect object (``myeff''),
the short name of the effect to modify (``shortName'') and a required integer
value that defaults to zero (``parameter''). Depending on what it is desired to
be modified we can supply: ``fix = TRUE'', if we wish to fix that parameter;
``test = TRUE'' if we wish to test that parameter; ``initialValue = 2'' (or any
desired number) to modify the effect's initial value (Defaults to zero);
``include = TRUE or FALSE'' depending on whether we want to include/exclude the
effect (defaults to TRUE). The arguments ``name'', ``type'' and ``interaction1''
and
``interaction2'' are defined as in includeInteraction  and includeEffects\\
\hline

5 & print01Report & print01Report(data, myeff, modelname = ``Siena'', session =
NULL, getDocumentation = FALSE) & print01Report(MyData, MyEff) & Prints a report
of a Siena data object and its default effects. We need to supply a Siena data
object (``data'') a siena effects object (``myeff'') and a model name
(``modelname'') that defaults to ``Siena''. It creates and saves a file named
``modelname.out'' (Siena.out) that contains preliminary information
on the data\\
\hline

5 & sienaModelCreate & sienaModelCreate(fn = simstats0c,
usesimstats0c = deparse(substitute(fn)) == "simstats0c",
projname = ``Siena'', MaxDegree = 0, useStdInits = FALSE, n3 = 1000, nsub = 4,
maxlike = FALSE, diag = !maxlike,
condvarno = 0, condname = ``'', firstg = 0.2, cond
= NA, findiff = FALSE, seed = NULL) & MyModel $<$-- model.create (projname =
``MyProject'') & Creates a siena model object that can be used to call
siena07. ``fn'' is function to do one simulation in the Robbins-Monro
algorithm. ``usesimstats0c'' is boolean, if true the standard algorithm is being
used which can be used with multiple processors. Just used for
validation. ``projname'' is character string name of project. No embedded
spaces. ``MaxDegree'' is a named vector of maximum degree values for
corresponding networks. ``useStdInits'' is a boolean variable, if TRUE, the
initial values in the effects object will be ignored and default values used
instead. ``n3'' is the number of iterations in phase 3 (defaults to
1000). ``nsub'' is the number of subphases in phase 2 (defaults to
4). ``maxlike'', boolean to indicate whether to use maximum likelihood method or
straightforward simulation (defaults to false). ``diag'' is boolean to indicate
if the complete estimated derivative matrix should be used; ``condvarno'', if
conditional estimation is used the parameter is the sequential number of the
network or behaviour variable on which to condition. ``condname'' is the name of
the dependent variable on which to condition (only use condname or condvar, not
both). ``firstg'' initial value of scaling parameter in the Robbins-Monro
procedure. ``cond'' is boolean, If TRUE, use conditional simulation. If missing,
decision is deferred until siena07 is called, when it is set to TRUE if there is
only one dependent variable, FALSE otherwise. ``findiff'' is boolean, if TRUE,
estimate derivatives using finite differences and if FALSE, use scores. ``seed''
is an integer referring to the starting value of random seed. Not used if
parallel
testing\\
\hline

5   & model.create  & & &       See sienaModelCreate\\
\hline

5 & siena07 & siena07(x, batch = FALSE, verbose = FALSE, silent = FALSE,
useCluster = FALSE, nbrNodes = 2, initC = FALSE, clusterString =
rep(``localhost'', nbrNodes), tt = NULL, parallelTesting = FALSE, ...) & ans
$<$-- siena07(MyModel, data = MyData, effects = MyEff, batch = FALSE,
verbose = TRUE,
useCluster = TRUE, nbrNodes = 2, initC = TRUE) & Fits a model using the method
of
moments based on straightforward simulation, conditional or otherwise, or on an
MCMC simulation. Estimation is done using Robbins-Monro algorithm. Note that the
particular model to be used is passed on as the model object, and data for the
model must be passed by using named arguments. ``x'' is a model object;
``batch'' is a boolean variable to indicate if it is desired to open the GUI of
Siena simulation; ``verbose'' is a boolean variable to produce output on the
console; ``silent'' is also a boolean variable, if true, no output is printed to
the console; ``useCluster'' is a boolean variable to indicate if it is desired
to use a cluster of processors; ``nbrNodes'' is the number of processors to use
if useCluster is TRUE; ``initC'' is boolean: set to TRUE if the simulation will
use C routines (currently always needed). Only relevant if using multiple
processors, to ensure all copies are initialised correctly. ``clusterString'' is
the definition of clusters, default set up to use the local machine only; ``tt''
is a tcltk toplevel window used if called from the model options screen;
``parallelTesting'' is boolean, if TRUE, sets up random numbers to parallel
those in Siena 3. ``...''  Arguments for the simulation function, such as the
data, effects, etc. It returns an object of class sienaFit (lets say ans). The
main attributes are theta, which are the estimated coefficients (view with
ans\$theta); covtheta is the estimated covariance matrix of theta; dfra is the
matrix of estimated derivatives; ans\$targets and ans\$ targets2 are the
observed statistics and the observed statistic for each wave respectively;
ans\$ssc are the score function contributions for each wave for each simulation
in phase 3: ans\$sims is the simulated networks as edgelists. Use names(ans) to
obtain more characteristics; only recommended if you are
proficient in RSiena\\
\hline

6 & print.sienaFit & print(x, tstat=TRUE, ...) & print(ans) & The function
prints a table containing the estimated parameter values, standard errors and
(optionally) t-statistics for convergence. If ``x '' is a summary(sienaFit) it
prints on the console all the summary elements. ``tstat'' is a boolean argument,
set to TRUE if it is desired for the t-statistics for convergence to
be added to the report\\
\hline

6 & summary.sienaFit & summary(x,...) & summary(ans) & Prints a table containing
the estimated parameter values, standard errors and t-statistics for convergence
together with the covariance matrix of the estimates, the derivative matrix of
expected statistics D by parameters, and the covariance matrix of the expected
statistics D.  The only required argument is a
``sienaFit'' object ``x'', as produced by  siena07\\
\hline

6 & xtable.sienaFit & xtable(x, caption = NULL, label = NULL, align = NULL,
digits = NULL, display = NULL, ...) & sienaxtab $<$-- xtable(ans, caption = ``My
Table'', digits = 2).  &Creates an object of class xtable.sienaFit which inherits
from class xtable and passes an extra arguments to the print.xtable.
The argument is a sienaFit object ``x''. See ?xtable for more options\\
\hline


\end{longtable}
\end{footnotesize}
\end{landscape}

%% Document ends here
%\end{document}

%\section{List of Functions in Alphabetical Order}

%\input{ListFunctionsCleanrr}

\section{Changes compared to earlier versions}

This begins at end October 2009, and only details changes which affect the user.
(Programmers should consult the changeLog file on CRAN or in the R-forge
repository.)
\begin{itemize}
\item 2010-04-24 R-forge revision 81
New print, summary and edit methods for Siena effects objects
\item 2010-04-24 R-forge revision 80
\begin{itemize}
\item fixed bug causing crash with rate effects and bipartite networks.
\item added trap to stop conditional estimation hanging
\item new functions (INCOMPLETE) for maximum likelihood and Bayesian estimation
  (one period (two waves) only, no missing data, one dependent variable only for
  Bayesian model).
\end{itemize}

\item 2010-04-13 R-forge revision 79 new function: sienaTimeTest.
\item 2010-04-12 R-forge revision 78 fix minor bugs in reports, allow character
  input to effect utility functions, inlcude effect1-3 etc on diaplay of
  included effects in siena01Gui().
\item 2010-04-12 R-forge revision 77 (RSiena only) As for RSienaTest revision 76
\begin{itemize}
\item Report of 0 missings corrected
\item display of effect1-effect3 in siena01Gui
\item allow entry of character strings or not in includeEffects etc.
\end{itemize}
\item 2010-04-12 R-forge revision 76 (RSienaTest only) Various bug fixes
\begin{itemize}
\item Memory problems when calculating derivatives with many iterations and
  parameters.
\item Occasional effects not being included correctly due to trailing blanks
\item Some minor details of reports corrected.
\end{itemize}
\item 2010-03-31 R-forge revision 75 fixed bug with dyadic covariates and
  bipartite networks.
\item 2010-03-27 R-forge revision 71 (RSienaTest only)
\begin{itemize}
\item Fixes as for RSiena in revision 68/69/70 for RSiena
\item New version number 1.0.12
\end{itemize}
\item 2010-03-27 R-forge revision 70 (RSiena only)
\begin{itemize}
\item Fix to crash at end of phase 3 with multiple processors and
conditional estimation
\item Correct carry forward/backward/use mode for behavior variables
\item Fix bug causing crash in Finite Differences with only one effect
\end{itemize}
\item 2010-03-24 R-forge revision 69 (RSiena only)
\begin{itemize}
\item New features and bug fixes as for revision 63 in RSienaTest.
\item 4-cycles effect has new shortName: cycle4.
\item some percentages on reports were proportions not percentages
\item Sped up treatment of missing values in sparse format networks.
\item Fix: now allows more than one value to indicate missing in covariates.
\end{itemize}
\item 2010-03-12 R-forge revision 68 new version number for RSiena.\\
In \sfn{siena01Gui}, allow waves for SienaNet inputs to be numbered
arbitrarily, rather than insisting on 1-n. Change simply allows this, the actual
wave numbers are not yet used on reports etc.
\item 2010-03-17 R-forge revision 66
Corrected processing of user-specified interaction effects with multiple
processors. This had originally worked but failed when one no longer had to
include the underlying effects.
\item 2010-03-16 R-forge revision 64
covarBipartite ego effect had been given type dyadic rather than ego.
\item 2010-03-16 R-forge revision 63 (RSienaTest only)
\begin{itemize}
\item new functions \sfn{siena08} and \sfn{iwlsm}, for meta analysis
\item can now use different processors for each wave. Not recommended: usually
  slower than by iteration, but will be useful with ML routines when they are
  completed.
\item No longer crashes with missing dyadic covariates.
\end{itemize}
\item 2010-02-27 R-forge revision 61 (RSiena only) bug fix: random numbers used
  with multiple processes were the same in each run. Now seed is generated
  from the usual \R random number seed. Also fixed a display bug if running
  phase 3 with few iterations.
\item 2010-02-16 R-forge revision 60 (RSienaTest only) added average indegrees
  to reports. Also constraints.
\item 2010-02-12 R-forge revision 59 (RSienaTest only) Fix to bugs in printing
  version numbers and in using multiple processors (would revert to RSiena
  package.) Added a skeleton MCMC routine.
\item 2010-02-11 R-forge revision 57 Fix to bug in siena01Gui where in
  conditional estimation, the
  estimated values were not remembered for the next run.
\item 2010-02-11 R-forge revision 56 (RSiena only)
Multiple network effects, constraints between networks.
\item 2010-02-11 R-forge revision 55 (RSienaTest only)
New silent option for siena07.
\item 2010-02-11 R-forge revision 54 (RSienaTest only)
Fix to covariate behavior effect bug.
\item 2010-02-11 R-forge revision 53
Fixed bug in siena01 gui which ignored changes to all effeccts
\item 2010-02-07 R-forge revision 52 (RSiena only)
New silent option for siena07.
\item 2010-02-04 R-forge revision 51 (RSiena only)
\begin{itemize}
\item
Fix to covariate behavior effect bug.
\item
Fix to default effects with multiple networks.
\end{itemize}
\item 2010-02-01 R-forge revision 49 (RSienaTest) only
Fixes to bugs in constraints.
\item 2010-01-28 R-forge revision 48
Fix to bug in sorting effects for multiple dependent variables.
\item 2010-01-26 R-forge revision 47 (RSienaTest only)
\begin{itemize}
\item New version: 1.0.10
\item Multiple networks
\item Constraints of higher, disjoint, atLeastOne between pairs of networks.
\end{itemize}
\item 2010-01-19 R-forge revision 45 (RSiena), 46 (RSienaTest)\\
 New documentation for the effects object.
\item 2010-01-18 R-forge revision 43 (RSiena)
\begin{itemize}
\item new behavior effects
\item user specified interactions
\item new utilities to update the effects object
\end{itemize}
\item 2010-01-15 R-forge revision 41 (RSienaTest only)

\begin{itemize}
\item
   new effect: Popularity Alter, and altered effect1-3 to integers to correct
  bug in fix(myeff)
  \item new utility functions to update effects object
  \item no longer
  necessary to include underlying effects for interactions.
  \item user parameter for number of unspecified behavior interactions
  \item  remove extra sqrt roots in standard error of rates for conditional
  estimation (see revision 31)
\end{itemize}
\item 2010-01-15 R-forge revision 40: RSiena only

  remove extra sqrt roots in standard error of rates for conditional
  estimation (see revision 32)


\item 2010-01-02 R-forge revision 34

  Corrected layout of \sfn{print} and \sfn{xtable} for \sfn{SienaFit} objects
  with both behavior and network variables.

\item 2010-01-01 R-forge revision 33

Updated change log and manual in RSiena and changelog in RSienaTest.

\item 2010-01-01 R-forge revision 32
    print07report.r: corrected standard errors for rate estimate for
    conditional estimation: needed square roots. RSiena

\item 2009-12-31 R-forge revision 31
\begin{itemize}
\item
    print07report.r: corrected standard errors for rate estimate for
    conditional estimation: needed square roots. RSienaTest only

\item more behavior effects in RSienaTest.
\end{itemize}

\item 2009-12-17 R-forge revision 30

Fixed bug in dyadic interactions in RSienaTest

\item 2009-12-17 R-forge revision 29

Fixed bug in 3-way interactions in RSienaTest

\item 2009-12-14 R-forge revision 28

 Fixed bug in use of multiple processors for RSiena.

\item 2009-12-14 R-forge revision 27

Fixed bug in use of multiple processors for
  RSienaTest.

\item 2009-12-01 R-forge revision 26

Created RSienaTest which includes user
  specified interactions.

\item 2009-11-20 R-forge revision 25

  \begin{itemize}
  \item  version number 1.0.8
  \item The default method for estimation is conditional if there is only one
    dependent variable.
  \item Movement of behavior variable restricted if all observed changes are in
    one direction. In this case, linear change effects removed.
  \item If all observed changes in a network are in one direction, density
    effects are removed.
  \item If a behavior variable only takes two values the quadratic effects
    are not selected by default.
  \item t-statistics appear on print of \sfn{sienaFit} object.
  \item easier to use \sfn{xtable} method
  \item warning if behavior variables are not integers
  \item Fixed bug in editing all effects in the gui.
  \item Fixed a bug in effect creation for changing dyadic covariates
  \item Fixed a bug in returning simulated dependent variables
  \item Now fails if there are only two waves but you have a changing
    covariate. In the GUI, can just change the type.
  \end{itemize}

\item 2009-11-08 R-forge revision 24

  \begin{itemize}
  \item
    version Number 1.0.7
  \end{itemize}
\item  2009-11-08 R-forge revision 23

  \begin{itemize}
  \item corrected bug in creation of effects data frame for multi
    group projects and for changing covariates
  \item added effect numbers to the Estimation screen
  \end{itemize}
\item 2009-11-08 R-forge revision 22
  \begin{itemize}
  \item  new option to edit effects for one dependent variable at a time. Model
    options screen layout altered slightly.
  \end{itemize}
\item 2009-11-08 R-forge revision 21
  \begin{itemize}
  \item Fixed a bug causing crashes (but not on Windows!) due to bad calculation
    of derivative matrix.
  \end{itemize}
\item 2009-10-31 R-forge revision 17
\begin{itemize}
\item version Number 1.0.6
    \item xtable method to create \LaTeX tables from the estimation results
      object.
    \item added support for bipartite networks
    \item structural zeros and 1's processing checked and amended
   \item  use more sophisticated random number generator unless parallel
     testing with siena3.
   \end{itemize}
 \end{itemize}
\nocite{Federico04}
\nocite{Federico05}
\nocite{Frank91}
\nocite{FrankStrauss86}
\nocite{Handcock02}
\nocite{JariegoFederico06}

%\bibliographystyle{JRSS} % TS: I did not find this style so I changed it:
\bibliographystyle{Chicago}
\bibliography{RSiena}
\end{document}
